
================================================================
Quantisation mode: none
Output directory : onnx_exports/none
================================================================
[INFO] Core engines already exist for quant mode 'none'; skipping export.

--- Building extra precision fp32 for quant mode none ---
[INFO] Engine already exists (onnx_exports/none/vggt-8x3x518x518-pcd.engine); skipping.

================================================================
Quantisation mode: torch-int8-dynamic
Output directory : onnx_exports/torch-int8-dynamic
================================================================
[INFO] Missing core precisions for quant mode 'torch-int8-dynamic': int8

>>> python onnx/vggt_to_trt_chatgpt.py --export --all-precisions --quant-mode torch-int8-dynamic --output-dir onnx_exports/torch-int8-dynamic --num-cams 8 --height 518 --width 518 --model-name facebook/VGGT-1B --calib-batches 1 --calib-seed 1337 --pcd-only
[INFO] Building all precision variants: fp16, bf16, fp8, int8
[INFO] 
[INFO] ======================================================================
[INFO] Building FP16 variant (1/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp16 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: PyTorch dynamic INT8 on Linear/GRU/LSTM layers
[INFO] Requested quantisation mode 'torch-int8-dynamic' is not ONNX-compatible with ONNX export; skipping.
[INFO] Fallback (2/3) quantisation mode 'bitsandbytes-8bit' is not ONNX-compatible with ONNX export; skipping.
[INFO] Retrying export with fallback quantisation mode 'none' (3/3)
[INFO] using MLP layer as FFN
[INFO] Modifying model for PCD-only export...
[WARNING] PCD-only mode: will prune unused outputs after export
[INFO] Exporting to ONNX with shape [8, 3, 518, 518] on cuda:0 (quant='none')
/home/ashim/miniconda3/envs/compvis/lib/python3.10/site-packages/vggt/models/vggt.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
[INFO] Removed 851 unused nodes
[INFO] No unused functions to remove
[INFO] Skipping constant folding for node 'node_unsqueeze' because it is graph input to preserve graph signature
[INFO] Skipping constant folding for node Node(name='node_cat_3', domain='', op_type='Concat', inputs=(Value(name='expand_4', type=Tensor(FLOAT), shape=Shape([1, 1, 4, 1024]), producer='node_Constant_23972', index=0, const_value={Tensor(...)}), Value(name='expand_5', type=Tensor(FLOAT), shape=Shape([1, 7, 4, 1024]), producer='node_Constant_23994', index=0, const_value={Tensor(...)})), attributes={'axis': Attr('axis', INT, 1)}, overload='', outputs=(SymbolicTensor(name='cat_3', type=Tensor(FLOAT), shape=Shape([1, 8, 4, 1024]), producer='node_cat_3', index=0),), version=13, doc_string=None) due to large input sizes: [4096, 28672]
[INFO] Skipping constant folding for node Node(name='node_clone_74', domain='', op_type='Identity', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='clone_74', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_clone_74', index=0),), version=16, doc_string=None) due to large input sizes: [21904]
[INFO] Skipping constant folding for node Node(name='node_add_49', domain='', op_type='Add', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_25', type=Tensor(INT64), shape=Shape([]), producer='node_Constant_25', index=0, const_value={Tensor<INT64,[]>(array(1), name=None)})), attributes={}, overload='', outputs=(SymbolicTensor(name='add_49', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_add_49', index=0),), version=14, doc_string=None) due to large input sizes: [21904, 1]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20794', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21278', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_20794', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20797', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21281', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20797', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20803', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21291', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20803', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20845', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21333', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20845', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20847', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21337', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20847', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20856', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21346', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20856', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20858', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21350', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20858', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20900', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21392', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20900', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20902', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21396', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20902', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20911', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21405', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20911', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20913', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21409', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20913', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20955', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21451', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20955', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20957', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21455', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20957', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20966', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21464', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20966', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20968', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21468', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20968', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21010', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21510', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21010', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21012', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21514', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21012', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21021', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21523', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21021', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21023', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21527', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21023', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21032', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21536', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21032', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21034', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21538', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21034', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21037', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21541', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21037', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21042', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21550', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21042', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21084', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21592', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21084', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21086', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21596', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21086', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21095', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21605', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21095', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21097', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21609', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21097', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21139', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21651', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21139', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21141', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21655', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21141', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21150', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21664', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21150', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21152', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21668', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21152', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21194', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21710', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21194', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21196', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21714', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21196', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21205', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21723', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21205', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21207', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21727', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21207', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21249', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21769', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21249', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21251', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21773', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21251', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21260', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21782', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21260', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21262', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21786', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21262', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21271', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21795', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21271', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21273', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21797', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21273', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21276', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21800', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21276', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21281', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21809', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21281', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21323', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21851', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21323', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21325', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21855', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21325', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21334', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21864', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21334', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21336', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21868', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21336', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21378', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21910', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21378', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21380', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21914', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21380', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21389', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21923', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21389', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21391', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21927', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21391', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21433', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21969', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21433', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21435', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21973', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21435', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21444', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21982', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21444', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21446', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21986', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21446', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21488', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22028', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21488', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21490', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22032', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21490', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21499', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22041', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21499', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21501', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22045', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21501', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21510', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22054', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21510', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21512', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22056', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21512', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21515', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22059', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21515', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21520', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22068', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21520', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21562', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22110', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21562', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21564', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22114', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21564', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21573', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22123', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21573', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21575', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22127', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21575', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21617', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22169', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21617', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21619', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22173', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21619', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21628', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22182', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21628', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21630', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22186', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21630', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21672', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22228', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21672', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21674', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22232', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21674', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21683', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22241', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21683', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21685', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22245', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21685', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21727', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22287', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21727', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21729', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22291', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21729', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21738', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22300', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21738', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21740', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22304', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21740', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21749', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22313', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21749', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_sin_1', domain='', op_type='Sin', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_1', domain='', op_type='Cos', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_2', domain='', op_type='Sin', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_2', domain='', op_type='Cos', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_3', domain='', op_type='Sin', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_3', domain='', op_type='Cos', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_4', domain='', op_type='Sin', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_4', domain='', op_type='Cos', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_5 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_5', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_5', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_6 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_6', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_6', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_7 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_7', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_7', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_8 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_8', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_8', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_22 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_23 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22421', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_23', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_23', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22992', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22420', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22992')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22993', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22421', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22423', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_22', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_22', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22994', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22422', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22994')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22995', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22423', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_sin_11', domain='', op_type='Sin', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_11', domain='', op_type='Cos', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_12', domain='', op_type='Sin', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_12', domain='', op_type='Cos', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_13', domain='', op_type='Sin', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_13', domain='', op_type='Cos', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_14', domain='', op_type='Sin', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_14', domain='', op_type='Cos', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_15 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_15', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_15', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_16 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_16', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_16', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_17 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_17', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_17', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_18 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_18', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_18', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_37 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_38 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23144', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_38', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_38', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23723', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23143', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23723')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23724', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23144', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23146', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_37', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_37', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23725', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23145', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23725')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23726', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23146', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] Removed 19332 unused nodes
[INFO] No unused functions to remove
[INFO] Skipping constant folding for node 'node_unsqueeze' because it is graph input to preserve graph signature
[INFO] Skipping constant folding for node Node(name='node_cat_3', domain='', op_type='Concat', inputs=(Value(name='expand_4', type=Tensor(FLOAT), shape=Shape([1, 1, 4, 1024]), producer='node_Constant_23972', index=0, const_value={Tensor(...)}), Value(name='expand_5', type=Tensor(FLOAT), shape=Shape([1, 7, 4, 1024]), producer='node_Constant_23994', index=0, const_value={Tensor(...)})), attributes={'axis': Attr('axis', INT, 1)}, overload='', outputs=(SymbolicTensor(name='cat_3', type=Tensor(FLOAT), shape=Shape([1, 8, 4, 1024]), producer='node_cat_3', index=0),), version=13, doc_string=None) due to large input sizes: [4096, 28672]
[INFO] Skipping constant folding for node Node(name='node_add_49', domain='', op_type='Add', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_25', type=Tensor(INT64), shape=Shape([]), producer='node_Constant_25', index=0, const_value={Tensor<INT64,[]>(array(1), name=None)})), attributes={}, overload='', outputs=(SymbolicTensor(name='add_49', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_add_49', index=0),), version=14, doc_string=None) due to large input sizes: [21904, 1]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20794', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21278', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_20794', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20797', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21281', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20797', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20803', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21291', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20803', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20845', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21333', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20845', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20847', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21337', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20847', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20856', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21346', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20856', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20858', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21350', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20858', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20900', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21392', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20900', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20902', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21396', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20902', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20911', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21405', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20911', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20913', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21409', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20913', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20955', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21451', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20955', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20957', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21455', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20957', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20966', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21464', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20966', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20968', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21468', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20968', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21010', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21510', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21010', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21012', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21514', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21012', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21021', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21523', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21021', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21023', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21527', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21023', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21032', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21536', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21032', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21034', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21538', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21034', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21037', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21541', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21037', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21042', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21550', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21042', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21084', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21592', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21084', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21086', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21596', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21086', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21095', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21605', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21095', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21097', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21609', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21097', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21139', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21651', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21139', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21141', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21655', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21141', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21150', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21664', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21150', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21152', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21668', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21152', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21194', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21710', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21194', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21196', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21714', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21196', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21205', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21723', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21205', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21207', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21727', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21207', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21249', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21769', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21249', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21251', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21773', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21251', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21260', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21782', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21260', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21262', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21786', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21262', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21271', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21795', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21271', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21273', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21797', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21273', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21276', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21800', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21276', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21281', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21809', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21281', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21323', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21851', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21323', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21325', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21855', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21325', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21334', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21864', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21334', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21336', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21868', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21336', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21378', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21910', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21378', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21380', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21914', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21380', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21389', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21923', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21389', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21391', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21927', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21391', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21433', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21969', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21433', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21435', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21973', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21435', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21444', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21982', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21444', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21446', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21986', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21446', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21488', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22028', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21488', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21490', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22032', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21490', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21499', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22041', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21499', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21501', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22045', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21501', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21510', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22054', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21510', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21512', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22056', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21512', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21515', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22059', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21515', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21520', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22068', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21520', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21562', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22110', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21562', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21564', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22114', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21564', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21573', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22123', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21573', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21575', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22127', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21575', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21617', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22169', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21617', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21619', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22173', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21619', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21628', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22182', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21628', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21630', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22186', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21630', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21672', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22228', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21672', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21674', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22232', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21674', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21683', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22241', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21683', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21685', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22245', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21685', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21727', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22287', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21727', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21729', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22291', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21729', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21738', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22300', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21738', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21740', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22304', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21740', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21749', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22313', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21749', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_sin_1', domain='', op_type='Sin', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_1', domain='', op_type='Cos', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_2', domain='', op_type='Sin', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_2', domain='', op_type='Cos', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_3', domain='', op_type='Sin', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_3', domain='', op_type='Cos', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_4', domain='', op_type='Sin', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_4', domain='', op_type='Cos', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_5 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_5', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_5', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_6 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_6', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_6', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_7 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_7', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_7', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_8 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_8', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_8', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_22 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_23 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22421', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_23', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_23', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22992', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22420', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22992')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22993', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22421', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22423', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_22', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_22', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22994', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22422', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22994')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22995', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22423', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_sin_11', domain='', op_type='Sin', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_11', domain='', op_type='Cos', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_12', domain='', op_type='Sin', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_12', domain='', op_type='Cos', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_13', domain='', op_type='Sin', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_13', domain='', op_type='Cos', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_14', domain='', op_type='Sin', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_14', domain='', op_type='Cos', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_15 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_15', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_15', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_16 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_16', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_16', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_17 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_17', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_17', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_18 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_18', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_18', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_37 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_38 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23144', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_38', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_38', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23723', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23143', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23723')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23724', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23144', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23146', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_37', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_37', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23725', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23145', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23725')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23726', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23146', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] Removed 1189 unused nodes
[INFO] No unused functions to remove
[INFO] Replaced initializer 'val_2878' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_2889' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_3291' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_3693' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_3704' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_4106' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_4508' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4519' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_4921' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_5329' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_5340' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_5748' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6150' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6161' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_6563' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6965' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6976' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_7378' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_7780' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_7791' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_8193' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_8595' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8606' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_9008' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_9410' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_9421' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_9823' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_10225' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_10236' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_10638' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11046' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11057' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_11465' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11867' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11878' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_12280' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_12682' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_12693' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_13095' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_13497' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13508' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_13910' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_14312' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_14323' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_14725' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15127' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15138' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_15540' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15948' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15959' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_16367' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_16769' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_16780' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_17182' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_17584' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17595' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_17997' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_18399' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_18410' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_18812' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_19214' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_19225' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_19627' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20029' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20040' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_20442' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20850' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20861' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_24' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_31' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_32' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_46' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_50' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_51' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_90' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_98' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_124' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_141' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_144' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_146' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_149' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_157' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_164' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_167' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_169' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_183' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_200' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_203' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_205' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_208' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_216' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_223' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_226' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_228' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_242' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_259' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_262' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_264' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_267' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_275' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_282' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_285' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_287' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_301' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_318' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_321' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_323' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_326' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_334' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_341' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_344' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_346' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_360' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_377' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_380' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_382' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_385' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_393' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_400' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_403' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_405' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_419' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_436' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_439' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_441' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_444' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_452' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_459' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_462' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_464' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_478' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_495' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_498' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_500' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_503' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_511' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_518' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_521' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_523' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_537' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_554' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_557' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_559' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_562' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_570' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_577' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_580' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_582' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_596' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_613' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_616' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_618' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_621' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_629' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_636' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_639' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_641' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_655' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_672' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_675' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_677' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_680' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_688' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_695' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_698' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_700' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_714' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_731' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_734' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_736' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_739' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_747' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_754' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_757' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_759' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_773' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_790' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_793' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_795' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_798' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_806' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_813' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_816' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_818' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_832' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_849' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_852' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_854' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_857' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_865' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_872' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_875' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_877' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_891' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_908' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_911' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_913' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_916' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_924' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_931' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_934' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_936' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_950' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_967' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_970' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_972' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_975' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_983' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_990' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_993' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_995' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1009' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1026' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1029' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1031' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1034' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1042' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1049' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1052' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1054' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1068' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1085' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1088' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1090' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1093' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1101' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1108' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1111' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1113' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1127' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1144' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1147' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1149' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1152' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1160' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1167' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1170' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1172' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1186' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1203' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1206' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1208' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1211' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1219' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1226' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1229' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1231' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1245' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1262' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1265' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1267' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1270' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1278' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1285' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1288' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1290' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1304' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1321' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1324' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1326' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1329' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1337' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1344' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1347' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1349' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1363' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1380' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1383' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1385' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1388' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1396' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1403' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1406' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1408' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1422' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1439' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1442' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1444' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1447' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1455' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1462' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1465' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1467' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1483' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1486' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1487' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1662' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1673' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1686' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1740' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1748' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1751' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1754' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1757' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1758' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1759' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1760' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1823' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1826' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1829' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1830' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1833' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1836' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1839' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1840' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1841' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1842' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1905' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1908' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1911' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1912' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1915' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1918' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1921' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1922' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1923' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1924' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1987' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1990' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1993' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1994' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1997' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2000' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2003' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2004' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2021' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_2024' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_2026' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2029' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2037' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_2044' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2047' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2049' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2090' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2091' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2154' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2157' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2160' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2161' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2164' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2167' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2170' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2171' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2172' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2173' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2236' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2239' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2242' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2243' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2246' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2249' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2252' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2253' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2254' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2255' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2318' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2321' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2324' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2325' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2328' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2331' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2334' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2335' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2336' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2337' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2400' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2403' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2406' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2407' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2410' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2413' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2416' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2417' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2439' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2442' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2450' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_2457' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2460' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2462' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2487' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_2492' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2493' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2556' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2559' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2562' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2563' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2566' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2569' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2572' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2573' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2574' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2575' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2638' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2641' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2644' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2645' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2648' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2651' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2654' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2655' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2656' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2657' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2720' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2723' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2726' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2727' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2730' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2733' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2736' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2737' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2738' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2739' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2802' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2805' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2808' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2809' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2812' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2815' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2818' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2819' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2836' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_2839' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_2841' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2844' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2852' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_2859' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2862' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2864' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2900' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_2905' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2906' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2969' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2972' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2975' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2976' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2979' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2982' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2985' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2986' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2987' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2988' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3051' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3054' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3057' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3058' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3061' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3064' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3067' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3068' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3069' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3070' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3133' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3136' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3139' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3140' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3143' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3146' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3149' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3150' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3151' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3152' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3215' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3218' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3221' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3222' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3225' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3228' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3231' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3232' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3249' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_3252' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_3254' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3257' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3265' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_3272' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_3275' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_3277' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_3302' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_3307' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3308' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3371' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3374' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3377' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3378' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3381' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3384' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3387' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3388' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3389' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3390' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3453' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3456' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3459' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3460' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3463' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3466' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3469' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3470' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3471' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3472' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3535' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3538' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3541' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3542' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3545' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3548' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3551' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3552' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3553' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3554' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3617' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3620' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3623' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3624' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3627' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3630' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3633' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3634' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3651' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_3654' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_3656' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3659' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3667' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_3674' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_3677' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_3679' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_3715' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_3720' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3721' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3784' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3787' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3790' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3791' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3797' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3800' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3801' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3802' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3803' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3866' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3869' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3872' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3873' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3876' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3879' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3882' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3883' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3884' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3885' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3948' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3951' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3954' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3955' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3958' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3961' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3964' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3965' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3966' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3967' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4030' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4033' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4036' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4037' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4040' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4043' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4046' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4047' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4064' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_4067' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_4069' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4072' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4080' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4087' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4090' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4092' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4117' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_4122' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4123' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4186' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4189' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4192' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4193' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4196' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4199' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4202' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4203' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4204' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4205' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4268' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4271' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4274' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4275' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4278' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4281' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4284' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4285' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4286' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4287' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4350' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4353' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4356' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4357' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4360' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4363' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4366' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4367' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4368' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4369' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4432' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4435' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4438' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4439' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4442' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4445' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4448' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4449' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4466' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_4469' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_4471' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4474' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4482' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_4489' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4492' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4494' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4530' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_4535' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4536' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4599' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4602' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4605' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4606' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4609' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4612' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4615' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4616' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4617' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4618' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4681' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4684' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4687' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4688' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4691' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4694' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4697' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4698' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4699' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4700' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4763' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4766' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4769' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4770' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4773' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4776' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4779' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4780' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4781' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4782' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4845' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4848' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4851' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4852' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4855' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4858' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4861' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4862' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4879' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_4882' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_4884' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4887' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4895' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4902' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4905' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4907' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4932' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_4937' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4938' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5001' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5004' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5007' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5008' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5011' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5014' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5017' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5018' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5019' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5020' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5083' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5086' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5089' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5090' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5093' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5096' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5099' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5100' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5101' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5102' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5165' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5168' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5171' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5172' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5175' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5178' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5181' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5182' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5183' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5184' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5247' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5250' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5253' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5254' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5257' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5260' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5263' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5264' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5281' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_5284' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_5286' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5289' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5297' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_5304' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_5307' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_5309' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_5351' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_5356' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5357' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5420' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5423' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5426' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5427' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5430' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5433' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5436' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5437' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5438' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5439' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5502' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5505' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5508' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5509' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5512' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5515' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5518' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5519' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5520' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5521' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5584' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5587' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5590' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5591' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5594' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5597' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5600' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5601' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5602' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5603' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5666' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5669' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5672' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5673' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5676' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5679' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5682' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5683' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5700' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_5703' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_5705' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5708' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5716' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_5723' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_5726' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_5728' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_5737' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_5759' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_5764' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5765' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5828' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5831' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5834' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5835' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5838' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5841' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5844' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5845' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5846' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5847' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5910' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5913' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5916' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5917' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5920' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5923' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5926' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5927' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5928' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5929' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5992' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5995' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5998' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5999' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6002' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6005' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6008' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6009' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6010' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6011' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6074' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6077' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6080' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6081' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6084' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6087' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6090' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6091' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6108' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_6111' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_6113' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6116' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6124' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6131' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6134' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6136' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6172' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_6177' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6178' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6241' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6244' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6247' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6248' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6251' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6254' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6257' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6258' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6259' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6260' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6323' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6329' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6330' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6333' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6336' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6339' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6340' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6341' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6342' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6405' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6408' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6411' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6412' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6415' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6418' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6421' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6422' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6423' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6424' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6487' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6490' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6493' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6494' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6497' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6500' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6503' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6504' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6521' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_6524' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_6526' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6529' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6537' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6544' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6547' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6549' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6574' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_6579' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6580' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6643' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6646' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6649' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6650' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6653' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6656' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6659' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6660' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6661' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6662' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6725' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6728' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6731' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6732' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6735' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6738' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6741' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6742' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6743' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6744' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6807' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6810' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6813' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6814' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6817' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6820' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6823' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6824' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6825' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6826' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6889' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6892' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6895' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6896' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6899' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6902' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6905' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6906' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6923' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_6926' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_6928' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6931' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6939' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6946' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6949' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6951' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6987' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_6992' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6993' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7059' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7062' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7063' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7066' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7069' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7072' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7073' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7074' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7075' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7138' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7141' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7144' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7145' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7148' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7151' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7154' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7155' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7156' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7157' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7220' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7223' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7226' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7227' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7230' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7233' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7236' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7237' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7238' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7239' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7302' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7305' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7308' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7309' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7312' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7315' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7318' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7319' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7336' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_7339' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_7341' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7344' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7352' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_7359' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_7362' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_7364' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_7389' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_7394' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7395' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7458' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7461' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7464' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7465' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7468' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7471' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7474' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7475' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7476' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7477' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7540' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7543' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7546' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7547' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7550' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7553' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7556' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7557' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7558' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7559' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7622' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7625' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7628' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7629' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7632' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7635' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7638' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7639' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7640' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7641' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7704' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7707' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7710' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7711' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7714' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7717' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7720' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7721' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7738' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_7741' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_7743' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7746' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7754' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_7761' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_7764' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_7766' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_7802' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_7807' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7808' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7871' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7874' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7877' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7878' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7881' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7884' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7887' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7888' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7889' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7890' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7953' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7956' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7959' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7960' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7963' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7966' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7969' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7970' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7971' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7972' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8035' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8038' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8041' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8042' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8045' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8048' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8051' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8052' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8053' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8054' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8117' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8120' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8123' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8124' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8127' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8130' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8133' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8134' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8151' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_8154' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_8156' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8159' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8167' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8174' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8177' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8179' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_8204' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_8209' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8210' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8273' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8276' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8279' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8280' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8283' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8286' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8289' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8290' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8291' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8292' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8355' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8358' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8361' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8362' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8365' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8368' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8371' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8372' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8373' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8374' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8437' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8440' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8443' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8444' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8447' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8450' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8453' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8454' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8455' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8456' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8519' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8522' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8525' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8526' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8529' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8532' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8535' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8536' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8553' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_8556' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_8558' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8561' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8569' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_8576' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8579' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8581' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_8617' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_8622' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8623' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8686' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8689' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8692' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8693' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8699' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8702' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8703' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8704' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8705' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8768' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8771' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8774' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8775' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8778' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8781' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8784' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8785' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8786' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8787' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8850' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8853' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8856' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8857' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8860' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8863' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8866' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8867' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8868' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8869' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8932' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8935' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8938' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8939' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8942' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8945' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8948' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8949' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8966' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_8969' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_8971' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8974' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8982' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8989' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8992' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8994' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9019' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_9024' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9025' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9088' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9091' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9094' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9095' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9098' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9101' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9104' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9105' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9106' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9107' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9170' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9173' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9176' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9177' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9180' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9183' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9186' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9187' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9188' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9189' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9252' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9255' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9258' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9259' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9262' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9265' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9268' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9269' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9270' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9271' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9334' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9337' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9340' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9341' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9344' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9347' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9350' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9351' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9368' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_9371' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_9373' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9376' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9384' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_9391' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_9394' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_9396' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9432' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_9437' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9438' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9501' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9504' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9507' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9508' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9511' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9514' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9517' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9518' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9519' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9520' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9583' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9586' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9589' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9590' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9593' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9596' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9599' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9600' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9601' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9602' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9665' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9668' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9671' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9672' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9675' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9678' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9681' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9682' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9683' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9684' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9747' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9750' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9753' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9754' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9757' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9760' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9763' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9764' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9781' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_9784' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_9786' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9789' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9797' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_9804' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_9807' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_9809' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9834' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_9839' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9840' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9903' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9906' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9909' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9910' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9913' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9916' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9919' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9920' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9921' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9922' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9985' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9988' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9991' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9992' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9995' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9998' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10001' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10002' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10003' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10004' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10067' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10070' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10073' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10074' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10077' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10080' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10083' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10084' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10085' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10086' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10149' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10152' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10155' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10156' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10159' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10162' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10165' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10166' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10183' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_10186' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_10188' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10191' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10199' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_10206' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_10209' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_10211' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_10247' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_10252' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10253' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10316' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10319' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10322' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10323' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10329' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10332' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10333' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10334' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10335' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10398' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10401' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10404' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10405' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10408' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10411' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10414' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10415' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10416' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10417' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10480' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10483' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10486' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10487' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10490' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10493' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10496' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10497' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10498' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10499' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10562' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10565' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10568' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10569' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10572' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10575' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10578' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10579' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10596' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_10599' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_10601' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10604' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10612' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_10619' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_10622' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_10624' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_10649' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_10654' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10655' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10718' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10721' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10724' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10725' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10728' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10731' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10734' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10735' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10736' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10737' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10800' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10803' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10806' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10807' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10810' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10813' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10816' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10817' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10818' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10819' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10882' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10885' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10888' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10889' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10892' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10895' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10898' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10899' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10900' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10901' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10964' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10967' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10970' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10971' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10974' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10977' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10980' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10981' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10998' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_11001' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_11003' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11006' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11014' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11021' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11024' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11026' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11035' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_11068' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_11073' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11074' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11137' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11140' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11143' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11144' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11147' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11150' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11153' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11154' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11155' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11156' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11219' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11222' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11225' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11226' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11229' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11232' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11235' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11236' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11237' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11238' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11301' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11304' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11307' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11308' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11311' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11314' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11317' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11318' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11319' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11320' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11383' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11386' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11389' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11390' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11393' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11396' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11399' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11400' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11417' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_11420' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_11422' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11425' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11433' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11440' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11443' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11445' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11454' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_11476' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_11481' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11482' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11545' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11548' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11551' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11552' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11555' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11558' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11561' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11562' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11563' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11564' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11627' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11630' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11633' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11634' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11637' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11640' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11643' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11644' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11645' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11646' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11709' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11712' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11715' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11716' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11719' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11722' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11725' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11726' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11727' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11728' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11791' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11797' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11798' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11801' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11804' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11807' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11808' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11825' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_11828' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_11830' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11833' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11841' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11848' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11851' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11853' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11889' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_11894' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11895' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11958' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11961' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11964' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11965' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11968' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11971' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11974' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11975' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11976' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11977' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12040' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12043' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12046' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12047' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12050' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12053' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12056' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12057' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12058' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12059' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12122' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12125' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12128' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12129' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12132' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12135' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12138' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12139' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12140' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12141' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12204' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12207' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12210' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12211' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12214' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12217' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12220' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12221' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12238' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_12241' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_12243' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12246' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12254' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_12261' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_12264' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_12266' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_12291' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_12296' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12297' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12360' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12363' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12366' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12367' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12370' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12373' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12376' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12377' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12378' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12379' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12442' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12445' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12448' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12449' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12452' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12455' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12458' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12459' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12460' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12461' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12524' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12527' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12530' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12531' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12534' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12537' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12540' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12541' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12542' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12543' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12606' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12609' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12612' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12613' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12616' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12619' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12622' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12623' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12640' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_12643' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_12645' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12648' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12656' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_12663' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_12666' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_12668' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_12704' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_12709' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12710' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12773' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12776' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12779' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12780' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12783' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12786' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12789' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12790' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12791' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12792' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12855' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12858' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12861' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12862' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12865' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12868' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12871' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12872' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12873' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12874' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12937' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12940' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12943' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12944' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12947' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12950' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12953' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12954' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12955' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12956' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13019' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13022' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13025' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13026' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13029' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13032' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13035' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13036' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13053' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_13056' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_13058' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13061' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13069' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13076' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13079' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13081' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13106' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_13111' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13112' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13175' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13178' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13181' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13182' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13185' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13188' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13191' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13192' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13193' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13194' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13257' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13260' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13263' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13264' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13267' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13270' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13273' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13274' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13275' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13276' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13339' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13342' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13345' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13346' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13349' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13352' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13355' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13356' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13357' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13358' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13421' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13424' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13427' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13428' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13431' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13434' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13437' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13438' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13455' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_13458' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_13460' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13463' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13471' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_13478' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13481' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13483' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13519' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_13524' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13525' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13588' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13591' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13594' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13595' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13598' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13601' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13604' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13605' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13606' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13607' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13670' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13673' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13676' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13677' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13680' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13683' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13686' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13687' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13688' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13689' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13752' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13755' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13758' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13759' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13762' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13765' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13768' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13769' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13770' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13771' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13834' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13837' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13840' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13841' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13844' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13847' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13850' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13851' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13868' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_13871' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_13873' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13876' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13884' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13891' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13894' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13896' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13921' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_13926' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13927' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13990' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13993' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13996' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13997' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14000' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14003' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14006' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14007' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14008' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14009' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14072' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14075' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14078' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14079' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14082' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14085' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14088' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14089' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14090' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14091' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14154' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14157' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14160' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14161' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14164' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14167' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14170' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14171' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14172' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14173' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14236' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14239' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14242' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14243' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14246' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14249' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14252' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14253' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14270' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_14273' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_14275' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14278' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14286' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_14293' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_14296' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_14298' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_14334' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_14339' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14340' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14403' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14406' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14409' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14410' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14413' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14416' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14419' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14420' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14421' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14422' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14485' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14488' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14491' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14492' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14495' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14498' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14501' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14502' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14503' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14504' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14567' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14570' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14573' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14574' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14577' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14580' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14583' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14584' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14585' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14586' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14649' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14652' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14655' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14656' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14659' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14662' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14665' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14666' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14683' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_14686' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_14688' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14691' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14699' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_14706' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_14709' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_14711' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_14736' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_14741' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14742' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14805' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14808' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14811' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14812' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14815' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14818' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14821' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14822' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14823' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14824' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14887' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14890' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14893' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14894' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14897' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14900' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14903' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14904' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14905' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14906' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14969' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14972' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14975' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14976' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14979' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14982' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14985' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14986' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14987' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14988' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15051' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15054' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15057' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15058' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15061' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15064' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15067' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15068' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15085' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_15088' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_15090' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15093' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15101' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15108' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15111' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15113' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15149' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_15154' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15155' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15218' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15221' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15224' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15225' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15228' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15231' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15234' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15235' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15236' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15237' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15300' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15303' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15306' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15307' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15310' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15313' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15316' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15317' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15318' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15319' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15382' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15385' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15388' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15389' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15392' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15395' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15398' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15399' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15400' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15401' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15464' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15467' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15470' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15471' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15474' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15477' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15480' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15481' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15498' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_15501' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_15503' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15506' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15514' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15521' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15524' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15526' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15551' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_15556' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15557' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15620' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15623' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15626' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15627' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15630' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15633' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15636' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15637' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15638' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15639' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15702' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15705' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15708' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15709' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15712' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15715' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15718' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15719' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15720' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15721' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15784' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15787' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15790' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15791' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15797' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15800' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15801' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15802' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15803' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15866' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15869' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15872' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15873' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15876' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15879' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15882' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15883' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15900' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_15903' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_15905' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15908' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15916' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15923' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15926' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15928' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15937' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_15970' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_15975' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15976' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16039' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16042' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16045' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16046' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16049' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16052' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16055' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16056' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16057' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16058' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16121' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16124' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16127' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16128' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16131' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16134' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16137' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16138' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16139' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16140' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16203' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16206' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16209' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16210' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16213' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16216' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16219' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16220' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16221' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16222' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16285' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16288' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16291' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16292' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16295' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16298' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16301' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16302' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16319' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_16322' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_16324' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16327' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16335' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_16342' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_16345' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_16347' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_16356' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_16378' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_16383' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16384' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16447' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16450' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16453' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16454' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16457' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16460' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16463' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16464' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16465' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16466' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16529' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16532' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16535' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16536' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16539' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16542' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16545' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16546' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16547' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16548' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16611' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16614' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16617' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16618' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16621' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16624' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16627' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16628' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16629' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16630' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16693' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16699' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16700' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16703' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16706' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16709' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16710' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16727' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_16730' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_16732' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16735' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16743' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_16750' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_16753' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_16755' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_16791' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_16796' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16797' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16860' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16863' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16866' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16867' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16870' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16873' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16876' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16877' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16878' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16879' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16942' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16945' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16948' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16949' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16952' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16955' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16958' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16959' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16960' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16961' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17024' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17027' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17030' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17031' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17034' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17037' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17040' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17041' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17042' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17043' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17106' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17109' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17112' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17113' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17116' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17119' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17122' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17123' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17140' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_17143' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_17145' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17148' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17156' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17163' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17166' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17168' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_17193' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_17198' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17199' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17262' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17265' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17268' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17269' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17272' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17275' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17278' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17279' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17280' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17281' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17344' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17347' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17350' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17351' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17354' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17357' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17360' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17361' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17362' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17363' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17426' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17429' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17432' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17433' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17436' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17439' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17442' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17443' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17444' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17445' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17508' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17511' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17514' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17515' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17518' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17521' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17524' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17525' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17542' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_17545' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_17547' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17550' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17558' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_17565' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17568' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17570' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_17606' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_17611' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17612' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17675' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17678' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17681' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17682' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17685' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17688' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17691' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17692' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17693' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17694' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17757' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17760' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17763' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17764' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17767' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17770' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17773' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17774' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17775' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17776' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17839' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17842' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17845' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17846' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17849' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17852' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17855' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17856' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17857' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17858' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17921' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17924' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17927' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17928' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17931' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17934' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17937' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17938' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17955' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_17958' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_17960' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17963' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17971' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17978' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17981' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17983' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18008' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_18013' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18014' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18077' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18080' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18083' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18084' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18087' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18090' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18093' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18094' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18095' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18096' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18159' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18162' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18165' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18166' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18169' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18172' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18175' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18176' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18177' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18178' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18241' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18244' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18247' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18248' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18251' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18254' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18257' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18258' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18259' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18260' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18323' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18329' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18330' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18333' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18336' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18339' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18340' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18357' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_18360' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_18362' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18365' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18373' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_18380' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_18383' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_18385' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18421' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_18426' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18427' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18490' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18493' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18496' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18497' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18500' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18503' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18506' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18507' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18508' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18509' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18572' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18575' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18578' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18579' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18582' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18585' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18588' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18589' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18590' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18591' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18654' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18657' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18660' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18661' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18664' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18667' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18670' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18671' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18672' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18673' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18736' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18739' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18742' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18743' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18746' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18749' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18752' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18753' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18770' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_18773' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_18775' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18778' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18786' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_18793' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_18796' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_18798' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18823' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_18828' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18829' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18892' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18895' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18898' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18899' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18902' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18905' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18908' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18909' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18910' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18911' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18974' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18977' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18980' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18981' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18984' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18987' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18990' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18991' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18992' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18993' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19059' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19062' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19063' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19066' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19069' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19072' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19073' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19074' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19075' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19138' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19141' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19144' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19145' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19148' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19151' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19154' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19155' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19172' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_19175' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_19177' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19180' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19188' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_19195' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_19198' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_19200' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_19236' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_19241' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19242' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19305' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19308' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19311' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19312' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19315' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19318' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19321' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19322' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19323' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19324' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19387' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19390' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19393' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19394' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19397' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19400' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19403' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19404' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19405' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19406' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19469' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19472' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19475' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19476' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19479' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19482' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19485' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19486' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19487' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19488' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19551' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19554' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19557' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19558' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19561' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19564' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19567' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19568' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19585' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_19588' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_19590' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19593' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19601' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_19608' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_19611' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_19613' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_19638' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_19643' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19644' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19707' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19710' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19713' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19714' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19717' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19720' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19723' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19724' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19725' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19726' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19789' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19792' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19795' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19796' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19799' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19802' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19805' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19806' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19807' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19808' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19871' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19874' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19877' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19878' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19881' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19884' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19887' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19888' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19889' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19890' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19953' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19956' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19959' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19960' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19963' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19966' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19969' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19970' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19987' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_19990' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_19992' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19995' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20003' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20010' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20013' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20015' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20051' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_20056' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20057' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20120' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20123' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20126' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20127' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20130' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20133' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20136' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20137' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20138' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20139' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20202' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20205' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20208' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20209' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20212' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20215' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20218' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20219' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20220' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20221' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20284' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20287' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20290' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20291' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20294' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20297' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20300' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20301' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20302' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20303' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20366' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20369' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20372' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20373' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20376' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20379' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20382' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20383' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20400' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_20403' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_20405' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20408' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20416' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20423' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20426' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20428' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20453' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_20458' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20459' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20522' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20525' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20528' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20529' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20532' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20535' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20538' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20539' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20540' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20541' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20604' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20607' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20610' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20611' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20614' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20617' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20620' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20621' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20622' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20623' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20686' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20689' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20692' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20693' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20699' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20702' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20703' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20704' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20705' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20768' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20771' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20774' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20775' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20778' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20781' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20784' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20785' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20802' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_20805' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_20807' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20810' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20818' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20825' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20828' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20830' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20839' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_20872' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_20877' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20878' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20941' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20944' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20947' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20948' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20951' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20954' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20957' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20958' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20959' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20960' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21023' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21026' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21029' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21030' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21033' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21036' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21039' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21040' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21041' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_21042' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21105' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21108' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21111' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21112' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21115' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21118' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21121' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21122' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21123' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_21124' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21187' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21190' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21193' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21194' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21197' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21200' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21203' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21204' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21221' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_21224' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_21226' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_21229' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_21237' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_21244' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21247' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21249' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21258' with existing initializer 'val_5318'
[INFO] Replaced initializer 'scalar_tensor_default_1' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21324' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21339' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21342' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21344' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21358' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21375' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21378' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21380' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21383' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21391' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21398' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21401' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21403' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21417' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21434' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21437' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21439' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21442' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21450' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21457' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21460' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21462' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21476' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21493' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21496' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21498' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21501' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21509' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21516' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21519' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21521' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21529' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21532' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21534' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_2' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21558' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21575' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21578' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21580' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21583' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21591' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21598' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21601' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21603' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21617' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21634' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21637' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21639' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21642' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21650' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21657' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21660' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21662' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21676' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21693' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21696' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21698' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21701' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21709' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21716' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21719' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21721' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21735' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21752' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21755' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21757' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21760' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21768' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21775' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21778' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21780' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21788' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21791' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21793' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_3' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21817' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21834' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21837' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21839' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21842' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21850' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21857' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21860' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21862' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21876' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21893' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21896' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21898' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21901' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21909' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21916' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21919' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21921' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21935' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21952' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21955' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21957' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21960' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21968' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21975' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21978' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21980' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21994' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22011' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22014' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22016' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22019' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22027' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22034' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22037' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22039' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22047' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22050' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22052' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_4' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22076' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22093' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22096' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22098' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22101' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22109' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22116' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22119' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22121' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22135' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22152' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22155' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22157' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22160' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22168' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22175' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22178' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22180' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22194' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22211' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22214' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22216' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22219' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22227' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22234' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22237' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22239' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22253' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22270' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22273' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22275' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22278' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22286' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22293' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22296' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22298' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22306' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22309' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22311' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22317' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_22320' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_22324' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22327' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_22334' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22335' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22338' with existing initializer 'val_22331'
[INFO] Replaced initializer 'val_22341' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22344' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22345' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22360' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22361' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22364' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22365' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22500' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22501' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22504' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22505' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22510' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22518' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_22621' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22636' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22637' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22640' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22641' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22646' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22654' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_576' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_22757' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22772' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22773' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22776' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22777' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22782' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22790' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_578' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_579' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_22885' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_22891' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22901' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22906' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22911' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22991' with existing initializer 'val_22983'
[INFO] Replaced initializer 'val_22994' with existing initializer 'val_22992'
[INFO] Replaced initializer 'mul_582' with existing initializer 'mul_581'
[INFO] Replaced initializer 'val_23037' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_23060' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23063' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_23064' with existing initializer 'val_28'
[INFO] Replaced initializer 'scalar_tensor_default_25' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_23110' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23111' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23114' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23115' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23120' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23128' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_23223' with existing initializer 'val_22478'
[INFO] Replaced initializer 'val_23229' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23244' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23245' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23248' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23249' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23254' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23262' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_23357' with existing initializer 'val_22615'
[INFO] Replaced initializer 'val_23363' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23378' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23379' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23382' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23383' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23388' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23396' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_590' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_591' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_23491' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_23497' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23512' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23513' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23516' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23517' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23522' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23530' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_593' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_594' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_23625' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_23631' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23636' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23641' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23646' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23651' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23654' with existing initializer 'val_22915'
[INFO] Replaced initializer 'val_23657' with existing initializer 'val_22919'
[INFO] Replaced initializer 'val_23660' with existing initializer 'val_22923'
[INFO] Replaced initializer 'val_23663' with existing initializer 'val_22927'
[INFO] Replaced initializer 'val_23666' with existing initializer 'val_22931'
[INFO] Replaced initializer 'view_445' with existing initializer 'view_405'
[INFO] Replaced initializer 'val_23714' with existing initializer 'val_22983'
[INFO] Replaced initializer 'view_446' with existing initializer 'view_406'
[INFO] Replaced initializer 'val_23722' with existing initializer 'val_22983'
[INFO] Replaced initializer 'val_23723' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23725' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23730' with existing initializer 'val_22999'
[INFO] Replaced initializer 'mul_596' with existing initializer 'mul_581'
[INFO] Replaced initializer 'mul_597' with existing initializer 'mul_581'
[INFO] Replaced initializer 'val_23761' with existing initializer 'val_23031'
[INFO] Replaced initializer 'val_23767' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23786' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_23789' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23792' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_23793' with existing initializer 'val_28'
[INFO] Replaced initializer 'int64_1_cast' with existing initializer 'val_108'
[INFO] Replaced initializer 'scalar_tensor_default_46' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_23824' with existing initializer 'val_23095'
[INFO] Wrote onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.onnx
[INFO] Rebinding external data...
[INFO] External data: /home/ashim/Documents/projects/vggt/onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.onnx.data
[INFO] Sizes: ONNX=10.1 MB, DATA=4.44 GB
[INFO] Pruning model for PCD-only mode...
[INFO]   Keeping output: cat_322
[INFO]   Keeping output: view_411
[INFO]   Keeping output: view_412
[INFO]   Removing output: view_451
[INFO]   Removing output: view_452
[INFO]   Removing output: unsqueeze
[INFO] Pruned model saved: onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.onnx
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (Half precision with TF32 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd_fp16.engine (2.20 GB)
[INFO] Shared ONNX for subsequent builds: onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.onnx
[INFO] 
[INFO] ======================================================================
[INFO] Building BF16 variant (2/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, bf16 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: PyTorch dynamic INT8 on Linear/GRU/LSTM layers
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (BFloat16 precision)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled BF16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd_bf16.engine (2.21 GB)
[INFO] 
[INFO] ======================================================================
[INFO] Building FP8 variant (3/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp8 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: PyTorch dynamic INT8 on Linear/GRU/LSTM layers
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (FP8 precision (RTX 5090 limited support))...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled FP8
[WARNING] FP8 enabled: RTX 5090 has limited FP8 support. Expect 'Unsupported data type FP8' warnings for some ops. TensorRT will fallback to FP16 automatically.
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] FP8 build: Ignore 'Unsupported data type FP8' warnings - this is normal
[INFO] FP8 strict mode: temporarily disabled FP16 fallback
[INFO] FP8 strict mode: temporarily disabled TF32 fallback
[10/24/2025-13:35:21] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-13:35:21] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception unimplemented scalar type!
[10/24/2025-13:35:21] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-13:35:21] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-13:35:21] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-13:36:07] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-13:36:07] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-13:36:07] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-13:36:09] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-13:36:09] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-13:36:09] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[INFO] Strict FP8 build succeeded without fallback precisions: FP16, TF32 disabled
[INFO] Engine saved: onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd_fp8.engine (4.36 GB)
[INFO] 
[INFO] ======================================================================
[INFO] Building INT8 variant (4/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, int8 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: PyTorch dynamic INT8 on Linear/GRU/LSTM layers
[INFO] INT8 calibration: source=random Gaussian batches=1 seed=1337 gpu=True
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
/home/ashim/Documents/projects/vggt/onnx/vggt_to_trt_chatgpt.py:1691: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.
  config.int8_calibrator = calibrator
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/torch-int8-dynamic/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-13:38:52] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-13:38:52] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-13:38:52] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-13:38:52] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-13:38:52] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-13:39:35] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-13:39:35] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-13:39:35] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-13:39:35] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-13:39:35] [TRT] [W] Using rough estimation for the inference time of the engine.
[INFO] INT8 calibrator exhausted after 1 batches
[INFO] Calibration cache saved to 'onnx_exports/torch-int8-dynamic/calibration-8x518x518.cache'
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23058_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26129_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12646_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9645_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9761_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15653_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25804_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_66_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18765_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.depth_head.norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 24318) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_210_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_212_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22923_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18771_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18773_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10072_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14795_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25181_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_27060_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_334_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_336_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14474_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25325_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23487_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13483_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14905_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_502_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24386_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18631_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12194_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_626_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_628_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_650_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24388_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23805_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13937_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11469_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_772_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_774_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_794_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12219_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10501_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26261_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26553_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11042_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24408_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24410_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12333_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_918_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_942_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22494_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18657_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25952_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1064_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1066_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25928_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1086_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16486_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22633_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19061_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19063_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1210_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1212_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1234_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9759_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25658_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13935_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26139_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17485_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19085_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19087_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22496_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1378_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1380_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10932_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10909_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21661_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10613_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15338_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor mul_575_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16080_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1502_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1524_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1526_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26877_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23491_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17346_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11788_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18343_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14053_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1650_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1670_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1672_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15332_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19195_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19197_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13052_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17048_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19203_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19201_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14366_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15769_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1794_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1816_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1818_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14341_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24264_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13185_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16192_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21659_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor select_203, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1942_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_27022_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1962_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1964_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13507_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9214_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor view_394_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18225_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25806_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2086_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2108_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2110_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22635_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22517_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26875_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor mul_581_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor select_202, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2234_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13075_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15224_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23062_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17367_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11338_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21771_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2378_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2380_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2400_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24534_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26285_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11898_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24532_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2524_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2526_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2546_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2548_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11477_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25303_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23915_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23064_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2670_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2672_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2692_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2694_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19491_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25471_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11900_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2816_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2818_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2838_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2840_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19516_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19514_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9193_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13621_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15630_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2962_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2964_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2984_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2986_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26429_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14047_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3108_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3110_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3130_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3132_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.token_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24556_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24554_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25305_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19624_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19626_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22200_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3276_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3278_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23354_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19630_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19632_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15340_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16057_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10182_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3400_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3422_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3424_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16621_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25950_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3546_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3548_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3568_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3570_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor expand_8_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor val_25_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3606_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor add_49, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor _to_copy_1_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor cat_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3610_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3612_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9753_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23807_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14770_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13193_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3720_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3722_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3726_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3728_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor val_19_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor select_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor val_1675_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3733_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3735_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 3984) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor clamp, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor cos_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3736_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor sin_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3737_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25473_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16511_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9326_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor select_2, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3805_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3807_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4053) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor clamp_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3808_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3809_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_24231_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14909_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12764_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10049_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9755_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17798_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26723_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9324_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22925_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10611_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4028_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4030_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor view_61, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4052_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17479_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17056_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16059_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4162_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4164_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4168_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4170_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor select_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4175_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4177_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4407) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor clamp_4, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4178_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16938_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10051_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11790_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor select_6, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4247_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4249_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4473) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor clamp_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4250_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4251_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19943_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19945_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22090_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18202_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25506_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24262_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4470_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4472_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4495_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22629_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9643_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24678_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24680_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4603_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4605_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4609_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4611_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25514_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20053_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11765_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20055_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20059_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20061_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26575_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25636_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11475_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22206_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26555_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11361_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8895_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26721_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8897_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24702_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24700_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4899_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4901_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4924_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26283_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9332_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.depth_head.norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18341_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14343_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10184_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24213_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5032_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5034_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5038_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5040_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17908_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12756_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9330_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25784_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12762_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15761_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26131_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9622_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26076_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14772_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25930_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5328_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5330_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5351_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5353_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25782_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23781_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5461_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5463_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5467_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5469_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23377_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23923_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10074_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26074_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18227_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17344_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26577_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13505_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16627_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16488_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20349_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20351_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23783_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17912_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20372_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20374_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor view_403_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5757_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5759_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5780_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5782_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15222_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9216_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13623_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5890_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5892_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor view_402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5896_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5898_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26098_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10619_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18655_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21775_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26096_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10478_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26965_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10930_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 23008) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15763_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20484_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25157_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16082_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20488_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26263_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20490_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26409_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17773_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11048_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor val_21804_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6186_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_27012_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6209_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6211_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22946_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26967_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11471_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25660_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_27010_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6319_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6321_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6325_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22065_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24824_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23921_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24826_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22519_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11906_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14476_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24846_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24848_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13481_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17483_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6640_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16915_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14045_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17906_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21777_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6748_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6750_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6754_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6756_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10907_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14051_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23056_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11336_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18633_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20780_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20778_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22627_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18204_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10503_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9620_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20801_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20803_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7044_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7046_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7067_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7069_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 23703) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24879_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor val_21545_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24881_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24889_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25159_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17477_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24887_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12196_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7177_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7183_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7185_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26699_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12623_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16625_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16619_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11040_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18337_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13913_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20913_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20919_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22198_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14364_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7473_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7475_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7497_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7499_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15628_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23375_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.token_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7607_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7609_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7613_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11359_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17914_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21636_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16196_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25449_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 24933) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor val_22063_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8901_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11904_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7905_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7927_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7929_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15201_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22204_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12758_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8037_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8039_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8043_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8045_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22948_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26407_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13191_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13187_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9191_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22067_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24235_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26431_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25512_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23485_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21207_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21209_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8333_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24211_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17050_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21230_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor val_21286_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15199_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8466_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8468_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor view_395_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8472_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8474_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17775_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25013_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25011_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11046_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25033_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16509_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25035_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26701_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13077_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14793_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21340_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21342_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21346_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21348_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16198_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8762_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8764_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8785_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8787_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15334_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12329_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25451_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12625_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12217_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26137_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor select_193, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21769_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23352_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17369_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24237_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15651_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-13:45:38] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[INFO] Engine saved: onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd_int8.engine (2.18 GB)
[INFO] 
[INFO] ======================================================================
[INFO] BUILD COMPLETE!
[INFO] ======================================================================
[INFO] Input shape: [8, 3, 518, 518]
[INFO] Mode: PCD-only (depth + camera heads)
[INFO] 
[INFO] Built engines:
[INFO]   FP16  (Half precision with TF32 fallback       ): onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd_fp16.engine
[INFO]         Size: 2.20 GB
[INFO]   BF16  (BFloat16 precision                      ): onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd_bf16.engine
[INFO]         Size: 2.21 GB
[INFO]   FP8   (FP8 precision (RTX 5090 limited support)): onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd_fp8.engine
[INFO]         Size: 4.36 GB
[INFO]   INT8  (INT8 quantization with FP16 fallback    ): onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd_int8.engine
[INFO]         Size: 2.18 GB
[INFO] 
[INFO] Next steps for live PCD construction:
[INFO]   1. Use FP16 for best speed/quality balance on RTX 5090
[INFO]   2. Extract outputs: cat_322 (camera), view_411/412 (depth)
[INFO]   3. Unproject depth to 3D using camera parameters
[INFO]   4. Expected latency: ~170ms FP16, ~230ms BF16 (from your benchmark)
[INFO]   5. With --pcd-only: expect ~30% faster (~120ms FP16)
[INFO] 
[INFO] Performance hierarchy (your RTX 5090):
[INFO]   FP16:  170ms (5.9 FPS)  RECOMMENDED for live PCD
[INFO]   BF16:  232ms (4.3 FPS)
[INFO]   FP8:   173ms (5.8 FPS) - similar to FP16, limited HW support
[INFO]   INT8:  TBD (needs proper calibration data)
[INFO] 
[INFO] Optimizations applied:
[INFO]    Timing cache (speeds up rebuilds)
[INFO]    Optimization level 5 (maximum)
[INFO]    Auxiliary streams: 4
[INFO]    TF32 enabled (Ampere+ GPUs)
[INFO]    All CUDA tactic sources (cuBLAS, cuDNN)
[INFO]    Workspace: 32 GB
[INFO]    FIXED: INT8 calibrator (TRT 10+ compatible)
[INFO]    FIXED: Opset handling (onnxsim now works)
[INFO] ======================================================================
Applied 1260 of general pattern rewrite rules.

--- Building extra precision fp32 for quant mode torch-int8-dynamic ---

>>> python onnx/vggt_to_trt_chatgpt.py --onnx-in onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.onnx --precision fp32 --quant-mode torch-int8-dynamic --output-dir onnx_exports/torch-int8-dynamic --num-cams 8 --height 518 --width 518 --model-name facebook/VGGT-1B --calib-batches 1 --calib-seed 1337 --pcd-only
[INFO] 
[INFO] ======================================================================
[INFO] Building FP32 variant (1/1)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp32 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: PyTorch dynamic INT8 on Linear/GRU/LSTM layers
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (Full precision (baseline))...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.engine (4.34 GB)
[INFO] 
[INFO] ======================================================================
[INFO] BUILD COMPLETE!
[INFO] ======================================================================
[INFO] Input shape: [8, 3, 518, 518]
[INFO] Mode: PCD-only (depth + camera heads)
[INFO] 
[INFO] Built engines:
[INFO]   FP32  (Full precision (baseline)               ): onnx_exports/torch-int8-dynamic/vggt-8x3x518x518-pcd.engine
[INFO]         Size: 4.34 GB
[INFO] 
[INFO] Next steps for live PCD construction:
[INFO]   1. Use FP16 for best speed/quality balance on RTX 5090
[INFO]   2. Extract outputs: cat_322 (camera), view_411/412 (depth)
[INFO]   3. Unproject depth to 3D using camera parameters
[INFO]   4. Expected latency: ~170ms FP16, ~230ms BF16 (from your benchmark)
[INFO]   5. With --pcd-only: expect ~30% faster (~120ms FP16)
[INFO] 
[INFO] Performance hierarchy (your RTX 5090):
[INFO]   FP16:  170ms (5.9 FPS)  RECOMMENDED for live PCD
[INFO]   BF16:  232ms (4.3 FPS)
[INFO]   FP8:   173ms (5.8 FPS) - similar to FP16, limited HW support
[INFO]   INT8:  TBD (needs proper calibration data)
[INFO] 
[INFO] Optimizations applied:
[INFO]    Timing cache (speeds up rebuilds)
[INFO]    Optimization level 5 (maximum)
[INFO]    Auxiliary streams: 4
[INFO]    TF32 enabled (Ampere+ GPUs)
[INFO]    All CUDA tactic sources (cuBLAS, cuDNN)
[INFO]    Workspace: 32 GB
[INFO]    FIXED: INT8 calibrator (TRT 10+ compatible)
[INFO]    FIXED: Opset handling (onnxsim now works)
[INFO] ======================================================================

================================================================
Quantisation mode: bitsandbytes-8bit
Output directory : onnx_exports/bitsandbytes-8bit
================================================================
[INFO] Missing core precisions for quant mode 'bitsandbytes-8bit': fp16 bf16 fp8 int8

>>> python onnx/vggt_to_trt_chatgpt.py --export --all-precisions --quant-mode bitsandbytes-8bit --output-dir onnx_exports/bitsandbytes-8bit --num-cams 8 --height 518 --width 518 --model-name facebook/VGGT-1B --calib-batches 1 --calib-seed 1337 --pcd-only
[INFO] Building all precision variants: fp16, bf16, fp8, int8
[INFO] 
[INFO] ======================================================================
[INFO] Building FP16 variant (1/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp16 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear8bitLt replacement
[INFO] Requested quantisation mode 'bitsandbytes-8bit' is not ONNX-compatible with ONNX export; skipping.
[INFO] Fallback (2/4) quantisation mode 'bitsandbytes-nf4' is not ONNX-compatible with ONNX export; skipping.
[INFO] Fallback (3/4) quantisation mode 'bitsandbytes-fp4' is not ONNX-compatible with ONNX export; skipping.
[INFO] Retrying export with fallback quantisation mode 'none' (4/4)
[INFO] using MLP layer as FFN
[INFO] Modifying model for PCD-only export...
[WARNING] PCD-only mode: will prune unused outputs after export
[INFO] Exporting to ONNX with shape [8, 3, 518, 518] on cuda:0 (quant='none')
/home/ashim/miniconda3/envs/compvis/lib/python3.10/site-packages/vggt/models/vggt.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
[INFO] Removed 851 unused nodes
[INFO] No unused functions to remove
[INFO] Skipping constant folding for node 'node_unsqueeze' because it is graph input to preserve graph signature
[INFO] Skipping constant folding for node Node(name='node_cat_3', domain='', op_type='Concat', inputs=(Value(name='expand_4', type=Tensor(FLOAT), shape=Shape([1, 1, 4, 1024]), producer='node_Constant_23972', index=0, const_value={Tensor(...)}), Value(name='expand_5', type=Tensor(FLOAT), shape=Shape([1, 7, 4, 1024]), producer='node_Constant_23994', index=0, const_value={Tensor(...)})), attributes={'axis': Attr('axis', INT, 1)}, overload='', outputs=(SymbolicTensor(name='cat_3', type=Tensor(FLOAT), shape=Shape([1, 8, 4, 1024]), producer='node_cat_3', index=0),), version=13, doc_string=None) due to large input sizes: [4096, 28672]
[INFO] Skipping constant folding for node Node(name='node_clone_74', domain='', op_type='Identity', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='clone_74', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_clone_74', index=0),), version=16, doc_string=None) due to large input sizes: [21904]
[INFO] Skipping constant folding for node Node(name='node_add_49', domain='', op_type='Add', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_25', type=Tensor(INT64), shape=Shape([]), producer='node_Constant_25', index=0, const_value={Tensor<INT64,[]>(array(1), name=None)})), attributes={}, overload='', outputs=(SymbolicTensor(name='add_49', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_add_49', index=0),), version=14, doc_string=None) due to large input sizes: [21904, 1]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20794', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21278', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_20794', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20797', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21281', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20797', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20803', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21291', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20803', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20845', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21333', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20845', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20847', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21337', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20847', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20856', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21346', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20856', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20858', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21350', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20858', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20900', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21392', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20900', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20902', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21396', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20902', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20911', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21405', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20911', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20913', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21409', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20913', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20955', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21451', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20955', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20957', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21455', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20957', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20966', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21464', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20966', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20968', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21468', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20968', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21010', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21510', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21010', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21012', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21514', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21012', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21021', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21523', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21021', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21023', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21527', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21023', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21032', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21536', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21032', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21034', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21538', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21034', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21037', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21541', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21037', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21042', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21550', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21042', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21084', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21592', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21084', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21086', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21596', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21086', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21095', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21605', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21095', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21097', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21609', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21097', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21139', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21651', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21139', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21141', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21655', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21141', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21150', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21664', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21150', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21152', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21668', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21152', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21194', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21710', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21194', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21196', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21714', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21196', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21205', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21723', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21205', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21207', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21727', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21207', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21249', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21769', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21249', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21251', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21773', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21251', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21260', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21782', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21260', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21262', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21786', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21262', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21271', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21795', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21271', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21273', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21797', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21273', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21276', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21800', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21276', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21281', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21809', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21281', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21323', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21851', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21323', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21325', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21855', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21325', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21334', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21864', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21334', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21336', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21868', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21336', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21378', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21910', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21378', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21380', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21914', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21380', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21389', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21923', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21389', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21391', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21927', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21391', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21433', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21969', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21433', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21435', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21973', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21435', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21444', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21982', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21444', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21446', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21986', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21446', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21488', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22028', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21488', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21490', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22032', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21490', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21499', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22041', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21499', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21501', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22045', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21501', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21510', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22054', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21510', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21512', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22056', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21512', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21515', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22059', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21515', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21520', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22068', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21520', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21562', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22110', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21562', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21564', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22114', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21564', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21573', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22123', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21573', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21575', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22127', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21575', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21617', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22169', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21617', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21619', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22173', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21619', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21628', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22182', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21628', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21630', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22186', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21630', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21672', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22228', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21672', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21674', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22232', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21674', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21683', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22241', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21683', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21685', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22245', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21685', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21727', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22287', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21727', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21729', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22291', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21729', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21738', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22300', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21738', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21740', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22304', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21740', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21749', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22313', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21749', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_sin_1', domain='', op_type='Sin', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_1', domain='', op_type='Cos', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_2', domain='', op_type='Sin', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_2', domain='', op_type='Cos', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_3', domain='', op_type='Sin', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_3', domain='', op_type='Cos', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_4', domain='', op_type='Sin', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_4', domain='', op_type='Cos', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_5 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_5', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_5', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_6 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_6', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_6', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_7 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_7', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_7', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_8 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_8', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_8', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_22 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_23 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22421', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_23', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_23', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22992', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22420', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22992')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22993', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22421', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22423', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_22', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_22', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22994', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22422', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22994')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22995', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22423', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_sin_11', domain='', op_type='Sin', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_11', domain='', op_type='Cos', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_12', domain='', op_type='Sin', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_12', domain='', op_type='Cos', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_13', domain='', op_type='Sin', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_13', domain='', op_type='Cos', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_14', domain='', op_type='Sin', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_14', domain='', op_type='Cos', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_15 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_15', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_15', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_16 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_16', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_16', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_17 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_17', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_17', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_18 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_18', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_18', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_37 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_38 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23144', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_38', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_38', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23723', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23143', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23723')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23724', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23144', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23146', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_37', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_37', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23725', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23145', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23725')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23726', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23146', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] Removed 19332 unused nodes
[INFO] No unused functions to remove
[INFO] Skipping constant folding for node 'node_unsqueeze' because it is graph input to preserve graph signature
[INFO] Skipping constant folding for node Node(name='node_cat_3', domain='', op_type='Concat', inputs=(Value(name='expand_4', type=Tensor(FLOAT), shape=Shape([1, 1, 4, 1024]), producer='node_Constant_23972', index=0, const_value={Tensor(...)}), Value(name='expand_5', type=Tensor(FLOAT), shape=Shape([1, 7, 4, 1024]), producer='node_Constant_23994', index=0, const_value={Tensor(...)})), attributes={'axis': Attr('axis', INT, 1)}, overload='', outputs=(SymbolicTensor(name='cat_3', type=Tensor(FLOAT), shape=Shape([1, 8, 4, 1024]), producer='node_cat_3', index=0),), version=13, doc_string=None) due to large input sizes: [4096, 28672]
[INFO] Skipping constant folding for node Node(name='node_add_49', domain='', op_type='Add', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_25', type=Tensor(INT64), shape=Shape([]), producer='node_Constant_25', index=0, const_value={Tensor<INT64,[]>(array(1), name=None)})), attributes={}, overload='', outputs=(SymbolicTensor(name='add_49', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_add_49', index=0),), version=14, doc_string=None) due to large input sizes: [21904, 1]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20794', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21278', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_20794', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20797', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21281', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20797', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20803', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21291', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20803', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20845', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21333', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20845', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20847', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21337', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20847', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20856', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21346', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20856', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20858', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21350', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20858', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20900', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21392', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20900', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20902', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21396', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20902', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20911', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21405', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20911', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20913', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21409', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20913', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20955', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21451', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20955', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20957', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21455', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20957', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20966', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21464', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20966', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20968', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21468', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20968', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21010', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21510', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21010', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21012', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21514', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21012', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21021', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21523', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21021', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21023', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21527', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21023', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21032', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21536', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21032', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21034', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21538', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21034', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21037', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21541', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21037', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21042', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21550', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21042', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21084', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21592', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21084', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21086', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21596', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21086', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21095', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21605', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21095', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21097', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21609', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21097', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21139', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21651', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21139', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21141', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21655', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21141', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21150', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21664', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21150', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21152', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21668', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21152', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21194', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21710', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21194', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21196', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21714', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21196', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21205', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21723', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21205', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21207', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21727', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21207', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21249', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21769', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21249', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21251', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21773', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21251', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21260', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21782', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21260', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21262', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21786', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21262', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21271', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21795', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21271', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21273', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21797', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21273', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21276', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21800', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21276', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21281', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21809', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21281', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21323', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21851', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21323', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21325', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21855', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21325', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21334', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21864', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21334', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21336', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21868', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21336', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21378', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21910', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21378', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21380', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21914', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21380', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21389', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21923', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21389', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21391', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21927', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21391', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21433', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21969', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21433', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21435', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21973', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21435', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21444', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21982', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21444', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21446', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21986', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21446', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21488', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22028', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21488', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21490', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22032', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21490', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21499', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22041', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21499', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21501', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22045', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21501', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21510', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22054', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21510', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21512', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22056', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21512', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21515', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22059', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21515', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21520', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22068', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21520', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21562', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22110', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21562', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21564', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22114', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21564', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21573', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22123', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21573', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21575', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22127', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21575', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21617', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22169', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21617', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21619', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22173', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21619', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21628', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22182', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21628', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21630', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22186', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21630', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21672', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22228', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21672', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21674', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22232', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21674', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21683', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22241', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21683', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21685', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22245', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21685', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21727', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22287', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21727', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21729', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22291', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21729', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21738', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22300', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21738', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21740', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22304', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21740', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21749', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22313', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21749', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_sin_1', domain='', op_type='Sin', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_1', domain='', op_type='Cos', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_2', domain='', op_type='Sin', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_2', domain='', op_type='Cos', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_3', domain='', op_type='Sin', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_3', domain='', op_type='Cos', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_4', domain='', op_type='Sin', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_4', domain='', op_type='Cos', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_5 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_5', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_5', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_6 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_6', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_6', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_7 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_7', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_7', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_8 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_8', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_8', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_22 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_23 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22421', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_23', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_23', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22992', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22420', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22992')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22993', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22421', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22423', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_22', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_22', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22994', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22422', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22994')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22995', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22423', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_sin_11', domain='', op_type='Sin', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_11', domain='', op_type='Cos', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_12', domain='', op_type='Sin', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_12', domain='', op_type='Cos', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_13', domain='', op_type='Sin', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_13', domain='', op_type='Cos', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_14', domain='', op_type='Sin', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_14', domain='', op_type='Cos', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_15 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_15', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_15', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_16 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_16', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_16', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_17 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_17', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_17', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_18 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_18', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_18', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_37 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_38 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23144', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_38', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_38', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23723', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23143', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23723')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23724', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23144', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23146', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_37', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_37', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23725', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23145', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23725')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23726', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23146', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] Removed 1189 unused nodes
[INFO] No unused functions to remove
[INFO] Replaced initializer 'val_2878' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_2889' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_3291' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_3693' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_3704' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_4106' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_4508' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4519' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_4921' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_5329' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_5340' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_5748' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6150' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6161' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_6563' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6965' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6976' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_7378' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_7780' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_7791' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_8193' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_8595' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8606' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_9008' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_9410' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_9421' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_9823' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_10225' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_10236' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_10638' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11046' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11057' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_11465' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11867' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11878' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_12280' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_12682' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_12693' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_13095' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_13497' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13508' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_13910' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_14312' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_14323' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_14725' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15127' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15138' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_15540' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15948' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15959' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_16367' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_16769' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_16780' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_17182' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_17584' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17595' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_17997' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_18399' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_18410' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_18812' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_19214' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_19225' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_19627' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20029' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20040' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_20442' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20850' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20861' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_24' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_31' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_32' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_46' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_50' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_51' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_90' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_98' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_124' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_141' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_144' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_146' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_149' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_157' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_164' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_167' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_169' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_183' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_200' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_203' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_205' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_208' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_216' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_223' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_226' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_228' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_242' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_259' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_262' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_264' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_267' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_275' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_282' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_285' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_287' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_301' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_318' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_321' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_323' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_326' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_334' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_341' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_344' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_346' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_360' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_377' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_380' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_382' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_385' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_393' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_400' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_403' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_405' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_419' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_436' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_439' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_441' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_444' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_452' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_459' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_462' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_464' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_478' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_495' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_498' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_500' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_503' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_511' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_518' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_521' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_523' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_537' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_554' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_557' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_559' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_562' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_570' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_577' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_580' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_582' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_596' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_613' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_616' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_618' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_621' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_629' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_636' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_639' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_641' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_655' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_672' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_675' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_677' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_680' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_688' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_695' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_698' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_700' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_714' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_731' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_734' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_736' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_739' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_747' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_754' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_757' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_759' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_773' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_790' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_793' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_795' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_798' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_806' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_813' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_816' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_818' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_832' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_849' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_852' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_854' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_857' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_865' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_872' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_875' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_877' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_891' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_908' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_911' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_913' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_916' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_924' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_931' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_934' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_936' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_950' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_967' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_970' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_972' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_975' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_983' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_990' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_993' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_995' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1009' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1026' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1029' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1031' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1034' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1042' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1049' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1052' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1054' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1068' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1085' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1088' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1090' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1093' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1101' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1108' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1111' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1113' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1127' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1144' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1147' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1149' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1152' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1160' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1167' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1170' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1172' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1186' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1203' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1206' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1208' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1211' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1219' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1226' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1229' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1231' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1245' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1262' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1265' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1267' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1270' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1278' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1285' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1288' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1290' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1304' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1321' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1324' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1326' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1329' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1337' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1344' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1347' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1349' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1363' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1380' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1383' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1385' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1388' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1396' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1403' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1406' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1408' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1422' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1439' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1442' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1444' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1447' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1455' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1462' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1465' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1467' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1483' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1486' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1487' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1662' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1673' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1686' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1740' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1748' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1751' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1754' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1757' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1758' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1759' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1760' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1823' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1826' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1829' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1830' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1833' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1836' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1839' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1840' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1841' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1842' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1905' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1908' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1911' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1912' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1915' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1918' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1921' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1922' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1923' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1924' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1987' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1990' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1993' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1994' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1997' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2000' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2003' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2004' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2021' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_2024' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_2026' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2029' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2037' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_2044' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2047' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2049' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2090' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2091' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2154' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2157' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2160' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2161' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2164' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2167' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2170' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2171' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2172' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2173' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2236' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2239' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2242' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2243' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2246' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2249' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2252' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2253' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2254' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2255' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2318' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2321' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2324' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2325' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2328' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2331' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2334' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2335' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2336' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2337' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2400' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2403' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2406' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2407' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2410' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2413' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2416' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2417' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2439' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2442' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2450' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_2457' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2460' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2462' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2487' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_2492' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2493' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2556' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2559' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2562' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2563' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2566' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2569' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2572' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2573' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2574' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2575' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2638' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2641' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2644' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2645' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2648' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2651' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2654' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2655' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2656' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2657' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2720' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2723' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2726' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2727' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2730' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2733' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2736' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2737' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2738' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2739' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2802' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2805' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2808' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2809' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2812' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2815' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2818' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2819' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2836' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_2839' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_2841' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2844' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2852' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_2859' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2862' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2864' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2900' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_2905' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2906' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2969' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2972' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2975' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2976' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2979' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2982' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2985' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2986' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2987' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2988' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3051' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3054' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3057' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3058' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3061' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3064' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3067' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3068' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3069' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3070' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3133' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3136' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3139' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3140' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3143' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3146' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3149' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3150' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3151' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3152' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3215' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3218' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3221' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3222' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3225' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3228' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3231' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3232' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3249' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_3252' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_3254' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3257' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3265' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_3272' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_3275' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_3277' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_3302' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_3307' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3308' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3371' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3374' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3377' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3378' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3381' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3384' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3387' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3388' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3389' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3390' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3453' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3456' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3459' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3460' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3463' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3466' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3469' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3470' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3471' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3472' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3535' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3538' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3541' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3542' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3545' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3548' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3551' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3552' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3553' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3554' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3617' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3620' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3623' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3624' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3627' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3630' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3633' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3634' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3651' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_3654' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_3656' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3659' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3667' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_3674' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_3677' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_3679' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_3715' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_3720' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3721' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3784' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3787' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3790' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3791' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3797' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3800' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3801' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3802' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3803' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3866' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3869' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3872' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3873' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3876' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3879' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3882' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3883' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3884' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3885' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3948' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3951' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3954' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3955' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3958' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3961' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3964' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3965' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3966' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3967' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4030' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4033' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4036' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4037' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4040' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4043' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4046' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4047' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4064' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_4067' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_4069' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4072' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4080' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4087' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4090' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4092' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4117' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_4122' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4123' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4186' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4189' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4192' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4193' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4196' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4199' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4202' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4203' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4204' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4205' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4268' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4271' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4274' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4275' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4278' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4281' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4284' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4285' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4286' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4287' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4350' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4353' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4356' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4357' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4360' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4363' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4366' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4367' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4368' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4369' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4432' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4435' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4438' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4439' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4442' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4445' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4448' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4449' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4466' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_4469' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_4471' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4474' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4482' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_4489' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4492' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4494' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4530' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_4535' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4536' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4599' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4602' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4605' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4606' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4609' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4612' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4615' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4616' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4617' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4618' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4681' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4684' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4687' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4688' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4691' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4694' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4697' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4698' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4699' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4700' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4763' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4766' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4769' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4770' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4773' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4776' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4779' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4780' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4781' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4782' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4845' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4848' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4851' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4852' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4855' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4858' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4861' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4862' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4879' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_4882' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_4884' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4887' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4895' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4902' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4905' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4907' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4932' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_4937' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4938' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5001' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5004' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5007' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5008' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5011' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5014' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5017' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5018' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5019' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5020' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5083' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5086' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5089' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5090' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5093' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5096' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5099' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5100' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5101' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5102' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5165' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5168' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5171' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5172' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5175' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5178' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5181' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5182' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5183' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5184' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5247' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5250' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5253' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5254' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5257' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5260' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5263' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5264' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5281' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_5284' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_5286' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5289' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5297' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_5304' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_5307' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_5309' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_5351' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_5356' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5357' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5420' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5423' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5426' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5427' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5430' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5433' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5436' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5437' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5438' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5439' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5502' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5505' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5508' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5509' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5512' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5515' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5518' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5519' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5520' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5521' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5584' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5587' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5590' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5591' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5594' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5597' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5600' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5601' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5602' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5603' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5666' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5669' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5672' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5673' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5676' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5679' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5682' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5683' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5700' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_5703' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_5705' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5708' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5716' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_5723' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_5726' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_5728' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_5737' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_5759' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_5764' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5765' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5828' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5831' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5834' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5835' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5838' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5841' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5844' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5845' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5846' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5847' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5910' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5913' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5916' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5917' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5920' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5923' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5926' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5927' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5928' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5929' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5992' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5995' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5998' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5999' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6002' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6005' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6008' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6009' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6010' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6011' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6074' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6077' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6080' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6081' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6084' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6087' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6090' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6091' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6108' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_6111' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_6113' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6116' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6124' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6131' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6134' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6136' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6172' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_6177' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6178' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6241' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6244' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6247' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6248' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6251' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6254' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6257' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6258' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6259' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6260' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6323' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6329' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6330' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6333' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6336' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6339' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6340' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6341' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6342' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6405' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6408' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6411' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6412' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6415' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6418' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6421' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6422' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6423' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6424' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6487' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6490' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6493' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6494' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6497' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6500' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6503' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6504' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6521' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_6524' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_6526' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6529' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6537' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6544' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6547' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6549' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6574' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_6579' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6580' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6643' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6646' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6649' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6650' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6653' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6656' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6659' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6660' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6661' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6662' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6725' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6728' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6731' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6732' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6735' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6738' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6741' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6742' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6743' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6744' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6807' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6810' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6813' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6814' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6817' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6820' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6823' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6824' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6825' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6826' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6889' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6892' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6895' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6896' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6899' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6902' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6905' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6906' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6923' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_6926' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_6928' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6931' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6939' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6946' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6949' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6951' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6987' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_6992' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6993' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7059' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7062' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7063' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7066' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7069' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7072' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7073' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7074' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7075' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7138' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7141' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7144' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7145' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7148' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7151' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7154' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7155' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7156' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7157' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7220' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7223' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7226' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7227' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7230' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7233' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7236' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7237' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7238' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7239' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7302' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7305' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7308' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7309' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7312' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7315' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7318' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7319' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7336' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_7339' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_7341' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7344' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7352' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_7359' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_7362' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_7364' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_7389' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_7394' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7395' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7458' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7461' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7464' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7465' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7468' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7471' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7474' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7475' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7476' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7477' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7540' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7543' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7546' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7547' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7550' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7553' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7556' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7557' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7558' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7559' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7622' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7625' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7628' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7629' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7632' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7635' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7638' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7639' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7640' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7641' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7704' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7707' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7710' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7711' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7714' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7717' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7720' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7721' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7738' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_7741' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_7743' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7746' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7754' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_7761' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_7764' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_7766' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_7802' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_7807' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7808' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7871' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7874' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7877' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7878' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7881' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7884' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7887' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7888' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7889' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7890' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7953' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7956' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7959' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7960' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7963' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7966' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7969' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7970' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7971' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7972' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8035' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8038' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8041' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8042' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8045' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8048' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8051' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8052' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8053' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8054' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8117' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8120' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8123' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8124' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8127' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8130' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8133' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8134' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8151' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_8154' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_8156' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8159' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8167' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8174' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8177' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8179' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_8204' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_8209' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8210' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8273' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8276' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8279' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8280' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8283' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8286' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8289' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8290' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8291' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8292' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8355' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8358' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8361' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8362' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8365' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8368' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8371' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8372' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8373' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8374' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8437' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8440' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8443' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8444' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8447' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8450' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8453' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8454' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8455' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8456' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8519' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8522' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8525' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8526' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8529' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8532' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8535' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8536' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8553' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_8556' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_8558' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8561' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8569' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_8576' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8579' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8581' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_8617' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_8622' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8623' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8686' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8689' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8692' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8693' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8699' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8702' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8703' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8704' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8705' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8768' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8771' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8774' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8775' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8778' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8781' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8784' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8785' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8786' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8787' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8850' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8853' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8856' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8857' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8860' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8863' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8866' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8867' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8868' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8869' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8932' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8935' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8938' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8939' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8942' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8945' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8948' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8949' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8966' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_8969' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_8971' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8974' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8982' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8989' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8992' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8994' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9019' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_9024' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9025' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9088' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9091' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9094' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9095' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9098' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9101' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9104' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9105' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9106' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9107' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9170' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9173' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9176' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9177' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9180' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9183' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9186' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9187' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9188' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9189' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9252' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9255' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9258' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9259' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9262' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9265' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9268' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9269' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9270' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9271' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9334' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9337' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9340' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9341' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9344' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9347' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9350' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9351' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9368' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_9371' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_9373' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9376' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9384' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_9391' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_9394' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_9396' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9432' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_9437' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9438' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9501' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9504' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9507' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9508' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9511' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9514' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9517' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9518' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9519' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9520' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9583' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9586' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9589' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9590' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9593' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9596' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9599' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9600' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9601' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9602' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9665' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9668' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9671' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9672' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9675' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9678' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9681' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9682' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9683' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9684' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9747' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9750' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9753' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9754' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9757' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9760' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9763' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9764' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9781' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_9784' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_9786' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9789' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9797' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_9804' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_9807' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_9809' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9834' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_9839' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9840' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9903' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9906' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9909' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9910' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9913' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9916' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9919' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9920' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9921' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9922' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9985' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9988' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9991' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9992' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9995' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9998' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10001' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10002' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10003' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10004' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10067' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10070' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10073' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10074' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10077' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10080' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10083' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10084' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10085' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10086' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10149' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10152' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10155' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10156' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10159' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10162' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10165' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10166' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10183' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_10186' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_10188' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10191' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10199' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_10206' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_10209' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_10211' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_10247' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_10252' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10253' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10316' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10319' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10322' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10323' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10329' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10332' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10333' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10334' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10335' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10398' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10401' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10404' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10405' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10408' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10411' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10414' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10415' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10416' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10417' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10480' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10483' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10486' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10487' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10490' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10493' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10496' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10497' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10498' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10499' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10562' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10565' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10568' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10569' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10572' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10575' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10578' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10579' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10596' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_10599' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_10601' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10604' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10612' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_10619' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_10622' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_10624' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_10649' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_10654' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10655' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10718' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10721' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10724' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10725' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10728' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10731' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10734' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10735' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10736' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10737' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10800' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10803' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10806' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10807' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10810' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10813' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10816' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10817' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10818' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10819' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10882' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10885' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10888' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10889' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10892' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10895' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10898' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10899' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10900' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10901' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10964' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10967' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10970' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10971' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10974' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10977' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10980' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10981' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10998' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_11001' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_11003' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11006' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11014' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11021' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11024' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11026' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11035' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_11068' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_11073' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11074' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11137' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11140' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11143' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11144' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11147' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11150' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11153' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11154' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11155' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11156' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11219' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11222' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11225' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11226' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11229' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11232' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11235' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11236' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11237' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11238' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11301' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11304' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11307' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11308' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11311' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11314' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11317' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11318' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11319' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11320' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11383' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11386' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11389' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11390' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11393' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11396' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11399' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11400' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11417' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_11420' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_11422' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11425' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11433' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11440' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11443' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11445' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11454' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_11476' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_11481' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11482' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11545' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11548' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11551' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11552' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11555' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11558' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11561' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11562' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11563' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11564' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11627' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11630' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11633' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11634' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11637' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11640' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11643' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11644' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11645' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11646' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11709' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11712' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11715' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11716' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11719' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11722' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11725' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11726' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11727' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11728' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11791' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11797' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11798' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11801' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11804' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11807' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11808' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11825' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_11828' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_11830' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11833' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11841' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11848' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11851' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11853' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11889' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_11894' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11895' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11958' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11961' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11964' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11965' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11968' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11971' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11974' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11975' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11976' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11977' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12040' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12043' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12046' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12047' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12050' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12053' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12056' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12057' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12058' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12059' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12122' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12125' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12128' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12129' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12132' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12135' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12138' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12139' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12140' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12141' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12204' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12207' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12210' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12211' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12214' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12217' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12220' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12221' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12238' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_12241' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_12243' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12246' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12254' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_12261' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_12264' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_12266' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_12291' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_12296' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12297' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12360' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12363' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12366' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12367' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12370' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12373' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12376' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12377' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12378' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12379' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12442' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12445' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12448' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12449' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12452' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12455' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12458' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12459' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12460' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12461' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12524' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12527' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12530' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12531' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12534' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12537' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12540' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12541' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12542' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12543' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12606' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12609' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12612' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12613' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12616' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12619' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12622' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12623' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12640' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_12643' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_12645' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12648' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12656' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_12663' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_12666' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_12668' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_12704' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_12709' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12710' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12773' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12776' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12779' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12780' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12783' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12786' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12789' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12790' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12791' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12792' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12855' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12858' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12861' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12862' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12865' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12868' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12871' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12872' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12873' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12874' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12937' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12940' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12943' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12944' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12947' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12950' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12953' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12954' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12955' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12956' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13019' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13022' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13025' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13026' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13029' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13032' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13035' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13036' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13053' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_13056' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_13058' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13061' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13069' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13076' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13079' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13081' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13106' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_13111' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13112' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13175' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13178' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13181' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13182' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13185' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13188' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13191' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13192' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13193' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13194' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13257' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13260' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13263' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13264' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13267' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13270' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13273' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13274' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13275' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13276' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13339' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13342' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13345' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13346' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13349' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13352' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13355' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13356' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13357' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13358' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13421' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13424' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13427' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13428' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13431' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13434' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13437' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13438' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13455' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_13458' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_13460' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13463' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13471' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_13478' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13481' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13483' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13519' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_13524' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13525' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13588' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13591' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13594' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13595' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13598' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13601' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13604' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13605' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13606' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13607' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13670' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13673' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13676' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13677' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13680' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13683' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13686' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13687' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13688' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13689' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13752' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13755' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13758' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13759' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13762' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13765' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13768' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13769' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13770' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13771' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13834' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13837' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13840' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13841' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13844' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13847' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13850' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13851' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13868' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_13871' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_13873' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13876' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13884' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13891' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13894' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13896' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13921' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_13926' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13927' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13990' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13993' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13996' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13997' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14000' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14003' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14006' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14007' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14008' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14009' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14072' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14075' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14078' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14079' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14082' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14085' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14088' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14089' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14090' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14091' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14154' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14157' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14160' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14161' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14164' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14167' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14170' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14171' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14172' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14173' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14236' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14239' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14242' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14243' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14246' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14249' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14252' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14253' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14270' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_14273' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_14275' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14278' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14286' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_14293' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_14296' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_14298' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_14334' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_14339' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14340' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14403' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14406' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14409' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14410' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14413' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14416' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14419' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14420' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14421' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14422' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14485' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14488' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14491' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14492' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14495' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14498' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14501' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14502' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14503' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14504' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14567' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14570' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14573' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14574' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14577' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14580' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14583' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14584' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14585' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14586' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14649' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14652' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14655' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14656' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14659' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14662' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14665' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14666' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14683' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_14686' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_14688' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14691' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14699' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_14706' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_14709' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_14711' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_14736' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_14741' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14742' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14805' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14808' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14811' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14812' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14815' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14818' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14821' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14822' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14823' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14824' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14887' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14890' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14893' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14894' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14897' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14900' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14903' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14904' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14905' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14906' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14969' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14972' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14975' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14976' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14979' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14982' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14985' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14986' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14987' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14988' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15051' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15054' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15057' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15058' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15061' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15064' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15067' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15068' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15085' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_15088' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_15090' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15093' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15101' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15108' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15111' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15113' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15149' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_15154' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15155' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15218' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15221' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15224' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15225' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15228' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15231' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15234' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15235' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15236' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15237' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15300' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15303' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15306' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15307' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15310' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15313' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15316' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15317' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15318' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15319' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15382' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15385' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15388' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15389' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15392' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15395' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15398' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15399' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15400' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15401' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15464' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15467' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15470' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15471' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15474' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15477' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15480' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15481' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15498' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_15501' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_15503' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15506' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15514' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15521' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15524' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15526' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15551' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_15556' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15557' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15620' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15623' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15626' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15627' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15630' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15633' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15636' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15637' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15638' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15639' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15702' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15705' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15708' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15709' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15712' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15715' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15718' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15719' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15720' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15721' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15784' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15787' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15790' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15791' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15797' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15800' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15801' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15802' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15803' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15866' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15869' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15872' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15873' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15876' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15879' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15882' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15883' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15900' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_15903' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_15905' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15908' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15916' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15923' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15926' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15928' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15937' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_15970' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_15975' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15976' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16039' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16042' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16045' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16046' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16049' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16052' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16055' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16056' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16057' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16058' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16121' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16124' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16127' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16128' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16131' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16134' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16137' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16138' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16139' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16140' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16203' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16206' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16209' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16210' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16213' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16216' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16219' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16220' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16221' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16222' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16285' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16288' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16291' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16292' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16295' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16298' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16301' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16302' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16319' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_16322' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_16324' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16327' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16335' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_16342' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_16345' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_16347' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_16356' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_16378' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_16383' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16384' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16447' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16450' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16453' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16454' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16457' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16460' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16463' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16464' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16465' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16466' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16529' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16532' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16535' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16536' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16539' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16542' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16545' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16546' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16547' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16548' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16611' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16614' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16617' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16618' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16621' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16624' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16627' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16628' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16629' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16630' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16693' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16699' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16700' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16703' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16706' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16709' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16710' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16727' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_16730' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_16732' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16735' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16743' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_16750' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_16753' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_16755' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_16791' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_16796' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16797' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16860' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16863' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16866' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16867' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16870' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16873' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16876' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16877' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16878' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16879' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16942' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16945' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16948' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16949' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16952' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16955' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16958' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16959' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16960' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16961' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17024' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17027' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17030' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17031' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17034' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17037' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17040' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17041' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17042' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17043' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17106' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17109' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17112' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17113' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17116' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17119' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17122' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17123' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17140' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_17143' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_17145' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17148' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17156' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17163' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17166' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17168' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_17193' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_17198' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17199' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17262' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17265' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17268' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17269' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17272' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17275' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17278' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17279' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17280' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17281' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17344' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17347' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17350' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17351' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17354' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17357' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17360' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17361' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17362' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17363' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17426' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17429' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17432' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17433' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17436' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17439' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17442' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17443' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17444' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17445' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17508' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17511' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17514' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17515' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17518' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17521' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17524' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17525' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17542' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_17545' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_17547' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17550' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17558' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_17565' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17568' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17570' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_17606' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_17611' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17612' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17675' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17678' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17681' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17682' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17685' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17688' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17691' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17692' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17693' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17694' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17757' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17760' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17763' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17764' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17767' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17770' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17773' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17774' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17775' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17776' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17839' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17842' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17845' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17846' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17849' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17852' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17855' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17856' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17857' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17858' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17921' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17924' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17927' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17928' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17931' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17934' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17937' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17938' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17955' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_17958' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_17960' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17963' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17971' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17978' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17981' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17983' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18008' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_18013' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18014' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18077' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18080' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18083' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18084' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18087' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18090' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18093' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18094' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18095' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18096' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18159' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18162' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18165' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18166' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18169' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18172' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18175' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18176' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18177' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18178' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18241' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18244' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18247' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18248' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18251' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18254' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18257' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18258' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18259' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18260' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18323' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18329' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18330' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18333' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18336' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18339' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18340' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18357' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_18360' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_18362' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18365' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18373' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_18380' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_18383' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_18385' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18421' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_18426' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18427' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18490' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18493' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18496' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18497' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18500' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18503' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18506' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18507' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18508' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18509' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18572' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18575' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18578' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18579' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18582' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18585' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18588' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18589' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18590' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18591' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18654' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18657' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18660' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18661' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18664' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18667' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18670' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18671' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18672' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18673' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18736' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18739' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18742' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18743' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18746' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18749' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18752' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18753' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18770' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_18773' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_18775' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18778' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18786' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_18793' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_18796' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_18798' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18823' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_18828' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18829' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18892' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18895' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18898' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18899' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18902' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18905' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18908' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18909' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18910' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18911' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18974' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18977' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18980' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18981' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18984' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18987' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18990' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18991' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18992' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18993' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19059' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19062' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19063' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19066' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19069' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19072' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19073' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19074' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19075' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19138' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19141' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19144' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19145' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19148' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19151' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19154' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19155' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19172' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_19175' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_19177' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19180' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19188' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_19195' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_19198' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_19200' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_19236' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_19241' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19242' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19305' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19308' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19311' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19312' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19315' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19318' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19321' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19322' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19323' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19324' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19387' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19390' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19393' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19394' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19397' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19400' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19403' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19404' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19405' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19406' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19469' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19472' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19475' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19476' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19479' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19482' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19485' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19486' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19487' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19488' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19551' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19554' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19557' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19558' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19561' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19564' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19567' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19568' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19585' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_19588' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_19590' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19593' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19601' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_19608' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_19611' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_19613' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_19638' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_19643' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19644' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19707' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19710' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19713' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19714' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19717' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19720' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19723' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19724' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19725' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19726' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19789' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19792' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19795' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19796' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19799' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19802' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19805' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19806' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19807' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19808' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19871' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19874' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19877' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19878' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19881' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19884' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19887' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19888' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19889' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19890' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19953' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19956' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19959' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19960' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19963' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19966' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19969' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19970' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19987' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_19990' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_19992' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19995' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20003' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20010' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20013' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20015' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20051' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_20056' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20057' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20120' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20123' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20126' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20127' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20130' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20133' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20136' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20137' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20138' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20139' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20202' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20205' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20208' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20209' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20212' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20215' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20218' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20219' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20220' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20221' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20284' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20287' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20290' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20291' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20294' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20297' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20300' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20301' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20302' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20303' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20366' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20369' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20372' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20373' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20376' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20379' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20382' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20383' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20400' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_20403' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_20405' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20408' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20416' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20423' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20426' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20428' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20453' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_20458' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20459' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20522' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20525' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20528' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20529' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20532' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20535' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20538' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20539' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20540' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20541' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20604' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20607' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20610' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20611' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20614' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20617' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20620' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20621' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20622' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20623' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20686' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20689' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20692' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20693' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20699' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20702' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20703' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20704' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20705' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20768' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20771' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20774' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20775' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20778' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20781' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20784' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20785' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20802' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_20805' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_20807' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20810' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20818' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20825' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20828' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20830' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20839' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_20872' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_20877' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20878' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20941' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20944' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20947' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20948' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20951' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20954' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20957' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20958' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20959' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20960' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21023' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21026' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21029' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21030' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21033' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21036' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21039' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21040' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21041' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_21042' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21105' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21108' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21111' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21112' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21115' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21118' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21121' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21122' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21123' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_21124' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21187' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21190' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21193' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21194' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21197' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21200' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21203' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21204' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21221' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_21224' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_21226' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_21229' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_21237' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_21244' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21247' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21249' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21258' with existing initializer 'val_5318'
[INFO] Replaced initializer 'scalar_tensor_default_1' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21324' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21339' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21342' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21344' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21358' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21375' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21378' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21380' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21383' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21391' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21398' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21401' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21403' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21417' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21434' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21437' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21439' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21442' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21450' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21457' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21460' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21462' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21476' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21493' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21496' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21498' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21501' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21509' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21516' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21519' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21521' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21529' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21532' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21534' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_2' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21558' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21575' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21578' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21580' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21583' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21591' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21598' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21601' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21603' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21617' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21634' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21637' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21639' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21642' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21650' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21657' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21660' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21662' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21676' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21693' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21696' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21698' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21701' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21709' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21716' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21719' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21721' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21735' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21752' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21755' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21757' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21760' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21768' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21775' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21778' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21780' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21788' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21791' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21793' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_3' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21817' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21834' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21837' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21839' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21842' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21850' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21857' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21860' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21862' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21876' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21893' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21896' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21898' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21901' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21909' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21916' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21919' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21921' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21935' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21952' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21955' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21957' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21960' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21968' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21975' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21978' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21980' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21994' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22011' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22014' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22016' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22019' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22027' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22034' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22037' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22039' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22047' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22050' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22052' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_4' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22076' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22093' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22096' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22098' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22101' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22109' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22116' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22119' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22121' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22135' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22152' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22155' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22157' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22160' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22168' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22175' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22178' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22180' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22194' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22211' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22214' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22216' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22219' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22227' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22234' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22237' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22239' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22253' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22270' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22273' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22275' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22278' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22286' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22293' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22296' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22298' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22306' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22309' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22311' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22317' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_22320' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_22324' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22327' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_22334' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22335' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22338' with existing initializer 'val_22331'
[INFO] Replaced initializer 'val_22341' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22344' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22345' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22360' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22361' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22364' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22365' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22500' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22501' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22504' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22505' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22510' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22518' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_22621' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22636' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22637' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22640' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22641' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22646' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22654' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_576' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_22757' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22772' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22773' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22776' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22777' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22782' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22790' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_578' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_579' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_22885' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_22891' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22901' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22906' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22911' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22991' with existing initializer 'val_22983'
[INFO] Replaced initializer 'val_22994' with existing initializer 'val_22992'
[INFO] Replaced initializer 'mul_582' with existing initializer 'mul_581'
[INFO] Replaced initializer 'val_23037' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_23060' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23063' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_23064' with existing initializer 'val_28'
[INFO] Replaced initializer 'scalar_tensor_default_25' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_23110' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23111' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23114' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23115' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23120' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23128' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_23223' with existing initializer 'val_22478'
[INFO] Replaced initializer 'val_23229' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23244' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23245' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23248' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23249' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23254' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23262' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_23357' with existing initializer 'val_22615'
[INFO] Replaced initializer 'val_23363' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23378' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23379' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23382' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23383' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23388' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23396' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_590' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_591' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_23491' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_23497' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23512' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23513' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23516' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23517' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23522' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23530' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_593' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_594' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_23625' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_23631' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23636' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23641' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23646' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23651' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23654' with existing initializer 'val_22915'
[INFO] Replaced initializer 'val_23657' with existing initializer 'val_22919'
[INFO] Replaced initializer 'val_23660' with existing initializer 'val_22923'
[INFO] Replaced initializer 'val_23663' with existing initializer 'val_22927'
[INFO] Replaced initializer 'val_23666' with existing initializer 'val_22931'
[INFO] Replaced initializer 'view_445' with existing initializer 'view_405'
[INFO] Replaced initializer 'val_23714' with existing initializer 'val_22983'
[INFO] Replaced initializer 'view_446' with existing initializer 'view_406'
[INFO] Replaced initializer 'val_23722' with existing initializer 'val_22983'
[INFO] Replaced initializer 'val_23723' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23725' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23730' with existing initializer 'val_22999'
[INFO] Replaced initializer 'mul_596' with existing initializer 'mul_581'
[INFO] Replaced initializer 'mul_597' with existing initializer 'mul_581'
[INFO] Replaced initializer 'val_23761' with existing initializer 'val_23031'
[INFO] Replaced initializer 'val_23767' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23786' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_23789' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23792' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_23793' with existing initializer 'val_28'
[INFO] Replaced initializer 'int64_1_cast' with existing initializer 'val_108'
[INFO] Replaced initializer 'scalar_tensor_default_46' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_23824' with existing initializer 'val_23095'
[INFO] Wrote onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.onnx
[INFO] Rebinding external data...
[INFO] External data: /home/ashim/Documents/projects/vggt/onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.onnx.data
[INFO] Sizes: ONNX=10.1 MB, DATA=4.44 GB
[INFO] Pruning model for PCD-only mode...
[INFO]   Keeping output: cat_322
[INFO]   Keeping output: view_411
[INFO]   Keeping output: view_412
[INFO]   Removing output: view_451
[INFO]   Removing output: view_452
[INFO]   Removing output: unsqueeze
[INFO] Pruned model saved: onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.onnx
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (Half precision with TF32 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd_fp16.engine (2.20 GB)
[INFO] Shared ONNX for subsequent builds: onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.onnx
[INFO] 
[INFO] ======================================================================
[INFO] Building BF16 variant (2/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, bf16 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear8bitLt replacement
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (BFloat16 precision)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled BF16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd_bf16.engine (2.20 GB)
[INFO] 
[INFO] ======================================================================
[INFO] Building FP8 variant (3/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp8 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear8bitLt replacement
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (FP8 precision (RTX 5090 limited support))...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled FP8
[WARNING] FP8 enabled: RTX 5090 has limited FP8 support. Expect 'Unsupported data type FP8' warnings for some ops. TensorRT will fallback to FP16 automatically.
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] FP8 build: Ignore 'Unsupported data type FP8' warnings - this is normal
[INFO] FP8 strict mode: temporarily disabled FP16 fallback
[INFO] FP8 strict mode: temporarily disabled TF32 fallback
[10/24/2025-14:28:42] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-14:28:42] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception unimplemented scalar type!
[10/24/2025-14:28:42] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-14:28:42] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-14:28:42] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-14:29:25] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-14:29:25] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-14:29:25] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-14:29:27] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-14:29:27] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-14:29:27] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[INFO] Strict FP8 build succeeded without fallback precisions: FP16, TF32 disabled
[INFO] Engine saved: onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd_fp8.engine (4.36 GB)
[INFO] 
[INFO] ======================================================================
[INFO] Building INT8 variant (4/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, int8 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear8bitLt replacement
[INFO] INT8 calibration: source=random Gaussian batches=1 seed=1337 gpu=True
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
/home/ashim/Documents/projects/vggt/onnx/vggt_to_trt_chatgpt.py:1691: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.
  config.int8_calibrator = calibrator
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/bitsandbytes-8bit/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-14:32:02] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-14:32:02] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-14:32:02] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-14:32:02] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-14:32:02] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-14:32:44] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-14:32:44] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-14:32:44] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-14:32:44] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-14:32:44] [TRT] [W] Using rough estimation for the inference time of the engine.
[INFO] INT8 calibrator exhausted after 1 batches
[INFO] Calibration cache saved to 'onnx_exports/bitsandbytes-8bit/calibration-8x518x518.cache'
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24408_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26877_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8895_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_66_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12646_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24410_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17346_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14053_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12196_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10613_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18204_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_210_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_212_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25451_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15630_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25514_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_27060_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_334_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_336_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23062_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13052_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24386_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_502_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15199_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10501_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_626_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_628_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_650_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9761_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_772_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_774_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_794_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19061_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19063_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14341_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11469_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_918_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_942_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19085_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19087_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15628_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22065_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1064_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1066_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1086_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14047_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11336_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24388_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25930_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25784_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23805_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17798_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10072_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1210_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1212_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1234_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17906_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14772_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19195_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19197_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1378_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1380_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19201_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19203_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16938_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11477_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17485_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1502_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1524_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1526_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17369_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24532_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1650_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1670_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1672_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11471_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9326_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24534_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16057_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15222_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12217_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22629_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22948_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14366_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22496_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25506_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1794_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1816_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1818_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25928_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18633_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.token_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14474_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13505_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor select_203, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1942_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_27022_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1962_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1964_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23377_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24556_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24554_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10932_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9645_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2086_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2108_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2110_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9216_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8901_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21769_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16192_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor mul_581_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor select_202, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2234_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14045_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15769_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11475_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23056_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14905_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10611_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2378_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2380_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2400_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22923_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19491_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2524_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2526_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2546_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2548_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17344_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26577_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19514_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19516_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2670_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2672_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2692_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2694_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17914_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15763_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18227_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2816_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2818_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2838_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2840_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14770_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2962_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2964_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2984_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2986_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22090_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22494_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15653_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15761_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19624_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19626_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19630_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19632_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3108_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3110_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3130_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3132_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23058_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.depth_head.norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10051_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24262_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.token_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16059_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11048_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3276_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3278_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12219_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10049_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9753_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11788_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16196_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3400_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3422_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3424_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15334_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22200_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17773_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3546_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3548_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3568_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3570_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26429_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9759_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor expand_8_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor val_25_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3606_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor add_49, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor _to_copy_1_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor cat_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3610_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3612_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26431_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3720_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3722_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3726_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3728_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor val_19_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor select_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor val_1675_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3733_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3735_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 3984) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor clamp, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor cos_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3736_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor sin_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3737_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11900_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor select_2, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3805_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3807_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4053) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor clamp_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3808_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3809_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23781_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor val_21804_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12762_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13913_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24678_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24237_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25806_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23807_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor view_394_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18225_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4028_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4030_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor view_61, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4052_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14793_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16080_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19943_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19945_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4162_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4164_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4168_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4170_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor select_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4175_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4177_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4407) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor clamp_4, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4178_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24680_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24235_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21659_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_24231_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor mul_575_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor select_6, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4247_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4249_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4473) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor clamp_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4250_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4251_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10907_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26407_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24700_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24702_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4470_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4472_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4495_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20055_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20053_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20059_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20061_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26965_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14051_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17775_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21636_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23354_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25512_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4603_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4605_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4609_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4611_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17477_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18771_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13187_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14476_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25950_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9324_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18773_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4899_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4901_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4924_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26261_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18341_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26098_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26076_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13481_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26074_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13483_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5032_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5034_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5038_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5040_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14909_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16488_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24213_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10503_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10478_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12756_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25658_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12623_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5328_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5330_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5351_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5353_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21775_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25303_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13937_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9643_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20349_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20351_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5461_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5463_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5467_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5469_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10184_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18337_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14343_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13935_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20372_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20374_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26699_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15338_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12329_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12194_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26096_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17056_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor select_193, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23352_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9332_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor view_403_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5757_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5759_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5780_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5782_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11790_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25325_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21771_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16511_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5890_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5892_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor view_402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5896_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5898_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12333_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25804_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24824_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22627_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26553_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21661_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20484_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20488_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20490_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26967_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24826_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11904_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22633_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16621_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22635_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23487_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24264_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26409_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14795_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6186_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_27012_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6209_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6211_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13075_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24846_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18765_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24848_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9620_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_27010_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6319_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6321_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6325_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11046_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17479_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16625_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25159_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9193_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16198_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18343_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22925_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 24318) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18655_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23485_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 23703) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13077_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6640_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor val_21545_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24879_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9191_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13621_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24881_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24887_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24889_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15224_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11765_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23921_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6748_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6750_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18202_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6754_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6756_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16619_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22517_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26875_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17483_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22198_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26701_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11359_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20778_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20780_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15201_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23923_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26555_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16509_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20803_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25471_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20801_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7044_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7046_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7067_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7069_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25636_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12758_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11906_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15332_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13507_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26721_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17367_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11042_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7177_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7183_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7185_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25782_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13191_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25660_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10930_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16627_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13193_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17912_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20913_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12764_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20919_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26283_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7473_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7475_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11338_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7497_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7499_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18631_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15651_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11040_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26285_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22067_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7607_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7609_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7613_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21777_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17048_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10182_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16915_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26575_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor val_21286_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23064_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22946_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9755_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 24933) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16082_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22519_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11898_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor val_22063_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23375_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26131_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25952_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26129_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7905_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7927_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7929_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25473_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25305_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9622_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9214_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24211_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8037_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8039_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8043_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8045_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25013_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25011_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11361_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25181_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13623_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 23008) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21207_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21209_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23783_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14364_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25157_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12625_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21230_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8333_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13185_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22204_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26723_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23915_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8466_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8468_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor view_395_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8472_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8474_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25035_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25033_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10619_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10909_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9330_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22206_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15340_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26137_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23491_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16486_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26263_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17908_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26139_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.depth_head.norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21340_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21342_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21346_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21348_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17050_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18657_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8762_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8764_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8785_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8787_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25449_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8897_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10074_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-14:38:47] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[INFO] Engine saved: onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd_int8.engine (2.19 GB)
[INFO] 
[INFO] ======================================================================
[INFO] BUILD COMPLETE!
[INFO] ======================================================================
[INFO] Input shape: [8, 3, 518, 518]
[INFO] Mode: PCD-only (depth + camera heads)
[INFO] 
[INFO] Built engines:
[INFO]   FP16  (Half precision with TF32 fallback       ): onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd_fp16.engine
[INFO]         Size: 2.20 GB
[INFO]   BF16  (BFloat16 precision                      ): onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd_bf16.engine
[INFO]         Size: 2.20 GB
[INFO]   FP8   (FP8 precision (RTX 5090 limited support)): onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd_fp8.engine
[INFO]         Size: 4.36 GB
[INFO]   INT8  (INT8 quantization with FP16 fallback    ): onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd_int8.engine
[INFO]         Size: 2.19 GB
[INFO] 
[INFO] Next steps for live PCD construction:
[INFO]   1. Use FP16 for best speed/quality balance on RTX 5090
[INFO]   2. Extract outputs: cat_322 (camera), view_411/412 (depth)
[INFO]   3. Unproject depth to 3D using camera parameters
[INFO]   4. Expected latency: ~170ms FP16, ~230ms BF16 (from your benchmark)
[INFO]   5. With --pcd-only: expect ~30% faster (~120ms FP16)
[INFO] 
[INFO] Performance hierarchy (your RTX 5090):
[INFO]   FP16:  170ms (5.9 FPS)  RECOMMENDED for live PCD
[INFO]   BF16:  232ms (4.3 FPS)
[INFO]   FP8:   173ms (5.8 FPS) - similar to FP16, limited HW support
[INFO]   INT8:  TBD (needs proper calibration data)
[INFO] 
[INFO] Optimizations applied:
[INFO]    Timing cache (speeds up rebuilds)
[INFO]    Optimization level 5 (maximum)
[INFO]    Auxiliary streams: 4
[INFO]    TF32 enabled (Ampere+ GPUs)
[INFO]    All CUDA tactic sources (cuBLAS, cuDNN)
[INFO]    Workspace: 32 GB
[INFO]    FIXED: INT8 calibrator (TRT 10+ compatible)
[INFO]    FIXED: Opset handling (onnxsim now works)
[INFO] ======================================================================
Applied 1260 of general pattern rewrite rules.

--- Building extra precision fp32 for quant mode bitsandbytes-8bit ---

>>> python onnx/vggt_to_trt_chatgpt.py --onnx-in onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.onnx --precision fp32 --quant-mode bitsandbytes-8bit --output-dir onnx_exports/bitsandbytes-8bit --num-cams 8 --height 518 --width 518 --model-name facebook/VGGT-1B --calib-batches 1 --calib-seed 1337 --pcd-only
[INFO] 
[INFO] ======================================================================
[INFO] Building FP32 variant (1/1)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp32 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear8bitLt replacement
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (Full precision (baseline))...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.engine (4.34 GB)
[INFO] 
[INFO] ======================================================================
[INFO] BUILD COMPLETE!
[INFO] ======================================================================
[INFO] Input shape: [8, 3, 518, 518]
[INFO] Mode: PCD-only (depth + camera heads)
[INFO] 
[INFO] Built engines:
[INFO]   FP32  (Full precision (baseline)               ): onnx_exports/bitsandbytes-8bit/vggt-8x3x518x518-pcd.engine
[INFO]         Size: 4.34 GB
[INFO] 
[INFO] Next steps for live PCD construction:
[INFO]   1. Use FP16 for best speed/quality balance on RTX 5090
[INFO]   2. Extract outputs: cat_322 (camera), view_411/412 (depth)
[INFO]   3. Unproject depth to 3D using camera parameters
[INFO]   4. Expected latency: ~170ms FP16, ~230ms BF16 (from your benchmark)
[INFO]   5. With --pcd-only: expect ~30% faster (~120ms FP16)
[INFO] 
[INFO] Performance hierarchy (your RTX 5090):
[INFO]   FP16:  170ms (5.9 FPS)  RECOMMENDED for live PCD
[INFO]   BF16:  232ms (4.3 FPS)
[INFO]   FP8:   173ms (5.8 FPS) - similar to FP16, limited HW support
[INFO]   INT8:  TBD (needs proper calibration data)
[INFO] 
[INFO] Optimizations applied:
[INFO]    Timing cache (speeds up rebuilds)
[INFO]    Optimization level 5 (maximum)
[INFO]    Auxiliary streams: 4
[INFO]    TF32 enabled (Ampere+ GPUs)
[INFO]    All CUDA tactic sources (cuBLAS, cuDNN)
[INFO]    Workspace: 32 GB
[INFO]    FIXED: INT8 calibrator (TRT 10+ compatible)
[INFO]    FIXED: Opset handling (onnxsim now works)
[INFO] ======================================================================

================================================================
Quantisation mode: bitsandbytes-nf4
Output directory : onnx_exports/bitsandbytes-nf4
================================================================
[INFO] Missing core precisions for quant mode 'bitsandbytes-nf4': fp16 bf16 fp8 int8

>>> python onnx/vggt_to_trt_chatgpt.py --export --all-precisions --quant-mode bitsandbytes-nf4 --output-dir onnx_exports/bitsandbytes-nf4 --num-cams 8 --height 518 --width 518 --model-name facebook/VGGT-1B --calib-batches 1 --calib-seed 1337 --pcd-only
[INFO] Building all precision variants: fp16, bf16, fp8, int8
[INFO] 
[INFO] ======================================================================
[INFO] Building FP16 variant (1/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp16 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear4bit (NF4) weight quantisation
[INFO] Requested quantisation mode 'bitsandbytes-nf4' is not ONNX-compatible with ONNX export; skipping.
[INFO] Fallback (2/3) quantisation mode 'bitsandbytes-fp4' is not ONNX-compatible with ONNX export; skipping.
[INFO] Retrying export with fallback quantisation mode 'none' (3/3)
[INFO] using MLP layer as FFN
[INFO] Modifying model for PCD-only export...
[WARNING] PCD-only mode: will prune unused outputs after export
[INFO] Exporting to ONNX with shape [8, 3, 518, 518] on cuda:0 (quant='none')
/home/ashim/miniconda3/envs/compvis/lib/python3.10/site-packages/vggt/models/vggt.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
[INFO] Removed 851 unused nodes
[INFO] No unused functions to remove
[INFO] Skipping constant folding for node 'node_unsqueeze' because it is graph input to preserve graph signature
[INFO] Skipping constant folding for node Node(name='node_cat_3', domain='', op_type='Concat', inputs=(Value(name='expand_4', type=Tensor(FLOAT), shape=Shape([1, 1, 4, 1024]), producer='node_Constant_23972', index=0, const_value={Tensor(...)}), Value(name='expand_5', type=Tensor(FLOAT), shape=Shape([1, 7, 4, 1024]), producer='node_Constant_23994', index=0, const_value={Tensor(...)})), attributes={'axis': Attr('axis', INT, 1)}, overload='', outputs=(SymbolicTensor(name='cat_3', type=Tensor(FLOAT), shape=Shape([1, 8, 4, 1024]), producer='node_cat_3', index=0),), version=13, doc_string=None) due to large input sizes: [4096, 28672]
[INFO] Skipping constant folding for node Node(name='node_clone_74', domain='', op_type='Identity', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='clone_74', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_clone_74', index=0),), version=16, doc_string=None) due to large input sizes: [21904]
[INFO] Skipping constant folding for node Node(name='node_add_49', domain='', op_type='Add', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_25', type=Tensor(INT64), shape=Shape([]), producer='node_Constant_25', index=0, const_value={Tensor<INT64,[]>(array(1), name=None)})), attributes={}, overload='', outputs=(SymbolicTensor(name='add_49', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_add_49', index=0),), version=14, doc_string=None) due to large input sizes: [21904, 1]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20794', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21278', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_20794', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20797', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21281', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20797', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20803', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21291', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20803', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20845', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21333', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20845', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20847', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21337', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20847', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20856', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21346', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20856', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20858', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21350', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20858', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20900', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21392', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20900', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20902', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21396', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20902', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20911', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21405', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20911', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20913', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21409', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20913', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20955', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21451', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20955', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20957', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21455', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20957', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20966', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21464', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20966', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20968', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21468', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20968', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21010', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21510', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21010', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21012', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21514', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21012', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21021', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21523', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21021', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21023', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21527', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21023', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21032', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21536', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21032', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21034', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21538', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21034', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21037', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21541', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21037', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21042', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21550', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21042', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21084', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21592', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21084', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21086', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21596', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21086', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21095', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21605', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21095', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21097', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21609', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21097', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21139', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21651', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21139', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21141', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21655', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21141', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21150', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21664', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21150', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21152', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21668', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21152', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21194', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21710', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21194', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21196', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21714', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21196', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21205', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21723', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21205', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21207', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21727', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21207', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21249', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21769', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21249', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21251', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21773', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21251', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21260', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21782', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21260', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21262', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21786', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21262', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21271', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21795', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21271', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21273', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21797', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21273', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21276', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21800', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21276', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21281', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21809', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21281', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21323', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21851', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21323', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21325', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21855', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21325', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21334', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21864', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21334', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21336', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21868', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21336', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21378', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21910', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21378', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21380', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21914', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21380', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21389', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21923', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21389', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21391', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21927', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21391', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21433', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21969', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21433', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21435', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21973', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21435', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21444', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21982', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21444', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21446', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21986', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21446', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21488', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22028', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21488', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21490', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22032', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21490', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21499', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22041', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21499', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21501', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22045', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21501', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21510', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22054', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21510', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21512', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22056', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21512', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21515', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22059', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21515', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21520', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22068', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21520', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21562', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22110', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21562', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21564', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22114', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21564', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21573', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22123', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21573', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21575', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22127', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21575', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21617', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22169', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21617', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21619', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22173', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21619', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21628', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22182', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21628', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21630', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22186', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21630', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21672', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22228', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21672', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21674', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22232', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21674', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21683', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22241', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21683', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21685', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22245', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21685', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21727', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22287', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21727', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21729', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22291', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21729', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21738', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22300', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21738', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21740', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22304', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21740', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21749', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22313', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21749', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_sin_1', domain='', op_type='Sin', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_1', domain='', op_type='Cos', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_2', domain='', op_type='Sin', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_2', domain='', op_type='Cos', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_3', domain='', op_type='Sin', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_3', domain='', op_type='Cos', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_4', domain='', op_type='Sin', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_4', domain='', op_type='Cos', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_5 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_5', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_5', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_6 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_6', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_6', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_7 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_7', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_7', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_8 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_8', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_8', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_22 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_23 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22421', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_23', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_23', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22992', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22420', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22992')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22993', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22421', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22423', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_22', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_22', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22994', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22422', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22994')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22995', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22423', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_sin_11', domain='', op_type='Sin', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_11', domain='', op_type='Cos', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_12', domain='', op_type='Sin', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_12', domain='', op_type='Cos', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_13', domain='', op_type='Sin', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_13', domain='', op_type='Cos', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_14', domain='', op_type='Sin', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_14', domain='', op_type='Cos', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_15 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_15', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_15', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_16 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_16', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_16', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_17 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_17', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_17', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_18 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_18', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_18', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_37 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_38 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23144', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_38', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_38', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23723', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23143', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23723')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23724', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23144', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23146', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_37', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_37', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23725', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23145', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23725')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23726', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23146', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] Removed 19332 unused nodes
[INFO] No unused functions to remove
[INFO] Skipping constant folding for node 'node_unsqueeze' because it is graph input to preserve graph signature
[INFO] Skipping constant folding for node Node(name='node_cat_3', domain='', op_type='Concat', inputs=(Value(name='expand_4', type=Tensor(FLOAT), shape=Shape([1, 1, 4, 1024]), producer='node_Constant_23972', index=0, const_value={Tensor(...)}), Value(name='expand_5', type=Tensor(FLOAT), shape=Shape([1, 7, 4, 1024]), producer='node_Constant_23994', index=0, const_value={Tensor(...)})), attributes={'axis': Attr('axis', INT, 1)}, overload='', outputs=(SymbolicTensor(name='cat_3', type=Tensor(FLOAT), shape=Shape([1, 8, 4, 1024]), producer='node_cat_3', index=0),), version=13, doc_string=None) due to large input sizes: [4096, 28672]
[INFO] Skipping constant folding for node Node(name='node_add_49', domain='', op_type='Add', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_25', type=Tensor(INT64), shape=Shape([]), producer='node_Constant_25', index=0, const_value={Tensor<INT64,[]>(array(1), name=None)})), attributes={}, overload='', outputs=(SymbolicTensor(name='add_49', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_add_49', index=0),), version=14, doc_string=None) due to large input sizes: [21904, 1]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20794', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21278', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_20794', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20797', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21281', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20797', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20803', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21291', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20803', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20845', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21333', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20845', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20847', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21337', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20847', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20856', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21346', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20856', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20858', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21350', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20858', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20900', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21392', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20900', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20902', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21396', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20902', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20911', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21405', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20911', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20913', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21409', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20913', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20955', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21451', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20955', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20957', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21455', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20957', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20966', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21464', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20966', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20968', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21468', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20968', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21010', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21510', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21010', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21012', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21514', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21012', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21021', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21523', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21021', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21023', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21527', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21023', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21032', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21536', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21032', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21034', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21538', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21034', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21037', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21541', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21037', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21042', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21550', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21042', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21084', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21592', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21084', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21086', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21596', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21086', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21095', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21605', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21095', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21097', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21609', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21097', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21139', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21651', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21139', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21141', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21655', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21141', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21150', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21664', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21150', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21152', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21668', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21152', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21194', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21710', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21194', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21196', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21714', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21196', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21205', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21723', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21205', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21207', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21727', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21207', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21249', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21769', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21249', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21251', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21773', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21251', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21260', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21782', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21260', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21262', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21786', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21262', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21271', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21795', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21271', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21273', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21797', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21273', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21276', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21800', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21276', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21281', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21809', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21281', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21323', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21851', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21323', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21325', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21855', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21325', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21334', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21864', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21334', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21336', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21868', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21336', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21378', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21910', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21378', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21380', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21914', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21380', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21389', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21923', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21389', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21391', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21927', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21391', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21433', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21969', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21433', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21435', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21973', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21435', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21444', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21982', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21444', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21446', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21986', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21446', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21488', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22028', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21488', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21490', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22032', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21490', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21499', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22041', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21499', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21501', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22045', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21501', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21510', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22054', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21510', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21512', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22056', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21512', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21515', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22059', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21515', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21520', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22068', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21520', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21562', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22110', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21562', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21564', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22114', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21564', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21573', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22123', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21573', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21575', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22127', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21575', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21617', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22169', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21617', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21619', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22173', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21619', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21628', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22182', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21628', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21630', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22186', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21630', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21672', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22228', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21672', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21674', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22232', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21674', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21683', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22241', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21683', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21685', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22245', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21685', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21727', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22287', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21727', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21729', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22291', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21729', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21738', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22300', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21738', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21740', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22304', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21740', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21749', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22313', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21749', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_sin_1', domain='', op_type='Sin', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_1', domain='', op_type='Cos', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_2', domain='', op_type='Sin', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_2', domain='', op_type='Cos', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_3', domain='', op_type='Sin', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_3', domain='', op_type='Cos', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_4', domain='', op_type='Sin', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_4', domain='', op_type='Cos', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_5 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_5', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_5', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_6 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_6', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_6', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_7 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_7', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_7', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_8 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_8', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_8', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_22 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_23 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22421', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_23', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_23', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22992', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22420', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22992')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22993', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22421', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22423', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_22', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_22', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22994', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22422', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22994')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22995', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22423', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_sin_11', domain='', op_type='Sin', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_11', domain='', op_type='Cos', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_12', domain='', op_type='Sin', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_12', domain='', op_type='Cos', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_13', domain='', op_type='Sin', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_13', domain='', op_type='Cos', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_14', domain='', op_type='Sin', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_14', domain='', op_type='Cos', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_15 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_15', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_15', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_16 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_16', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_16', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_17 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_17', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_17', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_18 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_18', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_18', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_37 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_38 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23144', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_38', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_38', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23723', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23143', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23723')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23724', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23144', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23146', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_37', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_37', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23725', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23145', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23725')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23726', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23146', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] Removed 1189 unused nodes
[INFO] No unused functions to remove
[INFO] Replaced initializer 'val_2878' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_2889' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_3291' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_3693' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_3704' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_4106' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_4508' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4519' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_4921' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_5329' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_5340' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_5748' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6150' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6161' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_6563' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6965' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6976' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_7378' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_7780' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_7791' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_8193' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_8595' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8606' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_9008' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_9410' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_9421' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_9823' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_10225' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_10236' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_10638' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11046' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11057' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_11465' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11867' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11878' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_12280' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_12682' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_12693' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_13095' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_13497' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13508' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_13910' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_14312' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_14323' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_14725' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15127' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15138' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_15540' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15948' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15959' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_16367' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_16769' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_16780' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_17182' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_17584' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17595' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_17997' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_18399' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_18410' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_18812' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_19214' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_19225' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_19627' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20029' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20040' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_20442' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20850' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20861' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_24' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_31' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_32' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_46' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_50' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_51' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_90' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_98' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_124' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_141' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_144' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_146' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_149' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_157' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_164' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_167' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_169' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_183' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_200' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_203' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_205' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_208' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_216' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_223' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_226' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_228' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_242' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_259' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_262' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_264' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_267' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_275' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_282' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_285' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_287' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_301' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_318' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_321' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_323' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_326' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_334' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_341' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_344' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_346' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_360' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_377' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_380' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_382' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_385' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_393' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_400' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_403' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_405' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_419' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_436' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_439' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_441' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_444' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_452' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_459' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_462' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_464' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_478' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_495' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_498' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_500' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_503' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_511' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_518' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_521' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_523' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_537' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_554' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_557' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_559' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_562' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_570' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_577' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_580' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_582' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_596' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_613' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_616' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_618' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_621' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_629' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_636' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_639' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_641' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_655' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_672' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_675' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_677' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_680' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_688' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_695' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_698' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_700' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_714' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_731' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_734' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_736' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_739' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_747' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_754' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_757' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_759' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_773' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_790' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_793' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_795' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_798' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_806' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_813' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_816' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_818' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_832' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_849' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_852' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_854' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_857' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_865' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_872' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_875' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_877' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_891' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_908' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_911' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_913' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_916' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_924' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_931' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_934' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_936' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_950' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_967' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_970' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_972' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_975' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_983' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_990' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_993' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_995' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1009' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1026' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1029' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1031' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1034' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1042' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1049' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1052' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1054' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1068' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1085' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1088' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1090' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1093' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1101' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1108' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1111' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1113' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1127' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1144' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1147' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1149' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1152' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1160' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1167' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1170' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1172' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1186' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1203' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1206' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1208' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1211' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1219' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1226' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1229' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1231' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1245' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1262' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1265' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1267' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1270' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1278' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1285' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1288' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1290' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1304' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1321' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1324' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1326' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1329' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1337' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1344' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1347' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1349' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1363' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1380' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1383' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1385' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1388' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1396' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1403' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1406' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1408' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1422' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1439' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1442' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1444' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1447' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1455' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1462' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1465' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1467' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1483' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1486' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1487' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1662' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1673' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1686' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1740' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1748' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1751' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1754' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1757' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1758' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1759' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1760' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1823' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1826' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1829' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1830' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1833' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1836' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1839' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1840' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1841' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1842' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1905' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1908' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1911' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1912' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1915' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1918' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1921' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1922' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1923' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1924' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1987' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1990' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1993' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1994' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1997' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2000' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2003' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2004' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2021' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_2024' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_2026' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2029' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2037' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_2044' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2047' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2049' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2090' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2091' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2154' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2157' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2160' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2161' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2164' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2167' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2170' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2171' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2172' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2173' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2236' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2239' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2242' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2243' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2246' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2249' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2252' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2253' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2254' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2255' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2318' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2321' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2324' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2325' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2328' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2331' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2334' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2335' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2336' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2337' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2400' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2403' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2406' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2407' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2410' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2413' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2416' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2417' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2439' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2442' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2450' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_2457' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2460' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2462' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2487' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_2492' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2493' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2556' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2559' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2562' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2563' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2566' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2569' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2572' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2573' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2574' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2575' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2638' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2641' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2644' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2645' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2648' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2651' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2654' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2655' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2656' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2657' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2720' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2723' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2726' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2727' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2730' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2733' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2736' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2737' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2738' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2739' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2802' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2805' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2808' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2809' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2812' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2815' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2818' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2819' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2836' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_2839' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_2841' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2844' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2852' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_2859' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2862' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2864' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2900' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_2905' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2906' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2969' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2972' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2975' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2976' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2979' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2982' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2985' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2986' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2987' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2988' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3051' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3054' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3057' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3058' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3061' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3064' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3067' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3068' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3069' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3070' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3133' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3136' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3139' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3140' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3143' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3146' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3149' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3150' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3151' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3152' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3215' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3218' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3221' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3222' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3225' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3228' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3231' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3232' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3249' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_3252' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_3254' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3257' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3265' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_3272' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_3275' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_3277' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_3302' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_3307' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3308' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3371' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3374' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3377' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3378' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3381' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3384' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3387' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3388' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3389' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3390' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3453' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3456' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3459' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3460' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3463' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3466' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3469' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3470' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3471' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3472' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3535' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3538' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3541' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3542' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3545' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3548' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3551' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3552' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3553' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3554' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3617' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3620' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3623' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3624' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3627' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3630' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3633' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3634' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3651' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_3654' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_3656' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3659' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3667' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_3674' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_3677' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_3679' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_3715' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_3720' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3721' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3784' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3787' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3790' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3791' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3797' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3800' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3801' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3802' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3803' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3866' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3869' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3872' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3873' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3876' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3879' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3882' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3883' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3884' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3885' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3948' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3951' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3954' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3955' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3958' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3961' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3964' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3965' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3966' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3967' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4030' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4033' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4036' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4037' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4040' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4043' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4046' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4047' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4064' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_4067' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_4069' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4072' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4080' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4087' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4090' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4092' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4117' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_4122' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4123' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4186' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4189' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4192' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4193' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4196' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4199' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4202' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4203' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4204' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4205' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4268' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4271' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4274' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4275' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4278' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4281' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4284' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4285' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4286' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4287' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4350' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4353' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4356' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4357' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4360' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4363' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4366' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4367' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4368' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4369' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4432' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4435' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4438' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4439' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4442' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4445' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4448' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4449' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4466' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_4469' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_4471' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4474' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4482' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_4489' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4492' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4494' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4530' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_4535' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4536' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4599' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4602' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4605' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4606' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4609' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4612' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4615' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4616' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4617' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4618' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4681' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4684' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4687' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4688' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4691' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4694' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4697' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4698' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4699' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4700' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4763' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4766' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4769' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4770' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4773' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4776' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4779' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4780' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4781' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4782' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4845' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4848' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4851' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4852' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4855' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4858' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4861' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4862' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4879' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_4882' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_4884' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4887' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4895' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4902' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4905' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4907' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4932' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_4937' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4938' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5001' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5004' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5007' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5008' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5011' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5014' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5017' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5018' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5019' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5020' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5083' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5086' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5089' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5090' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5093' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5096' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5099' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5100' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5101' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5102' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5165' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5168' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5171' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5172' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5175' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5178' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5181' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5182' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5183' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5184' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5247' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5250' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5253' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5254' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5257' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5260' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5263' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5264' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5281' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_5284' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_5286' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5289' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5297' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_5304' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_5307' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_5309' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_5351' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_5356' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5357' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5420' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5423' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5426' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5427' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5430' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5433' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5436' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5437' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5438' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5439' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5502' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5505' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5508' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5509' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5512' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5515' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5518' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5519' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5520' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5521' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5584' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5587' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5590' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5591' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5594' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5597' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5600' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5601' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5602' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5603' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5666' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5669' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5672' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5673' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5676' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5679' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5682' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5683' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5700' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_5703' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_5705' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5708' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5716' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_5723' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_5726' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_5728' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_5737' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_5759' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_5764' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5765' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5828' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5831' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5834' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5835' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5838' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5841' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5844' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5845' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5846' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5847' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5910' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5913' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5916' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5917' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5920' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5923' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5926' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5927' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5928' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5929' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5992' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5995' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5998' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5999' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6002' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6005' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6008' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6009' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6010' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6011' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6074' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6077' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6080' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6081' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6084' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6087' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6090' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6091' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6108' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_6111' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_6113' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6116' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6124' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6131' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6134' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6136' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6172' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_6177' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6178' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6241' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6244' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6247' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6248' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6251' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6254' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6257' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6258' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6259' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6260' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6323' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6329' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6330' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6333' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6336' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6339' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6340' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6341' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6342' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6405' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6408' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6411' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6412' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6415' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6418' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6421' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6422' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6423' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6424' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6487' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6490' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6493' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6494' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6497' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6500' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6503' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6504' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6521' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_6524' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_6526' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6529' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6537' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6544' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6547' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6549' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6574' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_6579' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6580' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6643' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6646' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6649' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6650' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6653' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6656' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6659' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6660' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6661' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6662' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6725' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6728' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6731' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6732' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6735' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6738' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6741' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6742' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6743' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6744' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6807' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6810' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6813' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6814' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6817' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6820' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6823' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6824' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6825' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6826' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6889' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6892' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6895' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6896' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6899' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6902' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6905' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6906' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6923' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_6926' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_6928' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6931' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6939' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6946' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6949' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6951' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6987' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_6992' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6993' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7059' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7062' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7063' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7066' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7069' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7072' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7073' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7074' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7075' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7138' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7141' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7144' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7145' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7148' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7151' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7154' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7155' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7156' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7157' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7220' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7223' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7226' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7227' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7230' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7233' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7236' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7237' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7238' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7239' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7302' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7305' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7308' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7309' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7312' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7315' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7318' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7319' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7336' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_7339' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_7341' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7344' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7352' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_7359' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_7362' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_7364' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_7389' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_7394' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7395' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7458' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7461' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7464' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7465' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7468' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7471' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7474' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7475' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7476' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7477' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7540' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7543' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7546' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7547' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7550' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7553' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7556' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7557' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7558' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7559' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7622' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7625' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7628' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7629' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7632' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7635' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7638' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7639' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7640' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7641' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7704' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7707' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7710' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7711' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7714' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7717' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7720' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7721' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7738' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_7741' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_7743' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7746' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7754' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_7761' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_7764' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_7766' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_7802' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_7807' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7808' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7871' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7874' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7877' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7878' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7881' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7884' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7887' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7888' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7889' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7890' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7953' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7956' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7959' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7960' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7963' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7966' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7969' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7970' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7971' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7972' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8035' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8038' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8041' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8042' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8045' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8048' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8051' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8052' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8053' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8054' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8117' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8120' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8123' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8124' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8127' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8130' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8133' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8134' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8151' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_8154' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_8156' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8159' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8167' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8174' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8177' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8179' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_8204' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_8209' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8210' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8273' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8276' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8279' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8280' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8283' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8286' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8289' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8290' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8291' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8292' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8355' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8358' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8361' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8362' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8365' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8368' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8371' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8372' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8373' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8374' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8437' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8440' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8443' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8444' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8447' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8450' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8453' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8454' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8455' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8456' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8519' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8522' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8525' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8526' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8529' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8532' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8535' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8536' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8553' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_8556' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_8558' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8561' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8569' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_8576' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8579' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8581' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_8617' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_8622' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8623' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8686' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8689' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8692' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8693' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8699' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8702' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8703' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8704' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8705' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8768' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8771' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8774' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8775' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8778' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8781' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8784' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8785' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8786' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8787' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8850' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8853' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8856' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8857' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8860' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8863' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8866' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8867' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8868' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8869' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8932' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8935' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8938' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8939' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8942' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8945' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8948' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8949' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8966' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_8969' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_8971' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8974' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8982' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8989' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8992' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8994' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9019' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_9024' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9025' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9088' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9091' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9094' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9095' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9098' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9101' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9104' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9105' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9106' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9107' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9170' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9173' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9176' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9177' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9180' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9183' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9186' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9187' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9188' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9189' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9252' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9255' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9258' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9259' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9262' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9265' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9268' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9269' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9270' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9271' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9334' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9337' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9340' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9341' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9344' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9347' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9350' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9351' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9368' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_9371' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_9373' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9376' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9384' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_9391' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_9394' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_9396' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9432' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_9437' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9438' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9501' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9504' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9507' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9508' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9511' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9514' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9517' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9518' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9519' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9520' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9583' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9586' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9589' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9590' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9593' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9596' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9599' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9600' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9601' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9602' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9665' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9668' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9671' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9672' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9675' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9678' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9681' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9682' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9683' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9684' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9747' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9750' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9753' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9754' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9757' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9760' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9763' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9764' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9781' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_9784' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_9786' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9789' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9797' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_9804' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_9807' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_9809' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9834' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_9839' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9840' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9903' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9906' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9909' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9910' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9913' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9916' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9919' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9920' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9921' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9922' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9985' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9988' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9991' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9992' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9995' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9998' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10001' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10002' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10003' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10004' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10067' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10070' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10073' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10074' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10077' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10080' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10083' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10084' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10085' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10086' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10149' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10152' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10155' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10156' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10159' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10162' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10165' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10166' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10183' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_10186' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_10188' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10191' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10199' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_10206' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_10209' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_10211' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_10247' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_10252' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10253' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10316' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10319' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10322' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10323' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10329' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10332' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10333' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10334' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10335' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10398' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10401' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10404' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10405' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10408' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10411' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10414' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10415' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10416' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10417' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10480' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10483' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10486' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10487' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10490' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10493' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10496' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10497' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10498' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10499' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10562' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10565' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10568' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10569' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10572' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10575' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10578' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10579' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10596' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_10599' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_10601' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10604' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10612' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_10619' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_10622' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_10624' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_10649' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_10654' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10655' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10718' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10721' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10724' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10725' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10728' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10731' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10734' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10735' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10736' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10737' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10800' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10803' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10806' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10807' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10810' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10813' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10816' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10817' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10818' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10819' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10882' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10885' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10888' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10889' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10892' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10895' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10898' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10899' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10900' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10901' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10964' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10967' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10970' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10971' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10974' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10977' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10980' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10981' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10998' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_11001' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_11003' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11006' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11014' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11021' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11024' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11026' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11035' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_11068' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_11073' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11074' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11137' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11140' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11143' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11144' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11147' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11150' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11153' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11154' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11155' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11156' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11219' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11222' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11225' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11226' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11229' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11232' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11235' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11236' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11237' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11238' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11301' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11304' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11307' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11308' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11311' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11314' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11317' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11318' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11319' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11320' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11383' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11386' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11389' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11390' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11393' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11396' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11399' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11400' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11417' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_11420' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_11422' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11425' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11433' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11440' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11443' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11445' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11454' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_11476' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_11481' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11482' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11545' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11548' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11551' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11552' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11555' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11558' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11561' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11562' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11563' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11564' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11627' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11630' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11633' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11634' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11637' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11640' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11643' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11644' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11645' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11646' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11709' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11712' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11715' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11716' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11719' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11722' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11725' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11726' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11727' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11728' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11791' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11797' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11798' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11801' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11804' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11807' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11808' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11825' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_11828' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_11830' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11833' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11841' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11848' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11851' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11853' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11889' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_11894' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11895' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11958' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11961' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11964' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11965' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11968' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11971' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11974' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11975' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11976' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11977' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12040' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12043' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12046' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12047' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12050' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12053' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12056' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12057' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12058' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12059' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12122' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12125' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12128' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12129' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12132' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12135' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12138' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12139' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12140' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12141' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12204' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12207' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12210' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12211' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12214' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12217' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12220' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12221' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12238' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_12241' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_12243' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12246' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12254' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_12261' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_12264' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_12266' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_12291' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_12296' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12297' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12360' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12363' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12366' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12367' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12370' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12373' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12376' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12377' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12378' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12379' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12442' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12445' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12448' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12449' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12452' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12455' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12458' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12459' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12460' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12461' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12524' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12527' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12530' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12531' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12534' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12537' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12540' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12541' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12542' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12543' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12606' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12609' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12612' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12613' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12616' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12619' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12622' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12623' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12640' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_12643' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_12645' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12648' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12656' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_12663' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_12666' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_12668' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_12704' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_12709' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12710' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12773' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12776' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12779' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12780' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12783' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12786' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12789' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12790' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12791' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12792' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12855' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12858' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12861' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12862' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12865' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12868' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12871' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12872' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12873' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12874' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12937' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12940' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12943' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12944' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12947' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12950' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12953' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12954' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12955' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12956' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13019' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13022' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13025' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13026' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13029' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13032' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13035' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13036' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13053' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_13056' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_13058' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13061' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13069' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13076' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13079' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13081' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13106' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_13111' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13112' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13175' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13178' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13181' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13182' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13185' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13188' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13191' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13192' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13193' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13194' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13257' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13260' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13263' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13264' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13267' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13270' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13273' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13274' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13275' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13276' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13339' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13342' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13345' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13346' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13349' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13352' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13355' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13356' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13357' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13358' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13421' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13424' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13427' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13428' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13431' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13434' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13437' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13438' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13455' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_13458' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_13460' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13463' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13471' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_13478' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13481' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13483' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13519' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_13524' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13525' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13588' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13591' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13594' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13595' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13598' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13601' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13604' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13605' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13606' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13607' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13670' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13673' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13676' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13677' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13680' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13683' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13686' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13687' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13688' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13689' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13752' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13755' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13758' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13759' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13762' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13765' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13768' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13769' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13770' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13771' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13834' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13837' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13840' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13841' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13844' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13847' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13850' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13851' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13868' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_13871' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_13873' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13876' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13884' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13891' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13894' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13896' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13921' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_13926' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13927' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13990' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13993' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13996' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13997' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14000' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14003' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14006' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14007' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14008' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14009' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14072' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14075' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14078' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14079' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14082' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14085' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14088' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14089' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14090' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14091' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14154' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14157' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14160' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14161' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14164' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14167' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14170' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14171' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14172' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14173' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14236' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14239' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14242' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14243' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14246' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14249' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14252' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14253' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14270' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_14273' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_14275' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14278' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14286' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_14293' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_14296' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_14298' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_14334' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_14339' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14340' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14403' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14406' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14409' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14410' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14413' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14416' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14419' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14420' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14421' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14422' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14485' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14488' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14491' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14492' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14495' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14498' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14501' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14502' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14503' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14504' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14567' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14570' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14573' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14574' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14577' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14580' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14583' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14584' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14585' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14586' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14649' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14652' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14655' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14656' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14659' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14662' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14665' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14666' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14683' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_14686' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_14688' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14691' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14699' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_14706' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_14709' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_14711' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_14736' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_14741' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14742' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14805' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14808' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14811' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14812' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14815' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14818' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14821' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14822' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14823' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14824' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14887' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14890' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14893' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14894' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14897' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14900' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14903' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14904' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14905' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14906' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14969' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14972' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14975' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14976' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14979' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14982' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14985' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14986' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14987' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14988' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15051' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15054' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15057' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15058' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15061' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15064' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15067' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15068' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15085' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_15088' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_15090' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15093' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15101' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15108' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15111' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15113' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15149' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_15154' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15155' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15218' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15221' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15224' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15225' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15228' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15231' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15234' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15235' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15236' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15237' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15300' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15303' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15306' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15307' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15310' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15313' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15316' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15317' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15318' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15319' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15382' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15385' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15388' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15389' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15392' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15395' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15398' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15399' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15400' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15401' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15464' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15467' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15470' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15471' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15474' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15477' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15480' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15481' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15498' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_15501' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_15503' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15506' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15514' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15521' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15524' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15526' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15551' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_15556' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15557' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15620' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15623' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15626' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15627' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15630' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15633' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15636' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15637' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15638' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15639' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15702' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15705' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15708' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15709' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15712' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15715' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15718' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15719' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15720' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15721' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15784' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15787' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15790' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15791' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15797' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15800' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15801' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15802' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15803' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15866' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15869' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15872' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15873' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15876' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15879' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15882' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15883' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15900' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_15903' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_15905' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15908' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15916' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15923' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15926' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15928' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15937' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_15970' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_15975' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15976' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16039' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16042' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16045' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16046' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16049' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16052' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16055' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16056' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16057' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16058' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16121' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16124' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16127' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16128' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16131' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16134' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16137' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16138' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16139' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16140' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16203' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16206' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16209' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16210' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16213' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16216' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16219' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16220' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16221' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16222' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16285' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16288' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16291' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16292' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16295' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16298' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16301' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16302' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16319' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_16322' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_16324' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16327' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16335' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_16342' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_16345' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_16347' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_16356' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_16378' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_16383' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16384' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16447' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16450' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16453' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16454' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16457' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16460' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16463' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16464' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16465' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16466' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16529' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16532' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16535' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16536' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16539' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16542' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16545' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16546' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16547' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16548' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16611' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16614' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16617' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16618' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16621' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16624' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16627' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16628' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16629' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16630' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16693' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16699' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16700' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16703' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16706' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16709' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16710' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16727' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_16730' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_16732' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16735' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16743' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_16750' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_16753' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_16755' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_16791' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_16796' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16797' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16860' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16863' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16866' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16867' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16870' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16873' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16876' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16877' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16878' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16879' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16942' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16945' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16948' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16949' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16952' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16955' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16958' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16959' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16960' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16961' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17024' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17027' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17030' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17031' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17034' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17037' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17040' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17041' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17042' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17043' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17106' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17109' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17112' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17113' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17116' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17119' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17122' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17123' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17140' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_17143' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_17145' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17148' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17156' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17163' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17166' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17168' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_17193' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_17198' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17199' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17262' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17265' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17268' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17269' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17272' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17275' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17278' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17279' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17280' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17281' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17344' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17347' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17350' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17351' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17354' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17357' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17360' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17361' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17362' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17363' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17426' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17429' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17432' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17433' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17436' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17439' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17442' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17443' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17444' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17445' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17508' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17511' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17514' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17515' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17518' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17521' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17524' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17525' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17542' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_17545' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_17547' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17550' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17558' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_17565' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17568' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17570' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_17606' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_17611' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17612' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17675' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17678' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17681' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17682' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17685' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17688' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17691' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17692' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17693' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17694' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17757' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17760' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17763' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17764' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17767' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17770' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17773' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17774' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17775' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17776' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17839' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17842' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17845' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17846' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17849' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17852' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17855' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17856' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17857' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17858' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17921' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17924' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17927' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17928' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17931' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17934' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17937' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17938' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17955' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_17958' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_17960' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17963' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17971' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17978' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17981' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17983' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18008' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_18013' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18014' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18077' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18080' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18083' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18084' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18087' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18090' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18093' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18094' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18095' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18096' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18159' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18162' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18165' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18166' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18169' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18172' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18175' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18176' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18177' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18178' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18241' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18244' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18247' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18248' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18251' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18254' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18257' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18258' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18259' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18260' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18323' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18329' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18330' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18333' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18336' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18339' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18340' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18357' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_18360' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_18362' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18365' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18373' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_18380' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_18383' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_18385' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18421' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_18426' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18427' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18490' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18493' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18496' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18497' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18500' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18503' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18506' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18507' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18508' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18509' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18572' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18575' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18578' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18579' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18582' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18585' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18588' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18589' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18590' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18591' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18654' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18657' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18660' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18661' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18664' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18667' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18670' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18671' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18672' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18673' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18736' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18739' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18742' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18743' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18746' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18749' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18752' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18753' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18770' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_18773' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_18775' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18778' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18786' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_18793' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_18796' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_18798' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18823' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_18828' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18829' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18892' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18895' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18898' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18899' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18902' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18905' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18908' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18909' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18910' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18911' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18974' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18977' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18980' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18981' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18984' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18987' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18990' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18991' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18992' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18993' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19059' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19062' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19063' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19066' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19069' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19072' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19073' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19074' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19075' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19138' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19141' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19144' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19145' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19148' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19151' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19154' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19155' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19172' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_19175' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_19177' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19180' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19188' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_19195' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_19198' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_19200' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_19236' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_19241' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19242' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19305' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19308' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19311' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19312' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19315' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19318' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19321' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19322' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19323' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19324' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19387' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19390' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19393' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19394' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19397' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19400' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19403' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19404' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19405' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19406' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19469' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19472' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19475' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19476' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19479' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19482' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19485' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19486' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19487' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19488' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19551' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19554' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19557' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19558' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19561' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19564' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19567' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19568' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19585' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_19588' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_19590' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19593' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19601' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_19608' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_19611' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_19613' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_19638' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_19643' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19644' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19707' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19710' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19713' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19714' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19717' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19720' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19723' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19724' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19725' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19726' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19789' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19792' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19795' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19796' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19799' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19802' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19805' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19806' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19807' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19808' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19871' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19874' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19877' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19878' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19881' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19884' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19887' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19888' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19889' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19890' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19953' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19956' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19959' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19960' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19963' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19966' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19969' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19970' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19987' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_19990' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_19992' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19995' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20003' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20010' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20013' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20015' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20051' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_20056' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20057' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20120' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20123' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20126' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20127' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20130' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20133' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20136' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20137' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20138' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20139' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20202' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20205' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20208' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20209' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20212' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20215' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20218' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20219' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20220' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20221' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20284' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20287' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20290' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20291' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20294' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20297' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20300' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20301' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20302' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20303' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20366' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20369' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20372' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20373' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20376' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20379' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20382' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20383' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20400' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_20403' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_20405' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20408' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20416' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20423' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20426' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20428' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20453' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_20458' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20459' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20522' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20525' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20528' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20529' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20532' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20535' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20538' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20539' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20540' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20541' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20604' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20607' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20610' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20611' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20614' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20617' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20620' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20621' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20622' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20623' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20686' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20689' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20692' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20693' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20699' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20702' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20703' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20704' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20705' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20768' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20771' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20774' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20775' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20778' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20781' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20784' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20785' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20802' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_20805' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_20807' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20810' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20818' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20825' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20828' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20830' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20839' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_20872' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_20877' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20878' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20941' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20944' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20947' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20948' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20951' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20954' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20957' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20958' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20959' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20960' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21023' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21026' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21029' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21030' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21033' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21036' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21039' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21040' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21041' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_21042' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21105' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21108' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21111' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21112' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21115' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21118' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21121' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21122' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21123' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_21124' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21187' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21190' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21193' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21194' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21197' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21200' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21203' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21204' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21221' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_21224' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_21226' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_21229' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_21237' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_21244' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21247' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21249' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21258' with existing initializer 'val_5318'
[INFO] Replaced initializer 'scalar_tensor_default_1' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21324' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21339' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21342' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21344' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21358' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21375' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21378' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21380' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21383' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21391' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21398' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21401' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21403' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21417' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21434' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21437' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21439' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21442' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21450' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21457' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21460' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21462' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21476' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21493' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21496' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21498' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21501' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21509' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21516' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21519' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21521' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21529' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21532' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21534' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_2' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21558' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21575' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21578' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21580' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21583' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21591' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21598' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21601' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21603' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21617' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21634' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21637' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21639' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21642' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21650' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21657' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21660' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21662' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21676' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21693' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21696' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21698' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21701' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21709' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21716' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21719' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21721' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21735' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21752' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21755' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21757' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21760' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21768' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21775' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21778' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21780' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21788' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21791' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21793' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_3' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21817' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21834' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21837' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21839' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21842' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21850' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21857' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21860' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21862' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21876' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21893' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21896' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21898' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21901' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21909' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21916' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21919' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21921' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21935' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21952' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21955' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21957' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21960' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21968' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21975' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21978' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21980' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21994' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22011' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22014' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22016' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22019' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22027' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22034' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22037' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22039' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22047' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22050' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22052' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_4' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22076' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22093' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22096' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22098' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22101' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22109' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22116' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22119' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22121' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22135' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22152' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22155' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22157' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22160' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22168' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22175' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22178' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22180' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22194' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22211' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22214' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22216' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22219' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22227' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22234' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22237' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22239' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22253' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22270' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22273' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22275' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22278' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22286' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22293' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22296' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22298' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22306' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22309' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22311' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22317' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_22320' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_22324' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22327' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_22334' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22335' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22338' with existing initializer 'val_22331'
[INFO] Replaced initializer 'val_22341' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22344' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22345' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22360' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22361' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22364' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22365' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22500' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22501' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22504' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22505' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22510' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22518' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_22621' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22636' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22637' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22640' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22641' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22646' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22654' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_576' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_22757' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22772' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22773' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22776' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22777' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22782' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22790' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_578' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_579' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_22885' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_22891' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22901' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22906' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22911' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22991' with existing initializer 'val_22983'
[INFO] Replaced initializer 'val_22994' with existing initializer 'val_22992'
[INFO] Replaced initializer 'mul_582' with existing initializer 'mul_581'
[INFO] Replaced initializer 'val_23037' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_23060' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23063' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_23064' with existing initializer 'val_28'
[INFO] Replaced initializer 'scalar_tensor_default_25' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_23110' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23111' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23114' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23115' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23120' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23128' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_23223' with existing initializer 'val_22478'
[INFO] Replaced initializer 'val_23229' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23244' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23245' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23248' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23249' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23254' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23262' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_23357' with existing initializer 'val_22615'
[INFO] Replaced initializer 'val_23363' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23378' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23379' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23382' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23383' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23388' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23396' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_590' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_591' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_23491' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_23497' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23512' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23513' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23516' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23517' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23522' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23530' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_593' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_594' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_23625' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_23631' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23636' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23641' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23646' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23651' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23654' with existing initializer 'val_22915'
[INFO] Replaced initializer 'val_23657' with existing initializer 'val_22919'
[INFO] Replaced initializer 'val_23660' with existing initializer 'val_22923'
[INFO] Replaced initializer 'val_23663' with existing initializer 'val_22927'
[INFO] Replaced initializer 'val_23666' with existing initializer 'val_22931'
[INFO] Replaced initializer 'view_445' with existing initializer 'view_405'
[INFO] Replaced initializer 'val_23714' with existing initializer 'val_22983'
[INFO] Replaced initializer 'view_446' with existing initializer 'view_406'
[INFO] Replaced initializer 'val_23722' with existing initializer 'val_22983'
[INFO] Replaced initializer 'val_23723' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23725' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23730' with existing initializer 'val_22999'
[INFO] Replaced initializer 'mul_596' with existing initializer 'mul_581'
[INFO] Replaced initializer 'mul_597' with existing initializer 'mul_581'
[INFO] Replaced initializer 'val_23761' with existing initializer 'val_23031'
[INFO] Replaced initializer 'val_23767' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23786' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_23789' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23792' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_23793' with existing initializer 'val_28'
[INFO] Replaced initializer 'int64_1_cast' with existing initializer 'val_108'
[INFO] Replaced initializer 'scalar_tensor_default_46' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_23824' with existing initializer 'val_23095'
[INFO] Wrote onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.onnx
[INFO] Rebinding external data...
[INFO] External data: /home/ashim/Documents/projects/vggt/onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.onnx.data
[INFO] Sizes: ONNX=10.1 MB, DATA=4.44 GB
[INFO] Pruning model for PCD-only mode...
[INFO]   Keeping output: cat_322
[INFO]   Keeping output: view_411
[INFO]   Keeping output: view_412
[INFO]   Removing output: view_451
[INFO]   Removing output: view_452
[INFO]   Removing output: unsqueeze
[INFO] Pruned model saved: onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.onnx
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (Half precision with TF32 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd_fp16.engine (2.20 GB)
[INFO] Shared ONNX for subsequent builds: onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.onnx
[INFO] 
[INFO] ======================================================================
[INFO] Building BF16 variant (2/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, bf16 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear4bit (NF4) weight quantisation
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (BFloat16 precision)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled BF16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd_bf16.engine (2.21 GB)
[INFO] 
[INFO] ======================================================================
[INFO] Building FP8 variant (3/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp8 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear4bit (NF4) weight quantisation
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (FP8 precision (RTX 5090 limited support))...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled FP8
[WARNING] FP8 enabled: RTX 5090 has limited FP8 support. Expect 'Unsupported data type FP8' warnings for some ops. TensorRT will fallback to FP16 automatically.
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] FP8 build: Ignore 'Unsupported data type FP8' warnings - this is normal
[INFO] FP8 strict mode: temporarily disabled FP16 fallback
[INFO] FP8 strict mode: temporarily disabled TF32 fallback
[10/24/2025-15:21:44] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-15:21:44] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception unimplemented scalar type!
[10/24/2025-15:21:44] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-15:21:45] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-15:21:45] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-15:22:29] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-15:22:30] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-15:22:30] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-15:22:31] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-15:22:31] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-15:22:31] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[INFO] Strict FP8 build succeeded without fallback precisions: FP16, TF32 disabled
[INFO] Engine saved: onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd_fp8.engine (4.36 GB)
[INFO] 
[INFO] ======================================================================
[INFO] Building INT8 variant (4/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, int8 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear4bit (NF4) weight quantisation
[INFO] INT8 calibration: source=random Gaussian batches=1 seed=1337 gpu=True
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
/home/ashim/Documents/projects/vggt/onnx/vggt_to_trt_chatgpt.py:1691: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.
  config.int8_calibrator = calibrator
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/bitsandbytes-nf4/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-15:25:08] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-15:25:08] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-15:25:08] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-15:25:08] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-15:25:08] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-15:25:51] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-15:25:51] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-15:25:51] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-15:25:51] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-15:25:51] [TRT] [W] Using rough estimation for the inference time of the engine.
[INFO] INT8 calibrator exhausted after 1 batches
[INFO] Calibration cache saved to 'onnx_exports/bitsandbytes-nf4/calibration-8x518x518.cache'
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18765_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor mul_575_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18771_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18773_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23485_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_66_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22925_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13052_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_210_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_212_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25157_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11788_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9620_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25514_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24410_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10613_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_27060_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_334_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_336_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24408_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 24318) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_502_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15201_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8895_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9214_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17369_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_626_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_628_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_650_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13507_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26875_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24235_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15628_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25928_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_772_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_774_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_794_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16192_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9326_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24211_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9332_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_918_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_942_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18204_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14051_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19061_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19063_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17346_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1064_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1066_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1086_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12623_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19085_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19087_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24386_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14474_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26965_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1210_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1212_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17485_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1234_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11477_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25506_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24213_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1378_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1380_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25806_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22629_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25952_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14476_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1502_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1524_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1526_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19195_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19197_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26577_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11790_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19201_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19203_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1650_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1670_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1672_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24532_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24534_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14366_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26723_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26721_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8897_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1794_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1816_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1818_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16196_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21775_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13185_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15630_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21636_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9330_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor select_203, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1942_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_27022_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1962_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1964_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25473_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor select_193, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15651_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2086_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2108_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2110_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17912_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24554_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24556_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17775_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor mul_581_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor select_202, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2234_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26699_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.depth_head.norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11475_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17483_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor view_394_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12196_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2378_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2380_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2400_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17798_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16488_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26407_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13077_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2524_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2526_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2546_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2548_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15340_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9645_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19491_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2670_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2672_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2692_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2694_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10182_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12194_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19514_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19516_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2816_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2818_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2838_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2840_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13935_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26409_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16486_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2962_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2964_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2984_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2986_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18227_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10907_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3108_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3110_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3130_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3132_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19624_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19626_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23487_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19630_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19632_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3276_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3278_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25512_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11040_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3400_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3422_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3424_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16625_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18657_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22090_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25636_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13505_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25181_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3546_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3548_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3568_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3570_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22946_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor expand_8_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor val_25_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3606_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor add_49, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor _to_copy_1_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor cat_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3610_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3612_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12329_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26555_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14341_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3720_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3722_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3726_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3728_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor val_19_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor select_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor val_1675_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3733_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3735_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 3984) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor clamp, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor cos_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3736_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor sin_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3737_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16080_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15769_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24680_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14053_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26575_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24678_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8901_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor select_2, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3805_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3807_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4053) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor clamp_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3808_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3809_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9755_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11046_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24264_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11898_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22065_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22627_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26553_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17479_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24702_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24700_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17367_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4028_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4030_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor view_61, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4052_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22633_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11900_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4162_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4164_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4168_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4170_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor select_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4175_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4177_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4407) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor clamp_4, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4178_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10611_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor select_6, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4247_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4249_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4473) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor clamp_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4250_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4251_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16198_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11906_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25303_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25782_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_24231_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19943_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19945_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14909_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.token_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21661_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26074_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4470_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4472_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4495_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26076_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22948_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16938_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18633_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26967_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15332_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23915_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13623_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14047_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9216_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26261_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13193_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21769_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15334_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12333_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13481_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4603_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4605_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4609_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4611_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16621_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22496_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20053_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20055_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10184_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20059_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20061_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22204_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12756_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23062_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22517_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24237_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25804_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18225_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26877_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 23008) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22519_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor val_21286_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4899_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4901_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4924_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22200_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10051_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23807_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5032_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5034_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5038_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5040_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26096_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18655_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11338_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17906_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26098_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25658_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17048_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9622_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23805_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26263_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18202_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13191_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9643_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15761_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5328_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5330_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5351_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5353_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11361_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24824_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11048_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24826_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23921_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11904_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11042_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5461_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5463_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5467_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5469_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22635_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13187_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10503_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20349_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20351_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20372_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20374_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor view_403_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5757_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5759_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5780_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5782_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18341_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25159_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21659_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13913_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11471_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18337_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23056_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10501_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5890_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5892_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor view_402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5896_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5898_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23354_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23375_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25451_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14045_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12646_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11359_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15199_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24846_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17477_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24848_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25305_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16627_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9193_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17056_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20484_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6186_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_27012_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6209_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6211_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17908_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20490_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20488_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10049_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17344_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14364_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17773_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.token_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25660_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_27010_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9761_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6319_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6321_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6325_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 23703) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor val_21545_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24879_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24881_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22206_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16057_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor val_21804_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24887_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24889_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15763_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22067_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6640_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26283_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13075_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14343_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23783_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12217_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6748_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6750_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6754_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6756_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24262_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13621_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14905_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15222_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23352_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12758_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17914_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16509_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 24933) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10930_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12762_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10478_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7044_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7046_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7067_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7069_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20778_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20780_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26285_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16915_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16059_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22923_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11765_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12625_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7177_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7183_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7185_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20801_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20803_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25930_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26429_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15224_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26129_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26131_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor val_22063_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10932_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25325_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13483_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26137_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22494_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26139_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16619_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7473_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7475_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9324_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7497_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7499_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20913_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20919_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25784_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14793_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22198_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23923_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7607_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7609_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7613_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9753_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10074_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13937_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25471_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25011_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25013_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7905_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7927_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7929_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18631_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16511_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25033_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25035_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12219_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23491_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21771_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11336_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26431_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8037_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8039_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8043_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8045_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23781_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23058_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10619_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15653_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26701_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10072_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14772_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.depth_head.norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9759_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11469_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8333_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21207_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21209_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10909_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24388_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14795_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12764_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14770_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8466_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8468_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor view_395_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8472_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8474_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25449_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21230_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23064_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21777_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16082_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15338_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25950_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18343_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17050_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8762_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8764_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8785_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8787_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23377_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21340_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21342_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21346_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21348_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-15:32:04] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9191_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[INFO] Engine saved: onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd_int8.engine (2.19 GB)
[INFO] 
[INFO] ======================================================================
[INFO] BUILD COMPLETE!
[INFO] ======================================================================
[INFO] Input shape: [8, 3, 518, 518]
[INFO] Mode: PCD-only (depth + camera heads)
[INFO] 
[INFO] Built engines:
[INFO]   FP16  (Half precision with TF32 fallback       ): onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd_fp16.engine
[INFO]         Size: 2.20 GB
[INFO]   BF16  (BFloat16 precision                      ): onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd_bf16.engine
[INFO]         Size: 2.21 GB
[INFO]   FP8   (FP8 precision (RTX 5090 limited support)): onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd_fp8.engine
[INFO]         Size: 4.36 GB
[INFO]   INT8  (INT8 quantization with FP16 fallback    ): onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd_int8.engine
[INFO]         Size: 2.19 GB
[INFO] 
[INFO] Next steps for live PCD construction:
[INFO]   1. Use FP16 for best speed/quality balance on RTX 5090
[INFO]   2. Extract outputs: cat_322 (camera), view_411/412 (depth)
[INFO]   3. Unproject depth to 3D using camera parameters
[INFO]   4. Expected latency: ~170ms FP16, ~230ms BF16 (from your benchmark)
[INFO]   5. With --pcd-only: expect ~30% faster (~120ms FP16)
[INFO] 
[INFO] Performance hierarchy (your RTX 5090):
[INFO]   FP16:  170ms (5.9 FPS)  RECOMMENDED for live PCD
[INFO]   BF16:  232ms (4.3 FPS)
[INFO]   FP8:   173ms (5.8 FPS) - similar to FP16, limited HW support
[INFO]   INT8:  TBD (needs proper calibration data)
[INFO] 
[INFO] Optimizations applied:
[INFO]    Timing cache (speeds up rebuilds)
[INFO]    Optimization level 5 (maximum)
[INFO]    Auxiliary streams: 4
[INFO]    TF32 enabled (Ampere+ GPUs)
[INFO]    All CUDA tactic sources (cuBLAS, cuDNN)
[INFO]    Workspace: 32 GB
[INFO]    FIXED: INT8 calibrator (TRT 10+ compatible)
[INFO]    FIXED: Opset handling (onnxsim now works)
[INFO] ======================================================================
Applied 1260 of general pattern rewrite rules.

--- Building extra precision fp32 for quant mode bitsandbytes-nf4 ---

>>> python onnx/vggt_to_trt_chatgpt.py --onnx-in onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.onnx --precision fp32 --quant-mode bitsandbytes-nf4 --output-dir onnx_exports/bitsandbytes-nf4 --num-cams 8 --height 518 --width 518 --model-name facebook/VGGT-1B --calib-batches 1 --calib-seed 1337 --pcd-only
[INFO] 
[INFO] ======================================================================
[INFO] Building FP32 variant (1/1)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp32 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear4bit (NF4) weight quantisation
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (Full precision (baseline))...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.engine (4.34 GB)
[INFO] 
[INFO] ======================================================================
[INFO] BUILD COMPLETE!
[INFO] ======================================================================
[INFO] Input shape: [8, 3, 518, 518]
[INFO] Mode: PCD-only (depth + camera heads)
[INFO] 
[INFO] Built engines:
[INFO]   FP32  (Full precision (baseline)               ): onnx_exports/bitsandbytes-nf4/vggt-8x3x518x518-pcd.engine
[INFO]         Size: 4.34 GB
[INFO] 
[INFO] Next steps for live PCD construction:
[INFO]   1. Use FP16 for best speed/quality balance on RTX 5090
[INFO]   2. Extract outputs: cat_322 (camera), view_411/412 (depth)
[INFO]   3. Unproject depth to 3D using camera parameters
[INFO]   4. Expected latency: ~170ms FP16, ~230ms BF16 (from your benchmark)
[INFO]   5. With --pcd-only: expect ~30% faster (~120ms FP16)
[INFO] 
[INFO] Performance hierarchy (your RTX 5090):
[INFO]   FP16:  170ms (5.9 FPS)  RECOMMENDED for live PCD
[INFO]   BF16:  232ms (4.3 FPS)
[INFO]   FP8:   173ms (5.8 FPS) - similar to FP16, limited HW support
[INFO]   INT8:  TBD (needs proper calibration data)
[INFO] 
[INFO] Optimizations applied:
[INFO]    Timing cache (speeds up rebuilds)
[INFO]    Optimization level 5 (maximum)
[INFO]    Auxiliary streams: 4
[INFO]    TF32 enabled (Ampere+ GPUs)
[INFO]    All CUDA tactic sources (cuBLAS, cuDNN)
[INFO]    Workspace: 32 GB
[INFO]    FIXED: INT8 calibrator (TRT 10+ compatible)
[INFO]    FIXED: Opset handling (onnxsim now works)
[INFO] ======================================================================

================================================================
Quantisation mode: bitsandbytes-fp4
Output directory : onnx_exports/bitsandbytes-fp4
================================================================
[INFO] Missing core precisions for quant mode 'bitsandbytes-fp4': fp16 bf16 fp8 int8

>>> python onnx/vggt_to_trt_chatgpt.py --export --all-precisions --quant-mode bitsandbytes-fp4 --output-dir onnx_exports/bitsandbytes-fp4 --num-cams 8 --height 518 --width 518 --model-name facebook/VGGT-1B --calib-batches 1 --calib-seed 1337 --pcd-only
[INFO] Building all precision variants: fp16, bf16, fp8, int8
[INFO] 
[INFO] ======================================================================
[INFO] Building FP16 variant (1/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp16 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear4bit (FP4) weight quantisation
[INFO] Requested quantisation mode 'bitsandbytes-fp4' is not ONNX-compatible with ONNX export; skipping.
[INFO] Retrying export with fallback quantisation mode 'none' (2/2)
[INFO] using MLP layer as FFN
[INFO] Modifying model for PCD-only export...
[WARNING] PCD-only mode: will prune unused outputs after export
[INFO] Exporting to ONNX with shape [8, 3, 518, 518] on cuda:0 (quant='none')
/home/ashim/miniconda3/envs/compvis/lib/python3.10/site-packages/vggt/models/vggt.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
[INFO] Removed 851 unused nodes
[INFO] No unused functions to remove
[INFO] Skipping constant folding for node 'node_unsqueeze' because it is graph input to preserve graph signature
[INFO] Skipping constant folding for node Node(name='node_cat_3', domain='', op_type='Concat', inputs=(Value(name='expand_4', type=Tensor(FLOAT), shape=Shape([1, 1, 4, 1024]), producer='node_Constant_23972', index=0, const_value={Tensor(...)}), Value(name='expand_5', type=Tensor(FLOAT), shape=Shape([1, 7, 4, 1024]), producer='node_Constant_23994', index=0, const_value={Tensor(...)})), attributes={'axis': Attr('axis', INT, 1)}, overload='', outputs=(SymbolicTensor(name='cat_3', type=Tensor(FLOAT), shape=Shape([1, 8, 4, 1024]), producer='node_cat_3', index=0),), version=13, doc_string=None) due to large input sizes: [4096, 28672]
[INFO] Skipping constant folding for node Node(name='node_clone_74', domain='', op_type='Identity', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='clone_74', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_clone_74', index=0),), version=16, doc_string=None) due to large input sizes: [21904]
[INFO] Skipping constant folding for node Node(name='node_add_49', domain='', op_type='Add', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_25', type=Tensor(INT64), shape=Shape([]), producer='node_Constant_25', index=0, const_value={Tensor<INT64,[]>(array(1), name=None)})), attributes={}, overload='', outputs=(SymbolicTensor(name='add_49', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_add_49', index=0),), version=14, doc_string=None) due to large input sizes: [21904, 1]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20794', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21278', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_20794', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20797', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21281', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20797', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20803', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21291', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20803', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20845', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21333', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20845', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20847', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21337', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20847', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20856', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21346', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20856', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20858', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21350', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20858', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20900', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21392', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20900', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20902', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21396', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20902', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20911', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21405', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20911', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20913', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21409', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20913', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20955', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21451', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20955', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20957', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21455', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20957', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20966', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21464', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20966', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20968', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21468', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20968', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21010', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21510', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21010', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21012', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21514', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21012', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21021', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21523', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21021', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21023', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21527', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21023', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21032', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21536', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21032', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21034', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21538', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21034', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21037', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21541', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21037', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21042', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21550', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21042', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21084', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21592', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21084', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21086', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21596', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21086', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21095', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21605', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21095', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21097', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21609', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21097', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21139', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21651', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21139', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21141', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21655', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21141', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21150', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21664', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21150', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21152', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21668', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21152', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21194', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21710', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21194', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21196', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21714', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21196', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21205', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21723', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21205', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21207', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21727', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21207', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21249', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21769', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21249', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21251', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21773', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21251', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21260', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21782', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21260', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21262', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21786', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21262', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21271', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21795', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21271', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21273', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21797', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21273', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21276', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21800', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21276', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21281', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21809', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21281', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21323', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21851', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21323', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21325', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21855', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21325', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21334', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21864', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21334', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21336', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21868', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21336', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21378', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21910', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21378', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21380', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21914', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21380', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21389', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21923', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21389', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21391', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21927', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21391', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21433', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21969', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21433', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21435', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21973', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21435', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21444', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21982', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21444', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21446', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21986', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21446', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21488', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22028', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21488', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21490', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22032', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21490', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21499', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22041', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21499', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21501', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22045', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21501', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21510', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22054', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21510', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21512', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22056', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21512', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21515', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22059', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21515', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21520', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22068', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21520', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21562', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22110', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21562', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21564', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22114', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21564', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21573', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22123', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21573', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21575', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22127', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21575', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21617', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22169', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21617', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21619', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22173', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21619', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21628', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22182', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21628', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21630', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22186', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21630', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21672', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22228', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21672', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21674', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22232', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21674', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21683', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22241', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21683', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21685', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22245', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21685', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21727', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22287', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21727', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21729', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22291', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21729', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21738', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22300', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21738', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21740', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22304', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21740', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21749', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22313', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21749', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_sin_1', domain='', op_type='Sin', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_1', domain='', op_type='Cos', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_2', domain='', op_type='Sin', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_2', domain='', op_type='Cos', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_3', domain='', op_type='Sin', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_3', domain='', op_type='Cos', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_4', domain='', op_type='Sin', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_4', domain='', op_type='Cos', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_5 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_5', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_5', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_6 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_6', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_6', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_7 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_7', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_7', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_8 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_8', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_8', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_22 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_23 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22421', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_23', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_23', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22992', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22420', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22992')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22993', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22421', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22423', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_22', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_22', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22994', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22422', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22994')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22995', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22423', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_sin_11', domain='', op_type='Sin', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_11', domain='', op_type='Cos', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_12', domain='', op_type='Sin', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_12', domain='', op_type='Cos', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_13', domain='', op_type='Sin', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_13', domain='', op_type='Cos', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_14', domain='', op_type='Sin', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_14', domain='', op_type='Cos', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_15 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_15', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_15', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_16 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_16', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_16', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_17 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_17', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_17', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_18 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_18', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_18', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_37 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_38 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23144', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_38', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_38', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23723', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23143', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23723')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23724', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23144', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23146', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_37', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_37', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23725', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23145', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23725')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23726', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23146', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] Removed 19332 unused nodes
[INFO] No unused functions to remove
[INFO] Skipping constant folding for node 'node_unsqueeze' because it is graph input to preserve graph signature
[INFO] Skipping constant folding for node Node(name='node_cat_3', domain='', op_type='Concat', inputs=(Value(name='expand_4', type=Tensor(FLOAT), shape=Shape([1, 1, 4, 1024]), producer='node_Constant_23972', index=0, const_value={Tensor(...)}), Value(name='expand_5', type=Tensor(FLOAT), shape=Shape([1, 7, 4, 1024]), producer='node_Constant_23994', index=0, const_value={Tensor(...)})), attributes={'axis': Attr('axis', INT, 1)}, overload='', outputs=(SymbolicTensor(name='cat_3', type=Tensor(FLOAT), shape=Shape([1, 8, 4, 1024]), producer='node_cat_3', index=0),), version=13, doc_string=None) due to large input sizes: [4096, 28672]
[INFO] Skipping constant folding for node Node(name='node_add_49', domain='', op_type='Add', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_25', type=Tensor(INT64), shape=Shape([]), producer='node_Constant_25', index=0, const_value={Tensor<INT64,[]>(array(1), name=None)})), attributes={}, overload='', outputs=(SymbolicTensor(name='add_49', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_add_49', index=0),), version=14, doc_string=None) due to large input sizes: [21904, 1]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20794', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21278', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_20794', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20797', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21281', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20797', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20803', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21291', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20803', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20845', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21333', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20845', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20847', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21337', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20847', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20856', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21346', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20856', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20858', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21350', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20858', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20900', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21392', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20900', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20902', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21396', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20902', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20911', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21405', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20911', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20913', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21409', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20913', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20955', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21451', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20955', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20957', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21455', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20957', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20966', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21464', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20966', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20968', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21468', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20968', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21010', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21510', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21010', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21012', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21514', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21012', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21021', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21523', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21021', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21023', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21527', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21023', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21032', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21536', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21032', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21034', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21538', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21034', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21037', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21541', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21037', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21042', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21550', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21042', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21084', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21592', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21084', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21086', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21596', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21086', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21095', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21605', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21095', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21097', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21609', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21097', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21139', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21651', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21139', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21141', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21655', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21141', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21150', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21664', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21150', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21152', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21668', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21152', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21194', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21710', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21194', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21196', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21714', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21196', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21205', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21723', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21205', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21207', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21727', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21207', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21249', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21769', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21249', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21251', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21773', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21251', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21260', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21782', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21260', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21262', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21786', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21262', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21271', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21795', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21271', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21273', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21797', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21273', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21276', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21800', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21276', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21281', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21809', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21281', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21323', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21851', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21323', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21325', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21855', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21325', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21334', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21864', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21334', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21336', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21868', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21336', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21378', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21910', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21378', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21380', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21914', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21380', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21389', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21923', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21389', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21391', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21927', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21391', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21433', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21969', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21433', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21435', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21973', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21435', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21444', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21982', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21444', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21446', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21986', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21446', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21488', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22028', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21488', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21490', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22032', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21490', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21499', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22041', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21499', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21501', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22045', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21501', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21510', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22054', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21510', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21512', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22056', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21512', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21515', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22059', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21515', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21520', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22068', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21520', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21562', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22110', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21562', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21564', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22114', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21564', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21573', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22123', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21573', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21575', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22127', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21575', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21617', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22169', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21617', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21619', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22173', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21619', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21628', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22182', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21628', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21630', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22186', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21630', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21672', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22228', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21672', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21674', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22232', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21674', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21683', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22241', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21683', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21685', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22245', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21685', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21727', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22287', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21727', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21729', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22291', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21729', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21738', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22300', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21738', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21740', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22304', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21740', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21749', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22313', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21749', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_sin_1', domain='', op_type='Sin', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_1', domain='', op_type='Cos', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_2', domain='', op_type='Sin', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_2', domain='', op_type='Cos', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_3', domain='', op_type='Sin', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_3', domain='', op_type='Cos', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_4', domain='', op_type='Sin', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_4', domain='', op_type='Cos', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_5 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_5', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_5', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_6 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_6', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_6', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_7 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_7', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_7', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_8 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_8', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_8', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_22 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_23 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22421', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_23', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_23', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22992', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22420', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22992')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22993', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22421', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22423', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_22', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_22', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22994', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22422', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22994')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22995', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22423', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_sin_11', domain='', op_type='Sin', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_11', domain='', op_type='Cos', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_12', domain='', op_type='Sin', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_12', domain='', op_type='Cos', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_13', domain='', op_type='Sin', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_13', domain='', op_type='Cos', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_14', domain='', op_type='Sin', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_14', domain='', op_type='Cos', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_15 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_15', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_15', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_16 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_16', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_16', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_17 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_17', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_17', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_18 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_18', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_18', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_37 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_38 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23144', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_38', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_38', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23723', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23143', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23723')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23724', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23144', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23146', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_37', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_37', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23725', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23145', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23725')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23726', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23146', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] Removed 1189 unused nodes
[INFO] No unused functions to remove
[INFO] Replaced initializer 'val_2878' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_2889' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_3291' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_3693' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_3704' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_4106' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_4508' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4519' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_4921' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_5329' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_5340' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_5748' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6150' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6161' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_6563' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6965' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6976' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_7378' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_7780' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_7791' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_8193' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_8595' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8606' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_9008' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_9410' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_9421' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_9823' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_10225' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_10236' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_10638' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11046' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11057' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_11465' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11867' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11878' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_12280' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_12682' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_12693' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_13095' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_13497' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13508' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_13910' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_14312' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_14323' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_14725' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15127' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15138' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_15540' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15948' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15959' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_16367' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_16769' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_16780' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_17182' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_17584' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17595' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_17997' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_18399' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_18410' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_18812' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_19214' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_19225' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_19627' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20029' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20040' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_20442' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20850' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20861' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_24' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_31' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_32' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_46' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_50' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_51' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_90' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_98' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_124' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_141' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_144' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_146' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_149' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_157' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_164' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_167' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_169' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_183' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_200' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_203' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_205' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_208' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_216' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_223' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_226' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_228' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_242' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_259' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_262' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_264' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_267' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_275' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_282' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_285' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_287' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_301' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_318' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_321' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_323' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_326' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_334' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_341' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_344' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_346' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_360' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_377' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_380' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_382' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_385' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_393' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_400' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_403' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_405' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_419' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_436' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_439' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_441' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_444' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_452' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_459' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_462' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_464' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_478' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_495' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_498' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_500' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_503' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_511' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_518' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_521' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_523' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_537' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_554' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_557' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_559' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_562' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_570' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_577' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_580' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_582' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_596' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_613' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_616' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_618' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_621' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_629' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_636' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_639' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_641' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_655' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_672' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_675' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_677' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_680' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_688' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_695' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_698' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_700' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_714' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_731' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_734' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_736' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_739' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_747' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_754' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_757' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_759' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_773' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_790' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_793' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_795' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_798' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_806' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_813' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_816' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_818' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_832' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_849' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_852' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_854' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_857' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_865' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_872' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_875' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_877' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_891' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_908' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_911' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_913' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_916' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_924' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_931' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_934' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_936' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_950' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_967' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_970' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_972' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_975' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_983' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_990' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_993' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_995' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1009' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1026' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1029' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1031' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1034' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1042' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1049' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1052' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1054' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1068' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1085' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1088' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1090' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1093' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1101' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1108' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1111' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1113' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1127' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1144' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1147' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1149' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1152' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1160' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1167' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1170' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1172' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1186' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1203' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1206' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1208' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1211' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1219' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1226' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1229' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1231' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1245' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1262' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1265' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1267' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1270' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1278' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1285' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1288' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1290' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1304' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1321' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1324' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1326' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1329' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1337' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1344' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1347' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1349' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1363' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1380' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1383' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1385' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1388' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1396' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1403' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1406' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1408' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1422' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1439' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1442' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1444' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1447' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1455' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1462' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1465' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1467' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1483' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1486' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1487' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1662' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1673' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1686' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1740' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1748' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1751' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1754' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1757' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1758' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1759' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1760' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1823' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1826' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1829' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1830' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1833' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1836' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1839' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1840' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1841' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1842' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1905' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1908' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1911' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1912' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1915' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1918' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1921' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1922' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1923' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1924' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1987' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1990' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1993' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1994' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1997' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2000' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2003' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2004' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2021' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_2024' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_2026' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2029' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2037' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_2044' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2047' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2049' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2090' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2091' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2154' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2157' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2160' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2161' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2164' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2167' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2170' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2171' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2172' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2173' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2236' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2239' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2242' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2243' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2246' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2249' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2252' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2253' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2254' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2255' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2318' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2321' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2324' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2325' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2328' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2331' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2334' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2335' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2336' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2337' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2400' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2403' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2406' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2407' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2410' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2413' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2416' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2417' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2439' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2442' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2450' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_2457' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2460' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2462' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2487' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_2492' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2493' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2556' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2559' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2562' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2563' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2566' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2569' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2572' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2573' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2574' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2575' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2638' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2641' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2644' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2645' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2648' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2651' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2654' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2655' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2656' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2657' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2720' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2723' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2726' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2727' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2730' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2733' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2736' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2737' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2738' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2739' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2802' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2805' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2808' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2809' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2812' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2815' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2818' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2819' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2836' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_2839' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_2841' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2844' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2852' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_2859' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2862' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2864' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2900' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_2905' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2906' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2969' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2972' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2975' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2976' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2979' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2982' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2985' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2986' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2987' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2988' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3051' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3054' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3057' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3058' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3061' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3064' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3067' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3068' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3069' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3070' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3133' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3136' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3139' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3140' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3143' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3146' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3149' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3150' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3151' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3152' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3215' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3218' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3221' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3222' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3225' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3228' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3231' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3232' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3249' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_3252' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_3254' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3257' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3265' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_3272' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_3275' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_3277' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_3302' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_3307' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3308' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3371' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3374' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3377' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3378' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3381' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3384' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3387' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3388' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3389' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3390' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3453' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3456' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3459' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3460' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3463' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3466' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3469' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3470' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3471' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3472' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3535' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3538' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3541' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3542' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3545' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3548' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3551' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3552' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3553' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3554' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3617' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3620' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3623' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3624' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3627' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3630' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3633' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3634' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3651' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_3654' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_3656' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3659' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3667' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_3674' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_3677' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_3679' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_3715' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_3720' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3721' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3784' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3787' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3790' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3791' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3797' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3800' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3801' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3802' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3803' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3866' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3869' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3872' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3873' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3876' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3879' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3882' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3883' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3884' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3885' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3948' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3951' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3954' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3955' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3958' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3961' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3964' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3965' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3966' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3967' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4030' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4033' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4036' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4037' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4040' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4043' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4046' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4047' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4064' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_4067' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_4069' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4072' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4080' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4087' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4090' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4092' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4117' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_4122' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4123' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4186' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4189' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4192' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4193' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4196' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4199' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4202' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4203' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4204' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4205' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4268' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4271' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4274' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4275' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4278' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4281' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4284' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4285' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4286' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4287' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4350' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4353' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4356' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4357' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4360' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4363' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4366' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4367' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4368' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4369' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4432' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4435' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4438' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4439' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4442' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4445' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4448' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4449' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4466' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_4469' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_4471' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4474' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4482' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_4489' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4492' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4494' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4530' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_4535' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4536' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4599' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4602' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4605' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4606' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4609' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4612' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4615' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4616' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4617' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4618' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4681' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4684' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4687' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4688' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4691' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4694' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4697' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4698' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4699' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4700' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4763' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4766' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4769' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4770' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4773' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4776' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4779' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4780' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4781' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4782' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4845' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4848' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4851' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4852' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4855' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4858' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4861' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4862' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4879' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_4882' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_4884' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4887' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4895' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4902' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4905' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4907' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4932' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_4937' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4938' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5001' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5004' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5007' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5008' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5011' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5014' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5017' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5018' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5019' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5020' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5083' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5086' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5089' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5090' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5093' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5096' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5099' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5100' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5101' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5102' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5165' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5168' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5171' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5172' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5175' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5178' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5181' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5182' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5183' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5184' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5247' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5250' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5253' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5254' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5257' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5260' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5263' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5264' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5281' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_5284' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_5286' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5289' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5297' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_5304' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_5307' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_5309' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_5351' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_5356' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5357' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5420' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5423' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5426' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5427' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5430' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5433' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5436' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5437' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5438' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5439' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5502' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5505' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5508' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5509' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5512' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5515' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5518' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5519' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5520' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5521' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5584' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5587' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5590' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5591' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5594' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5597' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5600' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5601' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5602' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5603' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5666' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5669' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5672' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5673' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5676' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5679' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5682' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5683' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5700' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_5703' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_5705' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5708' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5716' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_5723' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_5726' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_5728' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_5737' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_5759' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_5764' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5765' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5828' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5831' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5834' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5835' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5838' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5841' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5844' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5845' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5846' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5847' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5910' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5913' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5916' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5917' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5920' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5923' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5926' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5927' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5928' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5929' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5992' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5995' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5998' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5999' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6002' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6005' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6008' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6009' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6010' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6011' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6074' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6077' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6080' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6081' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6084' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6087' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6090' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6091' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6108' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_6111' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_6113' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6116' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6124' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6131' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6134' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6136' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6172' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_6177' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6178' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6241' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6244' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6247' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6248' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6251' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6254' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6257' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6258' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6259' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6260' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6323' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6329' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6330' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6333' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6336' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6339' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6340' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6341' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6342' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6405' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6408' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6411' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6412' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6415' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6418' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6421' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6422' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6423' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6424' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6487' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6490' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6493' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6494' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6497' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6500' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6503' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6504' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6521' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_6524' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_6526' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6529' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6537' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6544' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6547' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6549' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6574' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_6579' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6580' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6643' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6646' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6649' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6650' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6653' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6656' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6659' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6660' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6661' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6662' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6725' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6728' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6731' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6732' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6735' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6738' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6741' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6742' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6743' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6744' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6807' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6810' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6813' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6814' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6817' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6820' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6823' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6824' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6825' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6826' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6889' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6892' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6895' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6896' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6899' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6902' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6905' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6906' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6923' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_6926' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_6928' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6931' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6939' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6946' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6949' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6951' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6987' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_6992' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6993' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7059' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7062' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7063' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7066' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7069' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7072' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7073' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7074' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7075' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7138' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7141' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7144' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7145' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7148' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7151' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7154' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7155' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7156' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7157' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7220' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7223' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7226' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7227' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7230' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7233' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7236' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7237' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7238' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7239' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7302' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7305' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7308' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7309' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7312' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7315' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7318' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7319' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7336' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_7339' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_7341' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7344' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7352' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_7359' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_7362' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_7364' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_7389' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_7394' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7395' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7458' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7461' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7464' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7465' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7468' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7471' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7474' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7475' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7476' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7477' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7540' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7543' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7546' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7547' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7550' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7553' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7556' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7557' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7558' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7559' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7622' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7625' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7628' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7629' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7632' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7635' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7638' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7639' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7640' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7641' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7704' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7707' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7710' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7711' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7714' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7717' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7720' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7721' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7738' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_7741' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_7743' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7746' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7754' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_7761' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_7764' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_7766' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_7802' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_7807' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7808' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7871' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7874' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7877' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7878' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7881' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7884' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7887' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7888' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7889' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7890' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7953' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7956' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7959' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7960' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7963' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7966' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7969' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7970' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7971' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7972' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8035' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8038' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8041' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8042' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8045' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8048' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8051' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8052' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8053' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8054' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8117' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8120' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8123' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8124' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8127' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8130' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8133' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8134' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8151' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_8154' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_8156' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8159' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8167' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8174' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8177' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8179' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_8204' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_8209' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8210' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8273' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8276' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8279' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8280' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8283' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8286' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8289' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8290' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8291' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8292' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8355' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8358' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8361' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8362' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8365' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8368' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8371' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8372' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8373' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8374' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8437' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8440' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8443' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8444' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8447' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8450' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8453' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8454' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8455' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8456' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8519' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8522' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8525' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8526' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8529' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8532' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8535' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8536' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8553' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_8556' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_8558' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8561' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8569' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_8576' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8579' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8581' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_8617' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_8622' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8623' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8686' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8689' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8692' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8693' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8699' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8702' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8703' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8704' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8705' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8768' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8771' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8774' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8775' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8778' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8781' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8784' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8785' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8786' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8787' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8850' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8853' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8856' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8857' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8860' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8863' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8866' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8867' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8868' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8869' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8932' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8935' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8938' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8939' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8942' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8945' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8948' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8949' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8966' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_8969' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_8971' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8974' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8982' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8989' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8992' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8994' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9019' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_9024' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9025' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9088' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9091' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9094' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9095' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9098' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9101' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9104' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9105' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9106' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9107' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9170' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9173' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9176' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9177' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9180' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9183' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9186' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9187' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9188' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9189' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9252' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9255' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9258' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9259' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9262' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9265' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9268' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9269' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9270' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9271' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9334' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9337' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9340' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9341' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9344' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9347' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9350' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9351' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9368' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_9371' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_9373' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9376' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9384' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_9391' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_9394' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_9396' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9432' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_9437' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9438' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9501' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9504' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9507' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9508' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9511' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9514' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9517' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9518' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9519' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9520' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9583' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9586' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9589' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9590' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9593' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9596' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9599' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9600' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9601' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9602' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9665' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9668' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9671' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9672' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9675' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9678' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9681' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9682' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9683' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9684' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9747' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9750' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9753' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9754' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9757' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9760' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9763' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9764' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9781' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_9784' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_9786' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9789' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9797' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_9804' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_9807' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_9809' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9834' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_9839' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9840' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9903' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9906' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9909' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9910' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9913' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9916' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9919' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9920' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9921' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9922' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9985' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9988' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9991' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9992' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9995' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9998' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10001' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10002' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10003' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10004' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10067' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10070' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10073' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10074' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10077' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10080' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10083' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10084' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10085' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10086' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10149' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10152' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10155' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10156' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10159' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10162' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10165' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10166' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10183' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_10186' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_10188' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10191' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10199' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_10206' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_10209' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_10211' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_10247' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_10252' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10253' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10316' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10319' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10322' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10323' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10329' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10332' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10333' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10334' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10335' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10398' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10401' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10404' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10405' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10408' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10411' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10414' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10415' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10416' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10417' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10480' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10483' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10486' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10487' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10490' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10493' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10496' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10497' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10498' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10499' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10562' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10565' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10568' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10569' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10572' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10575' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10578' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10579' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10596' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_10599' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_10601' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10604' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10612' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_10619' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_10622' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_10624' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_10649' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_10654' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10655' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10718' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10721' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10724' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10725' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10728' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10731' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10734' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10735' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10736' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10737' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10800' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10803' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10806' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10807' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10810' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10813' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10816' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10817' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10818' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10819' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10882' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10885' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10888' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10889' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10892' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10895' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10898' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10899' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10900' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10901' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10964' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10967' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10970' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10971' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10974' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10977' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10980' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10981' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10998' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_11001' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_11003' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11006' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11014' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11021' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11024' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11026' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11035' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_11068' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_11073' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11074' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11137' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11140' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11143' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11144' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11147' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11150' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11153' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11154' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11155' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11156' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11219' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11222' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11225' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11226' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11229' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11232' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11235' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11236' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11237' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11238' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11301' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11304' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11307' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11308' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11311' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11314' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11317' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11318' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11319' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11320' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11383' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11386' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11389' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11390' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11393' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11396' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11399' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11400' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11417' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_11420' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_11422' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11425' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11433' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11440' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11443' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11445' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11454' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_11476' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_11481' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11482' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11545' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11548' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11551' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11552' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11555' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11558' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11561' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11562' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11563' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11564' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11627' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11630' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11633' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11634' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11637' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11640' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11643' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11644' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11645' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11646' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11709' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11712' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11715' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11716' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11719' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11722' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11725' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11726' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11727' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11728' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11791' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11797' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11798' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11801' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11804' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11807' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11808' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11825' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_11828' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_11830' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11833' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11841' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11848' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11851' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11853' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11889' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_11894' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11895' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11958' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11961' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11964' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11965' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11968' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11971' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11974' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11975' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11976' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11977' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12040' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12043' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12046' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12047' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12050' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12053' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12056' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12057' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12058' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12059' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12122' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12125' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12128' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12129' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12132' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12135' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12138' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12139' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12140' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12141' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12204' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12207' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12210' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12211' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12214' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12217' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12220' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12221' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12238' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_12241' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_12243' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12246' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12254' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_12261' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_12264' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_12266' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_12291' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_12296' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12297' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12360' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12363' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12366' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12367' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12370' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12373' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12376' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12377' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12378' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12379' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12442' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12445' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12448' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12449' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12452' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12455' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12458' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12459' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12460' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12461' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12524' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12527' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12530' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12531' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12534' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12537' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12540' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12541' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12542' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12543' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12606' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12609' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12612' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12613' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12616' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12619' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12622' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12623' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12640' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_12643' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_12645' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12648' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12656' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_12663' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_12666' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_12668' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_12704' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_12709' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12710' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12773' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12776' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12779' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12780' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12783' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12786' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12789' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12790' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12791' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12792' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12855' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12858' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12861' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12862' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12865' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12868' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12871' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12872' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12873' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12874' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12937' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12940' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12943' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12944' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12947' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12950' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12953' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12954' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12955' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12956' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13019' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13022' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13025' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13026' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13029' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13032' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13035' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13036' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13053' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_13056' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_13058' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13061' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13069' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13076' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13079' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13081' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13106' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_13111' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13112' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13175' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13178' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13181' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13182' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13185' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13188' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13191' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13192' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13193' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13194' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13257' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13260' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13263' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13264' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13267' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13270' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13273' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13274' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13275' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13276' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13339' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13342' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13345' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13346' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13349' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13352' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13355' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13356' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13357' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13358' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13421' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13424' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13427' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13428' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13431' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13434' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13437' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13438' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13455' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_13458' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_13460' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13463' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13471' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_13478' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13481' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13483' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13519' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_13524' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13525' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13588' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13591' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13594' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13595' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13598' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13601' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13604' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13605' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13606' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13607' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13670' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13673' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13676' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13677' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13680' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13683' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13686' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13687' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13688' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13689' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13752' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13755' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13758' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13759' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13762' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13765' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13768' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13769' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13770' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13771' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13834' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13837' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13840' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13841' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13844' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13847' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13850' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13851' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13868' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_13871' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_13873' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13876' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13884' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13891' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13894' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13896' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13921' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_13926' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13927' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13990' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13993' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13996' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13997' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14000' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14003' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14006' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14007' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14008' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14009' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14072' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14075' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14078' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14079' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14082' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14085' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14088' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14089' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14090' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14091' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14154' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14157' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14160' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14161' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14164' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14167' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14170' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14171' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14172' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14173' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14236' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14239' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14242' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14243' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14246' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14249' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14252' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14253' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14270' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_14273' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_14275' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14278' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14286' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_14293' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_14296' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_14298' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_14334' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_14339' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14340' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14403' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14406' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14409' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14410' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14413' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14416' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14419' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14420' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14421' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14422' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14485' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14488' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14491' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14492' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14495' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14498' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14501' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14502' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14503' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14504' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14567' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14570' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14573' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14574' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14577' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14580' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14583' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14584' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14585' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14586' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14649' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14652' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14655' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14656' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14659' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14662' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14665' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14666' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14683' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_14686' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_14688' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14691' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14699' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_14706' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_14709' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_14711' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_14736' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_14741' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14742' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14805' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14808' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14811' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14812' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14815' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14818' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14821' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14822' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14823' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14824' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14887' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14890' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14893' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14894' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14897' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14900' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14903' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14904' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14905' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14906' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14969' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14972' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14975' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14976' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14979' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14982' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14985' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14986' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14987' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14988' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15051' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15054' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15057' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15058' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15061' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15064' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15067' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15068' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15085' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_15088' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_15090' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15093' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15101' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15108' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15111' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15113' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15149' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_15154' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15155' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15218' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15221' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15224' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15225' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15228' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15231' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15234' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15235' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15236' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15237' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15300' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15303' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15306' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15307' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15310' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15313' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15316' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15317' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15318' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15319' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15382' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15385' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15388' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15389' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15392' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15395' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15398' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15399' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15400' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15401' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15464' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15467' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15470' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15471' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15474' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15477' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15480' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15481' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15498' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_15501' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_15503' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15506' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15514' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15521' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15524' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15526' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15551' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_15556' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15557' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15620' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15623' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15626' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15627' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15630' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15633' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15636' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15637' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15638' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15639' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15702' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15705' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15708' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15709' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15712' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15715' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15718' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15719' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15720' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15721' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15784' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15787' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15790' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15791' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15797' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15800' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15801' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15802' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15803' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15866' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15869' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15872' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15873' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15876' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15879' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15882' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15883' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15900' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_15903' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_15905' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15908' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15916' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15923' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15926' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15928' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15937' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_15970' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_15975' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15976' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16039' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16042' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16045' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16046' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16049' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16052' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16055' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16056' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16057' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16058' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16121' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16124' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16127' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16128' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16131' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16134' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16137' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16138' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16139' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16140' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16203' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16206' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16209' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16210' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16213' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16216' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16219' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16220' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16221' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16222' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16285' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16288' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16291' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16292' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16295' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16298' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16301' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16302' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16319' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_16322' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_16324' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16327' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16335' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_16342' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_16345' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_16347' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_16356' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_16378' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_16383' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16384' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16447' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16450' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16453' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16454' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16457' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16460' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16463' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16464' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16465' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16466' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16529' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16532' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16535' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16536' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16539' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16542' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16545' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16546' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16547' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16548' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16611' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16614' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16617' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16618' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16621' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16624' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16627' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16628' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16629' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16630' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16693' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16699' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16700' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16703' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16706' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16709' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16710' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16727' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_16730' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_16732' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16735' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16743' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_16750' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_16753' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_16755' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_16791' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_16796' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16797' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16860' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16863' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16866' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16867' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16870' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16873' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16876' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16877' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16878' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16879' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16942' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16945' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16948' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16949' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16952' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16955' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16958' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16959' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16960' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16961' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17024' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17027' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17030' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17031' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17034' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17037' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17040' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17041' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17042' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17043' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17106' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17109' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17112' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17113' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17116' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17119' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17122' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17123' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17140' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_17143' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_17145' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17148' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17156' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17163' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17166' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17168' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_17193' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_17198' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17199' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17262' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17265' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17268' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17269' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17272' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17275' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17278' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17279' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17280' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17281' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17344' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17347' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17350' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17351' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17354' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17357' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17360' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17361' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17362' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17363' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17426' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17429' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17432' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17433' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17436' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17439' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17442' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17443' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17444' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17445' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17508' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17511' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17514' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17515' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17518' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17521' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17524' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17525' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17542' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_17545' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_17547' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17550' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17558' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_17565' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17568' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17570' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_17606' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_17611' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17612' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17675' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17678' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17681' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17682' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17685' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17688' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17691' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17692' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17693' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17694' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17757' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17760' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17763' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17764' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17767' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17770' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17773' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17774' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17775' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17776' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17839' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17842' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17845' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17846' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17849' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17852' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17855' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17856' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17857' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17858' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17921' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17924' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17927' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17928' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17931' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17934' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17937' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17938' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17955' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_17958' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_17960' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17963' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17971' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17978' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17981' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17983' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18008' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_18013' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18014' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18077' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18080' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18083' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18084' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18087' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18090' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18093' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18094' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18095' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18096' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18159' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18162' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18165' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18166' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18169' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18172' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18175' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18176' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18177' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18178' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18241' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18244' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18247' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18248' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18251' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18254' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18257' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18258' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18259' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18260' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18323' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18329' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18330' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18333' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18336' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18339' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18340' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18357' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_18360' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_18362' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18365' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18373' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_18380' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_18383' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_18385' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18421' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_18426' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18427' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18490' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18493' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18496' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18497' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18500' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18503' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18506' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18507' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18508' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18509' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18572' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18575' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18578' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18579' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18582' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18585' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18588' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18589' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18590' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18591' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18654' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18657' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18660' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18661' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18664' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18667' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18670' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18671' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18672' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18673' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18736' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18739' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18742' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18743' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18746' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18749' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18752' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18753' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18770' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_18773' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_18775' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18778' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18786' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_18793' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_18796' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_18798' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18823' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_18828' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18829' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18892' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18895' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18898' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18899' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18902' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18905' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18908' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18909' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18910' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18911' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18974' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18977' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18980' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18981' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18984' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18987' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18990' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18991' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18992' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18993' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19059' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19062' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19063' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19066' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19069' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19072' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19073' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19074' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19075' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19138' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19141' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19144' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19145' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19148' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19151' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19154' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19155' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19172' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_19175' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_19177' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19180' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19188' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_19195' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_19198' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_19200' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_19236' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_19241' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19242' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19305' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19308' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19311' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19312' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19315' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19318' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19321' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19322' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19323' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19324' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19387' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19390' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19393' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19394' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19397' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19400' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19403' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19404' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19405' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19406' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19469' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19472' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19475' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19476' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19479' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19482' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19485' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19486' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19487' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19488' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19551' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19554' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19557' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19558' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19561' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19564' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19567' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19568' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19585' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_19588' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_19590' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19593' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19601' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_19608' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_19611' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_19613' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_19638' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_19643' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19644' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19707' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19710' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19713' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19714' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19717' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19720' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19723' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19724' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19725' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19726' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19789' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19792' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19795' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19796' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19799' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19802' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19805' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19806' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19807' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19808' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19871' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19874' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19877' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19878' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19881' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19884' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19887' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19888' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19889' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19890' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19953' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19956' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19959' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19960' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19963' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19966' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19969' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19970' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19987' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_19990' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_19992' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19995' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20003' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20010' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20013' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20015' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20051' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_20056' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20057' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20120' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20123' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20126' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20127' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20130' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20133' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20136' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20137' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20138' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20139' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20202' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20205' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20208' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20209' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20212' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20215' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20218' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20219' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20220' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20221' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20284' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20287' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20290' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20291' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20294' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20297' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20300' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20301' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20302' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20303' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20366' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20369' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20372' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20373' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20376' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20379' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20382' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20383' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20400' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_20403' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_20405' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20408' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20416' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20423' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20426' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20428' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20453' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_20458' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20459' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20522' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20525' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20528' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20529' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20532' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20535' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20538' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20539' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20540' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20541' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20604' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20607' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20610' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20611' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20614' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20617' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20620' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20621' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20622' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20623' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20686' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20689' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20692' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20693' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20699' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20702' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20703' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20704' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20705' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20768' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20771' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20774' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20775' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20778' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20781' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20784' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20785' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20802' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_20805' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_20807' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20810' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20818' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20825' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20828' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20830' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20839' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_20872' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_20877' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20878' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20941' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20944' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20947' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20948' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20951' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20954' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20957' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20958' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20959' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20960' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21023' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21026' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21029' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21030' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21033' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21036' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21039' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21040' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21041' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_21042' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21105' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21108' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21111' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21112' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21115' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21118' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21121' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21122' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21123' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_21124' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21187' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21190' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21193' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21194' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21197' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21200' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21203' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21204' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21221' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_21224' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_21226' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_21229' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_21237' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_21244' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21247' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21249' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21258' with existing initializer 'val_5318'
[INFO] Replaced initializer 'scalar_tensor_default_1' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21324' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21339' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21342' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21344' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21358' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21375' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21378' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21380' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21383' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21391' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21398' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21401' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21403' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21417' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21434' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21437' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21439' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21442' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21450' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21457' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21460' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21462' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21476' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21493' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21496' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21498' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21501' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21509' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21516' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21519' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21521' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21529' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21532' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21534' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_2' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21558' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21575' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21578' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21580' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21583' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21591' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21598' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21601' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21603' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21617' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21634' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21637' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21639' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21642' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21650' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21657' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21660' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21662' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21676' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21693' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21696' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21698' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21701' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21709' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21716' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21719' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21721' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21735' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21752' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21755' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21757' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21760' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21768' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21775' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21778' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21780' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21788' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21791' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21793' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_3' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21817' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21834' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21837' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21839' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21842' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21850' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21857' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21860' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21862' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21876' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21893' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21896' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21898' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21901' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21909' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21916' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21919' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21921' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21935' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21952' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21955' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21957' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21960' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21968' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21975' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21978' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21980' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21994' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22011' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22014' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22016' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22019' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22027' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22034' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22037' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22039' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22047' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22050' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22052' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_4' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22076' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22093' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22096' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22098' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22101' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22109' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22116' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22119' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22121' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22135' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22152' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22155' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22157' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22160' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22168' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22175' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22178' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22180' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22194' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22211' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22214' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22216' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22219' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22227' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22234' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22237' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22239' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22253' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22270' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22273' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22275' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22278' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22286' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22293' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22296' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22298' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22306' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22309' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22311' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22317' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_22320' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_22324' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22327' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_22334' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22335' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22338' with existing initializer 'val_22331'
[INFO] Replaced initializer 'val_22341' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22344' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22345' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22360' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22361' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22364' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22365' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22500' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22501' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22504' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22505' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22510' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22518' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_22621' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22636' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22637' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22640' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22641' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22646' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22654' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_576' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_22757' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22772' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22773' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22776' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22777' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22782' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22790' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_578' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_579' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_22885' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_22891' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22901' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22906' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22911' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22991' with existing initializer 'val_22983'
[INFO] Replaced initializer 'val_22994' with existing initializer 'val_22992'
[INFO] Replaced initializer 'mul_582' with existing initializer 'mul_581'
[INFO] Replaced initializer 'val_23037' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_23060' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23063' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_23064' with existing initializer 'val_28'
[INFO] Replaced initializer 'scalar_tensor_default_25' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_23110' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23111' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23114' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23115' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23120' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23128' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_23223' with existing initializer 'val_22478'
[INFO] Replaced initializer 'val_23229' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23244' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23245' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23248' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23249' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23254' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23262' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_23357' with existing initializer 'val_22615'
[INFO] Replaced initializer 'val_23363' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23378' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23379' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23382' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23383' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23388' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23396' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_590' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_591' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_23491' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_23497' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23512' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23513' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23516' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23517' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23522' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23530' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_593' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_594' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_23625' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_23631' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23636' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23641' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23646' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23651' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23654' with existing initializer 'val_22915'
[INFO] Replaced initializer 'val_23657' with existing initializer 'val_22919'
[INFO] Replaced initializer 'val_23660' with existing initializer 'val_22923'
[INFO] Replaced initializer 'val_23663' with existing initializer 'val_22927'
[INFO] Replaced initializer 'val_23666' with existing initializer 'val_22931'
[INFO] Replaced initializer 'view_445' with existing initializer 'view_405'
[INFO] Replaced initializer 'val_23714' with existing initializer 'val_22983'
[INFO] Replaced initializer 'view_446' with existing initializer 'view_406'
[INFO] Replaced initializer 'val_23722' with existing initializer 'val_22983'
[INFO] Replaced initializer 'val_23723' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23725' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23730' with existing initializer 'val_22999'
[INFO] Replaced initializer 'mul_596' with existing initializer 'mul_581'
[INFO] Replaced initializer 'mul_597' with existing initializer 'mul_581'
[INFO] Replaced initializer 'val_23761' with existing initializer 'val_23031'
[INFO] Replaced initializer 'val_23767' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23786' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_23789' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23792' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_23793' with existing initializer 'val_28'
[INFO] Replaced initializer 'int64_1_cast' with existing initializer 'val_108'
[INFO] Replaced initializer 'scalar_tensor_default_46' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_23824' with existing initializer 'val_23095'
[INFO] Wrote onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.onnx
[INFO] Rebinding external data...
[INFO] External data: /home/ashim/Documents/projects/vggt/onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.onnx.data
[INFO] Sizes: ONNX=10.1 MB, DATA=4.44 GB
[INFO] Pruning model for PCD-only mode...
[INFO]   Keeping output: cat_322
[INFO]   Keeping output: view_411
[INFO]   Keeping output: view_412
[INFO]   Removing output: view_451
[INFO]   Removing output: view_452
[INFO]   Removing output: unsqueeze
[INFO] Pruned model saved: onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.onnx
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (Half precision with TF32 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd_fp16.engine (2.19 GB)
[INFO] Shared ONNX for subsequent builds: onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.onnx
[INFO] 
[INFO] ======================================================================
[INFO] Building BF16 variant (2/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, bf16 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear4bit (FP4) weight quantisation
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (BFloat16 precision)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled BF16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd_bf16.engine (2.20 GB)
[INFO] 
[INFO] ======================================================================
[INFO] Building FP8 variant (3/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp8 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear4bit (FP4) weight quantisation
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (FP8 precision (RTX 5090 limited support))...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled FP8
[WARNING] FP8 enabled: RTX 5090 has limited FP8 support. Expect 'Unsupported data type FP8' warnings for some ops. TensorRT will fallback to FP16 automatically.
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] FP8 build: Ignore 'Unsupported data type FP8' warnings - this is normal
[INFO] FP8 strict mode: temporarily disabled FP16 fallback
[INFO] FP8 strict mode: temporarily disabled TF32 fallback
[10/24/2025-16:18:28] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-16:18:28] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception unimplemented scalar type!
[10/24/2025-16:18:28] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-16:18:28] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-16:18:28] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-16:19:13] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-16:19:13] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-16:19:13] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-16:19:15] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-16:19:15] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-16:19:15] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[INFO] Strict FP8 build succeeded without fallback precisions: FP16, TF32 disabled
[INFO] Engine saved: onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd_fp8.engine (4.36 GB)
[INFO] 
[INFO] ======================================================================
[INFO] Building INT8 variant (4/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, int8 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear4bit (FP4) weight quantisation
[INFO] INT8 calibration: source=random Gaussian batches=1 seed=1337 gpu=True
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
/home/ashim/Documents/projects/vggt/onnx/vggt_to_trt_chatgpt.py:1691: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.
  config.int8_calibrator = calibrator
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/bitsandbytes-fp4/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-16:21:58] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-16:21:58] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-16:21:58] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-16:21:58] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-16:21:58] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-16:22:43] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-16:22:43] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-16:22:43] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-16:22:43] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-16:22:43] [TRT] [W] Using rough estimation for the inference time of the engine.
[INFO] INT8 calibrator exhausted after 1 batches
[INFO] Calibration cache saved to 'onnx_exports/bitsandbytes-fp4/calibration-8x518x518.cache'
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10619_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17798_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18765_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18773_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18771_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_66_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24213_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13075_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25449_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21661_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_210_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_212_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24410_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24408_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25325_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9753_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26283_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15338_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16509_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_27060_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_334_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_336_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9761_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17485_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16619_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23781_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10932_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17483_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14795_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26877_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11477_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15199_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25305_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_502_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16511_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13913_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_626_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_628_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_650_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11765_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16486_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10072_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15769_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12756_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_772_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_774_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_794_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26875_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15334_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21659_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11042_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22198_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13621_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_918_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_942_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21777_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1064_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1066_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1086_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19061_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19063_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor view_394_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16198_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9755_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18204_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19085_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19087_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1210_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1212_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1234_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22067_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26409_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23375_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13505_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10051_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1378_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1380_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14476_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17056_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25471_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21636_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16625_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1502_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1524_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1526_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23058_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13483_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.token_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19195_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19197_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18657_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26575_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1650_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1670_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1672_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19203_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19201_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17477_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16938_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1794_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1816_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1818_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15653_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25658_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22090_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24532_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor select_203, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1942_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_27022_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1962_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1964_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24534_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11040_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25784_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23354_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2086_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2108_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2110_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22946_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16488_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25512_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14364_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23783_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13185_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13481_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23487_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor mul_581_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor select_202, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2234_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10907_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10501_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24386_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24388_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26553_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24554_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22200_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24556_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2378_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2380_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2400_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10049_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22633_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13077_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25950_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25451_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2524_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2526_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2546_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2548_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8897_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23915_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2670_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2672_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2692_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2694_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15222_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16627_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23921_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23807_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19491_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24264_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26967_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2816_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2818_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2838_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2840_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12623_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12625_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8901_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19514_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19516_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11475_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22206_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17479_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2962_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2964_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2984_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_2986_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9759_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10613_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22925_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10503_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22496_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_24231_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12333_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15630_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11361_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26074_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16940_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3108_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3110_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3130_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3132_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26076_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9326_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25928_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9643_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14045_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23485_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10182_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25806_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19626_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19624_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3256_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3276_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3278_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9191_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19632_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19630_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10611_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17775_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14905_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26721_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.22.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3400_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3422_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3424_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.blocks.23.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3546_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3548_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.patch_embed.norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3568_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3570_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor expand_8_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor val_25_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3606_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor add_49, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor _to_copy_1_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor cat_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3610_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3612_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13623_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15628_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25636_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3720_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3722_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3726_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3728_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor val_19_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor select_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor val_1675_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3733_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3735_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 3984) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor clamp, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor cos_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3736_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor sin_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3737_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11046_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23805_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10930_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor select_2, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3805_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_3807_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4053) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor clamp_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3808_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_3809_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11338_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24680_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24678_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12646_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23056_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25804_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18341_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22923_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8895_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25473_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4028_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4030_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor view_61, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4052_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23377_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10909_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12764_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22088_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24700_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4162_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4164_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4168_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4170_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor select_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4175_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4177_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4407) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor clamp_4, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4178_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.18.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19920_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.token_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor select_6, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4247_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4249_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 4473) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor clamp_5, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4250_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_castHelper_4251_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11359_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19943_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18202_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_19945_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24702_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4470_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4472_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4495_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18343_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12329_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22635_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26429_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11048_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26261_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16057_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.depth_head.norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4603_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4605_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4609_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4611_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9324_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26096_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26098_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25660_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.17.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20053_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20055_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20059_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20061_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23923_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22065_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23062_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14474_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22517_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4899_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4901_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4922_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_4924_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16915_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14341_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16196_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13507_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11904_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26577_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24235_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5032_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5034_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5038_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5040_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25504_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26723_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11788_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15651_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9622_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18655_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14772_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor mul_575_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24254_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16082_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14909_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9216_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13054_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25303_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9332_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26431_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.1.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5328_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5330_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5351_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5353_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22629_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22204_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9620_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5461_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5463_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5467_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5469_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25514_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25159_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26263_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25506_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12196_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26407_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17773_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20349_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20351_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15340_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23493_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20372_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20374_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 23008) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor view_403_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5757_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5759_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5780_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5782_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24824_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26555_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24826_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25782_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5890_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5892_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor view_402_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5896_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_5898_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11469_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25157_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20482_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10184_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20484_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20488_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20490_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14047_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14770_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24846_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24848_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.2.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6186_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_27012_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6209_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6211_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16192_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_27010_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6319_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6321_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6325_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17048_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24879_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24881_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12762_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor val_21545_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 23703) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10478_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9214_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24889_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24887_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14366_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6640_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17369_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23491_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15763_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25181_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 24933) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26129_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor val_22063_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26131_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6748_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6750_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6754_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_6756_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14053_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.12.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17912_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9193_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13935_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor select_193, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18225_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26285_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15224_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.19.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20778_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20780_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.3.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7044_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7046_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7067_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7069_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.depth_head.norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22494_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20801_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20803_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24262_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11898_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7177_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7179_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7183_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7185_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13052_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10188_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14051_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor val_21286_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.16.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26701_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23064_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15332_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17796_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11471_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11790_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12219_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17346_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18337_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25930_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26139_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26137_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25327_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15201_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20913_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7473_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7475_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7497_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7499_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_20919_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12217_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11336_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11906_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.10.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13911_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21771_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14343_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7607_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7609_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7613_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12758_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.13.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11900_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.17.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17050_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_14793_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13615_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24211_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.21.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25011_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17914_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18633_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25013_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18227_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.4.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7903_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7905_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7927_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_7929_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10480_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16190_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16621_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10074_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9330_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17908_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 24318) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13193_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8037_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8039_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8043_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8045_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25033_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13191_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25035_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13187_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25638_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.11.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.22.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_13937_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21207_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21209_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.15.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8333_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8335_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8356_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_25952_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21230_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21232_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23917_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.23.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21769_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12194_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17367_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.9.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8466_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8468_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor view_395_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8472_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8474_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.22.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_18631_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_23352_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_24237_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26965_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22948_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_9645_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16080_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22627_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15761_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17344_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_15767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.13.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.12.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_16059_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.14.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.11.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.16.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_26699_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_22519_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.8.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.23.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21340_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21342_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21775_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.k_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.21.attn.q_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.20.attn.k_norm.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.5.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8762_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8764_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.6.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8785_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_8787_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21348_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.7.norm1.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_21346_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_10617_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_17906_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.global_blocks.10.attn.q_norm.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_11767_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.aggregator.frame_blocks.15.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor model.camera_head.trunk.0.norm2.bias_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor val_21804_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[10/24/2025-16:28:57] [TRT] [W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_12648_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[INFO] Engine saved: onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd_int8.engine (2.19 GB)
[INFO] 
[INFO] ======================================================================
[INFO] BUILD COMPLETE!
[INFO] ======================================================================
[INFO] Input shape: [8, 3, 518, 518]
[INFO] Mode: PCD-only (depth + camera heads)
[INFO] 
[INFO] Built engines:
[INFO]   FP16  (Half precision with TF32 fallback       ): onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd_fp16.engine
[INFO]         Size: 2.19 GB
[INFO]   BF16  (BFloat16 precision                      ): onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd_bf16.engine
[INFO]         Size: 2.20 GB
[INFO]   FP8   (FP8 precision (RTX 5090 limited support)): onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd_fp8.engine
[INFO]         Size: 4.36 GB
[INFO]   INT8  (INT8 quantization with FP16 fallback    ): onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd_int8.engine
[INFO]         Size: 2.19 GB
[INFO] 
[INFO] Next steps for live PCD construction:
[INFO]   1. Use FP16 for best speed/quality balance on RTX 5090
[INFO]   2. Extract outputs: cat_322 (camera), view_411/412 (depth)
[INFO]   3. Unproject depth to 3D using camera parameters
[INFO]   4. Expected latency: ~170ms FP16, ~230ms BF16 (from your benchmark)
[INFO]   5. With --pcd-only: expect ~30% faster (~120ms FP16)
[INFO] 
[INFO] Performance hierarchy (your RTX 5090):
[INFO]   FP16:  170ms (5.9 FPS)  RECOMMENDED for live PCD
[INFO]   BF16:  232ms (4.3 FPS)
[INFO]   FP8:   173ms (5.8 FPS) - similar to FP16, limited HW support
[INFO]   INT8:  TBD (needs proper calibration data)
[INFO] 
[INFO] Optimizations applied:
[INFO]    Timing cache (speeds up rebuilds)
[INFO]    Optimization level 5 (maximum)
[INFO]    Auxiliary streams: 4
[INFO]    TF32 enabled (Ampere+ GPUs)
[INFO]    All CUDA tactic sources (cuBLAS, cuDNN)
[INFO]    Workspace: 32 GB
[INFO]    FIXED: INT8 calibrator (TRT 10+ compatible)
[INFO]    FIXED: Opset handling (onnxsim now works)
[INFO] ======================================================================
Applied 1260 of general pattern rewrite rules.

--- Building extra precision fp32 for quant mode bitsandbytes-fp4 ---

>>> python onnx/vggt_to_trt_chatgpt.py --onnx-in onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.onnx --precision fp32 --quant-mode bitsandbytes-fp4 --output-dir onnx_exports/bitsandbytes-fp4 --num-cams 8 --height 518 --width 518 --model-name facebook/VGGT-1B --calib-batches 1 --calib-seed 1337 --pcd-only
[INFO] 
[INFO] ======================================================================
[INFO] Building FP32 variant (1/1)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp32 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: bitsandbytes Linear4bit (FP4) weight quantisation
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (Full precision (baseline))...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] Engine saved: onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.engine (4.34 GB)
[INFO] 
[INFO] ======================================================================
[INFO] BUILD COMPLETE!
[INFO] ======================================================================
[INFO] Input shape: [8, 3, 518, 518]
[INFO] Mode: PCD-only (depth + camera heads)
[INFO] 
[INFO] Built engines:
[INFO]   FP32  (Full precision (baseline)               ): onnx_exports/bitsandbytes-fp4/vggt-8x3x518x518-pcd.engine
[INFO]         Size: 4.34 GB
[INFO] 
[INFO] Next steps for live PCD construction:
[INFO]   1. Use FP16 for best speed/quality balance on RTX 5090
[INFO]   2. Extract outputs: cat_322 (camera), view_411/412 (depth)
[INFO]   3. Unproject depth to 3D using camera parameters
[INFO]   4. Expected latency: ~170ms FP16, ~230ms BF16 (from your benchmark)
[INFO]   5. With --pcd-only: expect ~30% faster (~120ms FP16)
[INFO] 
[INFO] Performance hierarchy (your RTX 5090):
[INFO]   FP16:  170ms (5.9 FPS)  RECOMMENDED for live PCD
[INFO]   BF16:  232ms (4.3 FPS)
[INFO]   FP8:   173ms (5.8 FPS) - similar to FP16, limited HW support
[INFO]   INT8:  TBD (needs proper calibration data)
[INFO] 
[INFO] Optimizations applied:
[INFO]    Timing cache (speeds up rebuilds)
[INFO]    Optimization level 5 (maximum)
[INFO]    Auxiliary streams: 4
[INFO]    TF32 enabled (Ampere+ GPUs)
[INFO]    All CUDA tactic sources (cuBLAS, cuDNN)
[INFO]    Workspace: 32 GB
[INFO]    FIXED: INT8 calibrator (TRT 10+ compatible)
[INFO]    FIXED: Opset handling (onnxsim now works)
[INFO] ======================================================================

================================================================
Quantisation mode: modelopt-fp8
Output directory : onnx_exports/modelopt-fp8
================================================================
[INFO] Missing core precisions for quant mode 'modelopt-fp8': fp16 bf16 fp8 int8

>>> python onnx/vggt_to_trt_chatgpt.py --export --all-precisions --quant-mode modelopt-fp8 --output-dir onnx_exports/modelopt-fp8 --num-cams 8 --height 518 --width 518 --model-name facebook/VGGT-1B --calib-batches 1 --calib-seed 1337 --pcd-only
[INFO] Building all precision variants: fp16, bf16, fp8, int8
[INFO] 
[INFO] ======================================================================
[INFO] Building FP16 variant (1/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp16 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: NVIDIA ModelOpt FP8 recipe
[INFO] Loading model: facebook/VGGT-1B
[INFO] using MLP layer as FFN
[INFO] Modifying model for PCD-only export...
[WARNING] PCD-only mode: will prune unused outputs after export
[INFO] Applying pre-quantisation: NVIDIA ModelOpt FP8 recipe
df: /home/ashim/.triton/autotune: No such file or directory
[INFO] PyTorch version 2.8.0+cu129 available.
[ERROR] Quantisation mode 'modelopt-fp8' failed prior to export: modelopt.torch.quantization.quantize_model not available. Ensure you installed modelopt>=0.9.
[INFO] Retrying export with fallback quantisation mode 'none' (2/2)
[INFO] using MLP layer as FFN
[INFO] Exporting to ONNX with shape [8, 3, 518, 518] on cuda:0 (quant='none')
/home/ashim/miniconda3/envs/compvis/lib/python3.10/site-packages/vggt/models/vggt.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
[INFO] Removed 851 unused nodes
[INFO] No unused functions to remove
[INFO] Skipping constant folding for node 'node_unsqueeze' because it is graph input to preserve graph signature
[INFO] Skipping constant folding for node Node(name='node_cat_3', domain='', op_type='Concat', inputs=(Value(name='expand_4', type=Tensor(FLOAT), shape=Shape([1, 1, 4, 1024]), producer='node_Constant_23972', index=0, const_value={Tensor(...)}), Value(name='expand_5', type=Tensor(FLOAT), shape=Shape([1, 7, 4, 1024]), producer='node_Constant_23994', index=0, const_value={Tensor(...)})), attributes={'axis': Attr('axis', INT, 1)}, overload='', outputs=(SymbolicTensor(name='cat_3', type=Tensor(FLOAT), shape=Shape([1, 8, 4, 1024]), producer='node_cat_3', index=0),), version=13, doc_string=None) due to large input sizes: [4096, 28672]
[INFO] Skipping constant folding for node Node(name='node_clone_74', domain='', op_type='Identity', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='clone_74', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_clone_74', index=0),), version=16, doc_string=None) due to large input sizes: [21904]
[INFO] Skipping constant folding for node Node(name='node_add_49', domain='', op_type='Add', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_25', type=Tensor(INT64), shape=Shape([]), producer='node_Constant_25', index=0, const_value={Tensor<INT64,[]>(array(1), name=None)})), attributes={}, overload='', outputs=(SymbolicTensor(name='add_49', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_add_49', index=0),), version=14, doc_string=None) due to large input sizes: [21904, 1]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20794', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21278', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_20794', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20797', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21281', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20797', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20803', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21291', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20803', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20845', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21333', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20845', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20847', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21337', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20847', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20856', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21346', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20856', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20858', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21350', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20858', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20900', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21392', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20900', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20902', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21396', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20902', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20911', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21405', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20911', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20913', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21409', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20913', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20955', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21451', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20955', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20957', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21455', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20957', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20966', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21464', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20966', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20968', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21468', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20968', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21010', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21510', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21010', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21012', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21514', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21012', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21021', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21523', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21021', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21023', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21527', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21023', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21032', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21536', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21032', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21034', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21538', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21034', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21037', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21541', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21037', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21042', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21550', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21042', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21084', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21592', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21084', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21086', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21596', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21086', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21095', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21605', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21095', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21097', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21609', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21097', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21139', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21651', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21139', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21141', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21655', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21141', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21150', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21664', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21150', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21152', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21668', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21152', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21194', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21710', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21194', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21196', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21714', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21196', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21205', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21723', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21205', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21207', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21727', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21207', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21249', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21769', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21249', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21251', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21773', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21251', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21260', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21782', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21260', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21262', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21786', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21262', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21271', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21795', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21271', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21273', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21797', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21273', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21276', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21800', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21276', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21281', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21809', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21281', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21323', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21851', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21323', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21325', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21855', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21325', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21334', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21864', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21334', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21336', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21868', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21336', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21378', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21910', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21378', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21380', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21914', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21380', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21389', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21923', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21389', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21391', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21927', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21391', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21433', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21969', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21433', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21435', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21973', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21435', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21444', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21982', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21444', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21446', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21986', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21446', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21488', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22028', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21488', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21490', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22032', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21490', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21499', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22041', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21499', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21501', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22045', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21501', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21510', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22054', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21510', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21512', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22056', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21512', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21515', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22059', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21515', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21520', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22068', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21520', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21562', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22110', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21562', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21564', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22114', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21564', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21573', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22123', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21573', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21575', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22127', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21575', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21617', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22169', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21617', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21619', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22173', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21619', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21628', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22182', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21628', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21630', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22186', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21630', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21672', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22228', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21672', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21674', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22232', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21674', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21683', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22241', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21683', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21685', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22245', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21685', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21727', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22287', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21727', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21729', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22291', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21729', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21738', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22300', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21738', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21740', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22304', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21740', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21749', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22313', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21749', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_sin_1', domain='', op_type='Sin', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_1', domain='', op_type='Cos', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_2', domain='', op_type='Sin', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_2', domain='', op_type='Cos', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_3', domain='', op_type='Sin', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_3', domain='', op_type='Cos', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_4', domain='', op_type='Sin', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_4', domain='', op_type='Cos', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_5 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_5', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_5', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_6 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_6', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_6', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_7 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_7', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_7', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_8 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_8', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_8', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_22 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_23 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22421', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_23', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_23', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22992', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22420', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22992')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22993', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22421', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22423', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_22', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_22', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22994', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22422', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22994')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22995', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22423', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_sin_11', domain='', op_type='Sin', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_11', domain='', op_type='Cos', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_12', domain='', op_type='Sin', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_12', domain='', op_type='Cos', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_13', domain='', op_type='Sin', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_13', domain='', op_type='Cos', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_14', domain='', op_type='Sin', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_14', domain='', op_type='Cos', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_15 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_15', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_15', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_16 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_16', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_16', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_17 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_17', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_17', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_18 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_18', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_18', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_37 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_38 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23144', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_38', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_38', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23723', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23143', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23723')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23724', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23144', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23146', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_37', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_37', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23725', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23145', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23725')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23726', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23146', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] Removed 19332 unused nodes
[INFO] No unused functions to remove
[INFO] Skipping constant folding for node 'node_unsqueeze' because it is graph input to preserve graph signature
[INFO] Skipping constant folding for node Node(name='node_cat_3', domain='', op_type='Concat', inputs=(Value(name='expand_4', type=Tensor(FLOAT), shape=Shape([1, 1, 4, 1024]), producer='node_Constant_23972', index=0, const_value={Tensor(...)}), Value(name='expand_5', type=Tensor(FLOAT), shape=Shape([1, 7, 4, 1024]), producer='node_Constant_23994', index=0, const_value={Tensor(...)})), attributes={'axis': Attr('axis', INT, 1)}, overload='', outputs=(SymbolicTensor(name='cat_3', type=Tensor(FLOAT), shape=Shape([1, 8, 4, 1024]), producer='node_cat_3', index=0),), version=13, doc_string=None) due to large input sizes: [4096, 28672]
[INFO] Skipping constant folding for node Node(name='node_add_49', domain='', op_type='Add', inputs=(Value(name='expand_8', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_Constant_24028', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_25', type=Tensor(INT64), shape=Shape([]), producer='node_Constant_25', index=0, const_value={Tensor<INT64,[]>(array(1), name=None)})), attributes={}, overload='', outputs=(SymbolicTensor(name='add_49', type=Tensor(INT64), shape=Shape([8, 1369, 2]), producer='node_add_49', index=0),), version=14, doc_string=None) due to large input sizes: [21904, 1]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20794', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21278', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_20794', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20797', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21281', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20797', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20803', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21291', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20803', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20845', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21333', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20845', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20847', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21337', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20847', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20856', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21346', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20856', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20858', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21350', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20858', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20900', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21392', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20900', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20902', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21396', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20902', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20911', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21405', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20911', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20913', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21409', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20913', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20955', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21451', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_20955', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20957', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21455', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_20957', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20966', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21464', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_20966', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_20968', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21468', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_20968', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21010', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21510', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21010', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21012', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21514', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21012', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21021', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21523', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21021', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21023', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21527', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21023', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21032', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21536', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21032', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21034', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21538', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21034', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21037', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21541', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21037', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21042', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21550', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21042', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21084', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21592', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21084', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21086', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21596', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21086', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21095', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21605', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21095', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21097', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21609', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21097', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21139', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21651', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21139', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21141', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21655', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21141', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21150', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21664', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21150', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21152', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21668', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21152', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21194', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21710', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21194', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21196', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21714', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21196', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21205', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21723', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21205', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21207', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21727', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21207', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21249', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21769', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21249', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21251', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21773', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21251', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21260', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21782', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21260', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21262', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21786', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21262', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21271', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21795', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21271', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21273', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21797', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21273', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21276', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21800', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21276', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21281', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21809', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21281', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21323', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21851', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21323', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21325', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21855', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21325', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21334', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21864', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21334', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21336', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21868', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21336', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21378', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21910', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21378', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21380', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21914', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21380', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21389', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21923', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21389', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21391', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21927', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21391', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21433', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21969', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21433', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21435', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21973', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21435', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21444', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21982', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21444', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21446', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_21986', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21446', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21488', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22028', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21488', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21490', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22032', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21490', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21499', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22041', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21499', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21501', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22045', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21501', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21510', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22054', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21510', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21512', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.embed_pose.weight', type=Tensor(FLOAT), shape=Shape([2048, 9]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22056', type=Tensor(FLOAT), shape=Shape([9, 2048]), producer='node_Transpose_21512', index=0),), version=13, doc_string=None) due to large input sizes: [18432]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21515', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.poseLN_modulation.1.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22059', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21515', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21520', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22068', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21520', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21562', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22110', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21562', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21564', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22114', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21564', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21573', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.0.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22123', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21573', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21575', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22127', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21575', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21617', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22169', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21617', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21619', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22173', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21619', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21628', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.1.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22182', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21628', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21630', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22186', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21630', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21672', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22228', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21672', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21674', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22232', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21674', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21683', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.2.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22241', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21683', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21685', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.qkv.weight', type=Tensor(FLOAT), shape=Shape([6144, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22245', type=Tensor(FLOAT), shape=Shape([2048, 6144]), producer='node_Transpose_21685', index=0),), version=13, doc_string=None) due to large input sizes: [12582912]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21727', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.attn.proj.weight', type=Tensor(FLOAT), shape=Shape([2048, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22287', type=Tensor(FLOAT), shape=Shape([2048, 2048]), producer='node_Transpose_21727', index=0),), version=13, doc_string=None) due to large input sizes: [4194304]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21729', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc1.weight', type=Tensor(FLOAT), shape=Shape([8192, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22291', type=Tensor(FLOAT), shape=Shape([2048, 8192]), producer='node_Transpose_21729', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21738', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.trunk.3.mlp.fc2.weight', type=Tensor(FLOAT), shape=Shape([2048, 8192]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22300', type=Tensor(FLOAT), shape=Shape([8192, 2048]), producer='node_Transpose_21738', index=0),), version=13, doc_string=None) due to large input sizes: [16777216]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21740', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc1.weight', type=Tensor(FLOAT), shape=Shape([1024, 2048]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22304', type=Tensor(FLOAT), shape=Shape([2048, 1024]), producer='node_Transpose_21740', index=0),), version=13, doc_string=None) due to large input sizes: [2097152]
[INFO] Skipping constant folding for node Node(name='node_Transpose_21749', domain='', op_type='Transpose', inputs=(SymbolicTensor(name='model.camera_head.pose_branch.fc2.weight', type=Tensor(FLOAT), shape=Shape([9, 1024]), const_value={TorchTensor(...)}),), attributes={'perm': Attr('perm', INTS, (1, 0))}, overload='', outputs=(SymbolicTensor(name='val_22313', type=Tensor(FLOAT), shape=Shape([1024, 9]), producer='node_Transpose_21749', index=0),), version=13, doc_string=None) due to large input sizes: [9216]
[INFO] Skipping constant folding for node Node(name='node_sin_1', domain='', op_type='Sin', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_1', domain='', op_type='Cos', inputs=(Value(name='einsum_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40940', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_1', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_1', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_2', domain='', op_type='Sin', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_2', domain='', op_type='Cos', inputs=(Value(name='einsum_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_40961', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_2', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_2', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_3', domain='', op_type='Sin', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_3', domain='', op_type='Cos', inputs=(Value(name='einsum_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41061', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_3', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_3', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_4', domain='', op_type='Sin', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_4', domain='', op_type='Cos', inputs=(Value(name='einsum_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41082', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_4', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_4', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_5 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_5', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_5', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_5', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_5', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_5', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_6 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_6', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_6', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_6', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_6', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_6', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_7 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_7', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_7', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_7', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_7', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_7', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_8 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_8', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_8', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_8', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_8', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_8', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_22 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_23 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22421', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_23', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_23', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22992', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22420', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22992')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22993', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22421', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_22423', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_22', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_22', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_22994', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_22422', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_22994')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_22995', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_22423', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_sin_11', domain='', op_type='Sin', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_11', domain='', op_type='Cos', inputs=(Value(name='einsum_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41584', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_11', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_11', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_12', domain='', op_type='Sin', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_sin_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_cos_12', domain='', op_type='Cos', inputs=(Value(name='einsum_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_Constant_41605', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_12', type=Tensor(DOUBLE), shape=Shape([1369, 64]), producer='node_cos_12', index=0),), version=7, doc_string=None) due to large input sizes: [87616]
[INFO] Skipping constant folding for node Node(name='node_sin_13', domain='', op_type='Sin', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_13', domain='', op_type='Cos', inputs=(Value(name='einsum_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41705', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_13', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_13', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_sin_14', domain='', op_type='Sin', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_sin_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skipping constant folding for node Node(name='node_cos_14', domain='', op_type='Cos', inputs=(Value(name='einsum_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_Constant_41726', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_14', type=Tensor(DOUBLE), shape=Shape([1369, 128]), producer='node_cos_14', index=0),), version=7, doc_string=None) due to large input sizes: [175232]
[INFO] Skip storing constant folded nvalue einsum_15 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_15', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_15', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_15', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_15', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_15', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_16 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_16', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_16', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_16', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_16', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_16', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_17 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_17', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_17', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_17', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_17', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_17', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue einsum_18 due to large size 350464.
[INFO] Skipping constant folding for node Node(name='node_sin_18', domain='', op_type='Sin', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='sin_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_sin_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skipping constant folding for node Node(name='node_cos_18', domain='', op_type='Cos', inputs=(SymbolicTensor(name='einsum_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_einsum_18', index=0, const_value={Tensor(...)}),), attributes={}, overload='', outputs=(SymbolicTensor(name='cos_18', type=Tensor(DOUBLE), shape=Shape([1369, 256]), producer='node_cos_18', index=0),), version=7, doc_string=None) due to large input sizes: [350464]
[INFO] Skip storing constant folded nvalue expand_37 due to large size 268324.
[INFO] Skip storing constant folded nvalue expand_38 due to large size 268324.
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23144', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_38', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_38', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23723', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23143', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23723')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23724', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23144', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] Skipping constant folding for node Node(name='node_Unsqueeze_23146', domain='', op_type='Unsqueeze', inputs=(SymbolicTensor(name='expand_37', type=Tensor(FLOAT), shape=Shape([518, 518]), producer='node_expand_37', index=0, const_value={Tensor(...)}), SymbolicTensor(name='val_23725', type=Tensor(INT64), shape=Shape([1]), producer='node_Constant_23145', index=0, const_value={Tensor<INT64,[1]>(array([-1]), name='val_23725')})), attributes={}, overload='', outputs=(SymbolicTensor(name='val_23726', type=Tensor(FLOAT), shape=Shape([518, 518, 1]), producer='node_Unsqueeze_23146', index=0),), version=13, doc_string=None) due to large input sizes: [268324, 1]
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'start' is not 0.
[INFO] The value 'end' is less than the shape of the specified axis.
[INFO] Removed 1189 unused nodes
[INFO] No unused functions to remove
[INFO] Replaced initializer 'val_2878' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_2889' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_3291' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_3693' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_3704' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_4106' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_4508' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4519' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_4921' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_5329' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_5340' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_5748' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6150' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6161' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_6563' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6965' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6976' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_7378' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_7780' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_7791' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_8193' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_8595' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8606' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_9008' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_9410' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_9421' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_9823' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_10225' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_10236' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_10638' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11046' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11057' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_11465' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11867' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11878' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_12280' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_12682' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_12693' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_13095' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_13497' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13508' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_13910' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_14312' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_14323' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_14725' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15127' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15138' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_15540' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15948' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15959' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_16367' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_16769' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_16780' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_17182' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_17584' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17595' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_17997' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_18399' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_18410' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_18812' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_19214' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_19225' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_19627' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20029' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20040' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_20442' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20850' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20861' with existing initializer 'val_2074'
[INFO] Replaced initializer 'val_24' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_31' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_32' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_46' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_50' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_51' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_90' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_98' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_124' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_141' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_144' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_146' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_149' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_157' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_164' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_167' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_169' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_183' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_200' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_203' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_205' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_208' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_216' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_223' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_226' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_228' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_242' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_259' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_262' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_264' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_267' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_275' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_282' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_285' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_287' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_301' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_318' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_321' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_323' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_326' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_334' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_341' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_344' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_346' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_360' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_377' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_380' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_382' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_385' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_393' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_400' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_403' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_405' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_419' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_436' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_439' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_441' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_444' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_452' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_459' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_462' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_464' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_478' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_495' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_498' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_500' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_503' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_511' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_518' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_521' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_523' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_537' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_554' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_557' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_559' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_562' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_570' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_577' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_580' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_582' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_596' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_613' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_616' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_618' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_621' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_629' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_636' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_639' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_641' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_655' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_672' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_675' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_677' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_680' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_688' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_695' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_698' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_700' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_714' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_731' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_734' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_736' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_739' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_747' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_754' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_757' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_759' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_773' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_790' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_793' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_795' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_798' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_806' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_813' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_816' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_818' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_832' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_849' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_852' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_854' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_857' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_865' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_872' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_875' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_877' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_891' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_908' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_911' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_913' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_916' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_924' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_931' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_934' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_936' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_950' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_967' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_970' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_972' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_975' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_983' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_990' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_993' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_995' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1009' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1026' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1029' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1031' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1034' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1042' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1049' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1052' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1054' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1068' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1085' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1088' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1090' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1093' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1101' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1108' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1111' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1113' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1127' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1144' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1147' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1149' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1152' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1160' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1167' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1170' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1172' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1186' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1203' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1206' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1208' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1211' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1219' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1226' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1229' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1231' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1245' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1262' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1265' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1267' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1270' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1278' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1285' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1288' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1290' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1304' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1321' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1324' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1326' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1329' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1337' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1344' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1347' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1349' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1363' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1380' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1383' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1385' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1388' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1396' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1403' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1406' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1408' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1422' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1439' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_1442' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_1444' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1447' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_1455' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_1462' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_1465' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_1467' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_1483' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1486' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1487' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1662' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_1673' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1686' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1740' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1748' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1751' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1754' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1757' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1758' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1759' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1760' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1823' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1826' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1829' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1830' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1833' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1836' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1839' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1840' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1841' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1842' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1905' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1908' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1911' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1912' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1915' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1918' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_1921' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1922' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1923' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_1924' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_1987' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_1990' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_1993' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_1994' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_1997' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2000' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2003' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2004' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2021' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_2024' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_2026' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2029' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2037' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_2044' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2047' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2049' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2090' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2091' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2154' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2157' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2160' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2161' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2164' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2167' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2170' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2171' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2172' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2173' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2236' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2239' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2242' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2243' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2246' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2249' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2252' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2253' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2254' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2255' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2318' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2321' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2324' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2325' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2328' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2331' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2334' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2335' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2336' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2337' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2400' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2403' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2406' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2407' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2410' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2413' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2416' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2417' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2439' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2442' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2450' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_2457' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2460' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2462' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2487' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_2492' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2493' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2556' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2559' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2562' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2563' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2566' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2569' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2572' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2573' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2574' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2575' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2638' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2641' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2644' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2645' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2648' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2651' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2654' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2655' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2656' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2657' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2720' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2723' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2726' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2727' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2730' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2733' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2736' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2737' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2738' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2739' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2802' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2805' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2808' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2809' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2812' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2815' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2818' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2819' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2836' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_2839' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_2841' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2844' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_2852' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_2859' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_2862' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_2864' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_2900' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_2905' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2906' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_2969' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_2972' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2975' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2976' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2979' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_2982' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_2985' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_2986' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_2987' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_2988' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3051' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3054' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3057' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3058' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3061' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3064' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3067' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3068' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3069' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3070' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3133' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3136' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3139' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3140' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3143' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3146' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3149' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3150' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3151' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3152' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3215' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3218' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3221' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3222' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3225' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3228' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3231' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3232' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3249' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_3252' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_3254' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3257' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3265' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_3272' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_3275' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_3277' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_3302' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_3307' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3308' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3371' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3374' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3377' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3378' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3381' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3384' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3387' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3388' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3389' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3390' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3453' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3456' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3459' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3460' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3463' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3466' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3469' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3470' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3471' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3472' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3535' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3538' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3541' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3542' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3545' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3548' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3551' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3552' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3553' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3554' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3617' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3620' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3623' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3624' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3627' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3630' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3633' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3634' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3651' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_3654' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_3656' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3659' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_3667' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_3674' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_3677' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_3679' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_3715' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_3720' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3721' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3784' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3787' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3790' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3791' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3797' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3800' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3801' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3802' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3803' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3866' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3869' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3872' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3873' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3876' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3879' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3882' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3883' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3884' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3885' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_3948' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_3951' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3954' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3955' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3958' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_3961' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_3964' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_3965' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_3966' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_3967' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4030' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4033' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4036' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4037' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4040' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4043' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4046' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4047' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4064' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_4067' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_4069' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4072' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4080' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4087' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4090' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4092' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4117' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_4122' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4123' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4186' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4189' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4192' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4193' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4196' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4199' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4202' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4203' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4204' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4205' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4268' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4271' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4274' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4275' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4278' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4281' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4284' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4285' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4286' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4287' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4350' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4353' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4356' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4357' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4360' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4363' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4366' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4367' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4368' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4369' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4432' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4435' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4438' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4439' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4442' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4445' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4448' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4449' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4466' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_4469' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_4471' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4474' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4482' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_4489' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4492' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4494' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4530' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_4535' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4536' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4599' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4602' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4605' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4606' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4609' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4612' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4615' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4616' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4617' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4618' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4681' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4684' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4687' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4688' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4691' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4694' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4697' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4698' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4699' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4700' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4763' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4766' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4769' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4770' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4773' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4776' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4779' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4780' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4781' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4782' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_4845' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_4848' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4851' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4852' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4855' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_4858' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_4861' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_4862' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_4879' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_4882' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_4884' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4887' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_4895' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_4902' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_4905' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_4907' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_4932' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_4937' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_4938' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5001' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5004' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5007' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5008' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5011' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5014' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5017' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5018' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5019' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5020' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5083' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5086' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5089' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5090' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5093' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5096' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5099' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5100' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5101' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5102' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5165' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5168' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5171' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5172' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5175' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5178' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5181' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5182' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5183' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5184' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5247' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5250' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5253' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5254' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5257' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5260' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5263' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5264' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5281' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_5284' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_5286' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5289' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5297' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_5304' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_5307' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_5309' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_5351' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_5356' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5357' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5420' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5423' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5426' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5427' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5430' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5433' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5436' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5437' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5438' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5439' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5502' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5505' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5508' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5509' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5512' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5515' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5518' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5519' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5520' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5521' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5584' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5587' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5590' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5591' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5594' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5597' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5600' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5601' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5602' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5603' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5666' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5669' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5672' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5673' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5676' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5679' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5682' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5683' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5700' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_5703' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_5705' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5708' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_5716' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_5723' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_5726' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_5728' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_5737' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_5759' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_5764' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5765' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5828' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5831' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5834' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5835' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5838' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5841' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5844' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5845' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5846' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5847' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5910' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5913' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5916' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5917' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5920' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5923' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_5926' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5927' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_5928' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_5929' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_5992' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_5995' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_5998' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_5999' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6002' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6005' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6008' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6009' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6010' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6011' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6074' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6077' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6080' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6081' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6084' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6087' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6090' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6091' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6108' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_6111' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_6113' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6116' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6124' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6131' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6134' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6136' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6172' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_6177' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6178' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6241' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6244' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6247' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6248' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6251' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6254' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6257' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6258' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6259' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6260' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6323' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6329' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6330' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6333' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6336' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6339' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6340' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6341' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6342' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6405' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6408' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6411' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6412' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6415' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6418' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6421' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6422' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6423' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6424' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6487' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6490' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6493' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6494' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6497' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6500' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6503' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6504' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6521' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_6524' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_6526' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6529' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6537' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_6544' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6547' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6549' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6574' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_6579' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6580' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6643' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6646' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6649' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6650' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6653' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6656' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6659' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6660' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6661' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6662' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6725' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6728' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6731' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6732' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6735' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6738' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6741' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6742' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6743' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6744' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6807' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6810' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6813' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6814' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6817' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6820' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6823' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6824' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6825' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6826' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_6889' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_6892' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6895' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6896' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6899' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_6902' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_6905' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_6906' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_6923' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_6926' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_6928' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6931' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_6939' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_6946' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_6949' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_6951' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_6987' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_6992' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_6993' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7059' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7062' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7063' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7066' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7069' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7072' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7073' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7074' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7075' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7138' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7141' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7144' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7145' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7148' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7151' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7154' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7155' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7156' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7157' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7220' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7223' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7226' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7227' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7230' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7233' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7236' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7237' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7238' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7239' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7302' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7305' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7308' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7309' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7312' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7315' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7318' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7319' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7336' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_7339' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_7341' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7344' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7352' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_7359' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_7362' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_7364' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_7389' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_7394' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7395' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7458' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7461' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7464' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7465' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7468' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7471' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7474' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7475' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7476' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7477' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7540' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7543' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7546' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7547' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7550' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7553' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7556' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7557' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7558' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7559' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7622' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7625' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7628' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7629' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7632' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7635' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7638' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7639' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7640' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7641' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7704' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7707' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7710' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7711' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7714' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7717' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7720' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7721' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7738' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_7741' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_7743' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7746' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_7754' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_7761' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_7764' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_7766' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_7802' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_7807' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7808' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7871' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7874' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7877' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7878' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7881' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7884' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7887' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7888' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7889' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7890' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_7953' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_7956' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7959' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7960' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7963' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_7966' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_7969' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_7970' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_7971' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_7972' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8035' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8038' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8041' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8042' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8045' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8048' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8051' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8052' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8053' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8054' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8117' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8120' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8123' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8124' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8127' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8130' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8133' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8134' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8151' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_8154' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_8156' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8159' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8167' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8174' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8177' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8179' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_8204' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_8209' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8210' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8273' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8276' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8279' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8280' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8283' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8286' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8289' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8290' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8291' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8292' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8355' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8358' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8361' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8362' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8365' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8368' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8371' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8372' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8373' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8374' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8437' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8440' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8443' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8444' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8447' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8450' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8453' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8454' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8455' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8456' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8519' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8522' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8525' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8526' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8529' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8532' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8535' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8536' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8553' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_8556' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_8558' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8561' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8569' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_8576' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8579' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8581' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_8617' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_8622' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8623' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8686' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8689' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8692' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8693' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8699' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8702' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8703' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8704' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8705' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8768' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8771' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8774' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8775' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8778' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8781' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8784' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8785' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8786' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8787' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8850' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8853' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8856' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8857' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8860' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8863' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8866' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8867' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8868' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_8869' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_8932' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_8935' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8938' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8939' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8942' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_8945' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_8948' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_8949' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_8966' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_8969' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_8971' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8974' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_8982' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_8989' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_8992' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_8994' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9019' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_9024' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9025' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9088' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9091' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9094' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9095' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9098' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9101' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9104' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9105' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9106' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9107' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9170' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9173' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9176' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9177' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9180' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9183' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9186' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9187' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9188' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9189' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9252' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9255' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9258' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9259' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9262' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9265' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9268' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9269' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9270' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9271' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9334' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9337' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9340' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9341' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9344' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9347' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9350' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9351' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9368' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_9371' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_9373' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9376' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9384' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_9391' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_9394' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_9396' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9432' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_9437' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9438' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9501' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9504' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9507' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9508' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9511' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9514' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9517' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9518' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9519' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9520' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9583' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9586' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9589' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9590' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9593' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9596' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9599' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9600' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9601' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9602' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9665' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9668' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9671' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9672' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9675' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9678' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9681' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9682' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9683' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9684' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9747' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9750' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9753' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9754' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9757' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9760' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9763' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9764' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9781' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_9784' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_9786' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9789' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_9797' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_9804' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_9807' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_9809' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_9834' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_9839' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9840' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9903' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9906' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9909' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9910' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9913' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9916' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_9919' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9920' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9921' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_9922' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_9985' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_9988' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9991' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_9992' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_9995' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_9998' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10001' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10002' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10003' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10004' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10067' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10070' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10073' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10074' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10077' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10080' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10083' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10084' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10085' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10086' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10149' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10152' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10155' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10156' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10159' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10162' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10165' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10166' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10183' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_10186' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_10188' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10191' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10199' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_10206' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_10209' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_10211' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_10247' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_10252' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10253' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10316' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10319' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10322' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10323' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10329' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10332' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10333' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10334' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10335' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10398' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10401' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10404' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10405' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10408' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10411' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10414' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10415' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10416' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10417' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10480' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10483' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10486' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10487' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10490' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10493' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10496' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10497' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10498' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10499' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10562' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10565' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10568' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10569' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10572' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10575' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10578' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10579' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10596' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_10599' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_10601' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10604' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_10612' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_10619' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_10622' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_10624' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_10649' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_10654' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10655' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10718' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10721' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10724' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10725' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10728' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10731' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10734' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10735' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10736' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10737' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10800' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10803' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10806' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10807' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10810' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10813' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10816' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10817' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10818' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10819' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10882' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10885' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10888' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10889' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10892' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10895' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10898' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10899' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10900' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_10901' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_10964' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_10967' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10970' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10971' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10974' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_10977' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_10980' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_10981' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_10998' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_11001' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_11003' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11006' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11014' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11021' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11024' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11026' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11035' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_11068' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_11073' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11074' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11137' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11140' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11143' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11144' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11147' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11150' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11153' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11154' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11155' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11156' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11219' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11222' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11225' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11226' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11229' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11232' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11235' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11236' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11237' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11238' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11301' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11304' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11307' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11308' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11311' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11314' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11317' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11318' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11319' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11320' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11383' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11386' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11389' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11390' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11393' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11396' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11399' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11400' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11417' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_11420' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_11422' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11425' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11433' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_11440' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11443' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11445' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11454' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_11476' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_11481' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11482' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11545' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11548' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11551' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11552' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11555' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11558' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11561' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11562' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11563' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11564' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11627' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11630' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11633' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11634' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11637' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11640' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11643' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11644' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11645' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11646' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11709' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11712' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11715' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11716' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11719' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11722' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11725' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11726' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11727' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11728' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11791' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11797' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11798' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11801' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11804' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11807' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11808' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11825' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_11828' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_11830' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11833' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_11841' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_11848' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_11851' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_11853' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_11889' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_11894' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11895' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_11958' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_11961' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11964' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11965' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11968' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_11971' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_11974' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_11975' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_11976' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_11977' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12040' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12043' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12046' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12047' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12050' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12053' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12056' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12057' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12058' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12059' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12122' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12125' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12128' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12129' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12132' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12135' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12138' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12139' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12140' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12141' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12204' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12207' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12210' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12211' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12214' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12217' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12220' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12221' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12238' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_12241' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_12243' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12246' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12254' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_12261' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_12264' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_12266' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_12291' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_12296' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12297' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12360' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12363' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12366' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12367' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12370' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12373' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12376' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12377' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12378' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12379' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12442' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12445' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12448' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12449' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12452' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12455' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12458' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12459' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12460' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12461' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12524' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12527' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12530' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12531' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12534' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12537' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12540' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12541' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12542' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12543' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12606' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12609' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12612' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12613' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12616' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12619' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12622' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12623' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12640' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_12643' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_12645' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12648' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_12656' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_12663' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_12666' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_12668' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_12704' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_12709' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12710' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12773' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12776' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12779' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12780' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12783' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12786' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12789' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12790' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12791' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12792' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12855' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12858' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12861' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12862' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12865' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12868' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12871' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12872' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12873' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12874' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_12937' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_12940' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12943' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12944' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12947' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_12950' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_12953' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_12954' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_12955' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_12956' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13019' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13022' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13025' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13026' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13029' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13032' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13035' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13036' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13053' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_13056' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_13058' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13061' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13069' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13076' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13079' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13081' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13106' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_13111' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13112' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13175' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13178' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13181' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13182' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13185' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13188' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13191' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13192' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13193' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13194' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13257' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13260' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13263' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13264' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13267' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13270' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13273' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13274' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13275' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13276' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13339' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13342' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13345' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13346' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13349' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13352' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13355' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13356' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13357' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13358' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13421' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13424' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13427' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13428' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13431' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13434' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13437' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13438' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13455' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_13458' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_13460' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13463' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13471' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_13478' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13481' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13483' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13519' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_13524' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13525' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13588' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13591' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13594' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13595' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13598' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13601' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13604' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13605' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13606' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13607' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13670' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13673' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13676' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13677' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13680' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13683' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13686' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13687' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13688' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13689' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13752' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13755' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13758' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13759' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13762' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13765' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13768' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13769' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13770' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13771' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13834' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13837' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13840' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13841' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13844' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13847' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_13850' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13851' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_13868' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_13871' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_13873' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13876' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_13884' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_13891' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_13894' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_13896' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_13921' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_13926' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_13927' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_13990' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_13993' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_13996' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_13997' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14000' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14003' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14006' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14007' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14008' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14009' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14072' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14075' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14078' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14079' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14082' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14085' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14088' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14089' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14090' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14091' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14154' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14157' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14160' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14161' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14164' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14167' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14170' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14171' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14172' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14173' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14236' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14239' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14242' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14243' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14246' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14249' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14252' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14253' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14270' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_14273' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_14275' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14278' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14286' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_14293' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_14296' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_14298' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_14334' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_14339' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14340' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14403' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14406' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14409' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14410' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14413' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14416' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14419' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14420' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14421' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14422' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14485' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14488' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14491' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14492' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14495' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14498' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14501' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14502' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14503' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14504' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14567' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14570' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14573' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14574' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14577' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14580' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14583' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14584' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14585' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14586' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14649' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14652' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14655' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14656' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14659' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14662' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14665' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14666' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14683' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_14686' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_14688' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14691' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_14699' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_14706' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_14709' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_14711' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_14736' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_14741' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14742' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14805' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14808' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14811' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14812' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14815' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14818' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14821' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14822' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14823' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14824' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14887' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14890' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14893' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14894' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14897' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14900' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14903' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14904' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14905' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14906' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_14969' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_14972' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14975' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14976' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14979' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_14982' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_14985' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_14986' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_14987' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_14988' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15051' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15054' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15057' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15058' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15061' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15064' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15067' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15068' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15085' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_15088' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_15090' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15093' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15101' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15108' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15111' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15113' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15149' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_15154' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15155' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15218' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15221' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15224' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15225' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15228' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15231' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15234' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15235' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15236' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15237' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15300' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15303' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15306' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15307' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15310' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15313' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15316' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15317' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15318' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15319' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15382' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15385' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15388' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15389' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15392' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15395' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15398' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15399' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15400' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15401' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15464' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15467' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15470' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15471' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15474' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15477' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15480' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15481' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15498' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_15501' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_15503' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15506' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15514' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_15521' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15524' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15526' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15551' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_15556' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15557' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15620' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15623' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15626' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15627' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15630' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15633' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15636' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15637' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15638' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15639' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15702' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15705' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15708' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15709' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15712' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15715' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15718' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15719' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15720' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15721' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15784' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15787' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15790' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15791' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15794' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15797' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15800' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15801' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15802' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15803' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_15866' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_15869' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15872' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15873' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15876' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_15879' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_15882' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_15883' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_15900' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_15903' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_15905' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15908' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_15916' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_15923' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_15926' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_15928' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_15937' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_15970' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_15975' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_15976' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16039' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16042' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16045' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16046' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16049' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16052' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16055' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16056' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16057' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16058' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16121' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16124' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16127' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16128' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16131' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16134' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16137' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16138' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16139' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16140' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16203' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16206' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16209' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16210' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16213' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16216' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16219' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16220' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16221' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16222' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16285' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16288' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16291' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16292' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16295' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16298' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16301' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16302' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16319' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_16322' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_16324' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16327' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16335' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_16342' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_16345' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_16347' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_16356' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_16378' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_16383' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16384' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16447' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16450' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16453' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16454' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16457' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16460' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16463' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16464' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16465' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16466' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16529' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16532' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16535' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16536' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16539' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16542' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16545' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16546' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16547' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16548' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16611' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16614' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16617' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16618' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16621' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16624' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16627' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16628' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16629' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16630' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16693' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16699' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16700' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16703' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16706' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16709' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16710' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16727' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_16730' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_16732' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16735' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_16743' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_16750' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_16753' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_16755' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_16791' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_16796' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16797' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16860' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16863' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16866' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16867' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16870' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16873' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16876' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16877' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16878' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16879' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_16942' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_16945' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16948' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16949' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16952' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_16955' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_16958' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_16959' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_16960' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_16961' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17024' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17027' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17030' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17031' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17034' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17037' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17040' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17041' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17042' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17043' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17106' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17109' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17112' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17113' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17116' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17119' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17122' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17123' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17140' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_17143' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_17145' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17148' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17156' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17163' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17166' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17168' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_17193' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_17198' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17199' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17262' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17265' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17268' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17269' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17272' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17275' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17278' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17279' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17280' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17281' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17344' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17347' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17350' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17351' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17354' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17357' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17360' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17361' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17362' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17363' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17426' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17429' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17432' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17433' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17436' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17439' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17442' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17443' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17444' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17445' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17508' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17511' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17514' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17515' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17518' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17521' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17524' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17525' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17542' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_17545' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_17547' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17550' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17558' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_17565' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17568' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17570' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_17606' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_17611' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17612' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17675' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17678' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17681' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17682' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17685' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17688' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17691' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17692' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17693' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17694' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17757' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17760' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17763' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17764' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17767' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17770' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17773' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17774' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17775' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17776' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17839' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17842' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17845' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17846' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17849' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17852' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17855' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17856' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17857' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_17858' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_17921' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_17924' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17927' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17928' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17931' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_17934' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_17937' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_17938' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_17955' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_17958' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_17960' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17963' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_17971' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_17978' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_17981' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_17983' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18008' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_18013' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18014' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18077' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18080' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18083' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18084' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18087' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18090' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18093' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18094' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18095' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18096' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18159' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18162' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18165' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18166' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18169' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18172' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18175' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18176' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18177' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18178' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18241' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18244' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18247' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18248' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18251' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18254' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18257' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18258' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18259' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18260' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18323' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18326' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18329' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18330' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18333' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18336' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18339' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18340' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18357' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_18360' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_18362' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18365' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18373' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_18380' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_18383' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_18385' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18421' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_18426' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18427' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18490' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18493' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18496' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18497' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18500' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18503' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18506' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18507' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18508' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18509' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18572' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18575' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18578' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18579' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18582' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18585' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18588' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18589' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18590' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18591' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18654' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18657' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18660' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18661' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18664' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18667' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18670' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18671' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18672' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18673' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18736' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18739' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18742' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18743' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18746' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18749' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18752' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18753' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18770' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_18773' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_18775' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18778' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_18786' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_18793' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_18796' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_18798' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_18823' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_18828' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18829' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18892' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18895' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18898' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18899' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18902' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18905' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18908' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18909' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18910' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18911' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_18974' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_18977' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18980' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18981' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18984' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_18987' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_18990' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_18991' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_18992' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_18993' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19059' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19062' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19063' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19066' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19069' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19072' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19073' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19074' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19075' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19138' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19141' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19144' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19145' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19148' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19151' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19154' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19155' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19172' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_19175' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_19177' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19180' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19188' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_19195' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_19198' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_19200' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_19236' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_19241' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19242' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19305' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19308' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19311' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19312' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19315' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19318' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19321' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19322' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19323' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19324' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19387' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19390' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19393' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19394' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19397' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19400' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19403' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19404' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19405' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19406' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19469' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19472' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19475' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19476' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19479' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19482' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19485' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19486' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19487' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19488' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19551' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19554' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19557' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19558' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19561' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19564' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19567' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19568' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19585' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_19588' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_19590' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19593' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19601' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_19608' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_19611' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_19613' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_19638' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_19643' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19644' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19707' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19710' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19713' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19714' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19717' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19720' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19723' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19724' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19725' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19726' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19789' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19792' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19795' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19796' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19799' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19802' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19805' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19806' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19807' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19808' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19871' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19874' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19877' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19878' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19881' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19884' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19887' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19888' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19889' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_19890' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_19953' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_19956' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19959' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19960' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19963' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_19966' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_19969' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_19970' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_19987' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_19990' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_19992' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_19995' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20003' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20010' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20013' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20015' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20051' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_20056' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20057' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20120' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20123' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20126' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20127' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20130' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20133' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20136' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20137' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20138' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20139' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20202' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20205' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20208' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20209' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20212' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20215' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20218' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20219' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20220' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20221' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20284' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20287' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20290' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20291' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20294' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20297' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20300' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20301' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20302' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20303' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20366' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20369' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20372' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20373' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20376' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20379' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20382' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20383' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20400' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_20403' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_20405' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20408' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20416' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_20423' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20426' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20428' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20453' with existing initializer 'val_62'
[INFO] Replaced initializer 'val_20458' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20459' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20522' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20525' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20528' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20529' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20532' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20535' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20538' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20539' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20540' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20541' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20604' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20607' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20610' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20611' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20614' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20617' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20620' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20621' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20622' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20623' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20686' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20689' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20692' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20693' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20696' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20699' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20702' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20703' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20704' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20705' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20768' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20771' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20774' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20775' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20778' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20781' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20784' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20785' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20802' with existing initializer 'val_82'
[INFO] Replaced initializer 'val_20805' with existing initializer 'val_85'
[INFO] Replaced initializer 'val_20807' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20810' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_20818' with existing initializer 'val_2476'
[INFO] Replaced initializer 'val_20825' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_20828' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_20830' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_20839' with existing initializer 'val_5318'
[INFO] Replaced initializer 'val_20872' with existing initializer 'val_2085'
[INFO] Replaced initializer 'val_20877' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20878' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_20941' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_20944' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20947' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20948' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20951' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_20954' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_20957' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_20958' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_20959' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_20960' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21023' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21026' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21029' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21030' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21033' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21036' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21039' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21040' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21041' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_21042' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21105' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21108' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21111' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21112' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21115' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21118' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21121' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21122' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21123' with existing initializer 'val_19'
[INFO] Replaced initializer 'val_21124' with existing initializer 'val_1675'
[INFO] Replaced initializer 'val_21187' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_21190' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21193' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21194' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21197' with existing initializer 'val_1744'
[INFO] Replaced initializer 'val_21200' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_21203' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_21204' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_21221' with existing initializer 'val_2434'
[INFO] Replaced initializer 'val_21224' with existing initializer 'val_2437'
[INFO] Replaced initializer 'val_21226' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_21229' with existing initializer 'val_87'
[INFO] Replaced initializer 'val_21237' with existing initializer 'val_2063'
[INFO] Replaced initializer 'val_21244' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21247' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21249' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21258' with existing initializer 'val_5318'
[INFO] Replaced initializer 'scalar_tensor_default_1' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21324' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21339' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21342' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21344' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21358' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21375' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21378' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21380' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21383' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21391' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21398' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21401' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21403' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21417' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21434' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21437' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21439' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21442' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21450' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21457' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21460' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21462' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21476' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21493' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21496' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21498' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21501' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21509' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21516' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21519' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21521' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21529' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21532' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21534' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_2' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21558' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21575' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21578' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21580' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21583' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21591' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21598' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21601' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21603' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21617' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21634' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21637' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21639' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21642' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21650' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21657' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21660' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21662' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21676' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21693' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21696' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21698' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21701' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21709' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21716' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21719' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21721' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21735' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21752' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21755' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21757' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21760' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21768' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21775' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21778' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21780' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21788' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21791' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21793' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_3' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21817' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21834' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21837' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21839' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21842' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21850' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21857' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21860' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21862' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21876' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21893' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21896' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21898' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21901' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21909' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21916' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21919' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21921' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21935' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_21952' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_21955' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_21957' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21960' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_21968' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_21975' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_21978' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_21980' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_21994' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22011' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22014' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22016' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22019' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22027' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22034' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22037' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22039' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22047' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22050' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22052' with existing initializer 'val_110'
[INFO] Replaced initializer 'scalar_tensor_default_4' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22076' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22093' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22096' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22098' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22101' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22109' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22116' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22119' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22121' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22135' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22152' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22155' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22157' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22160' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22168' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22175' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22178' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22180' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22194' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22211' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22214' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22216' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22219' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22227' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22234' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22237' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22239' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22253' with existing initializer 'val_21299'
[INFO] Replaced initializer 'val_22270' with existing initializer 'val_21316'
[INFO] Replaced initializer 'val_22273' with existing initializer 'val_21319'
[INFO] Replaced initializer 'val_22275' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22278' with existing initializer 'val_21321'
[INFO] Replaced initializer 'val_22286' with existing initializer 'val_21332'
[INFO] Replaced initializer 'val_22293' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22296' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22298' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22306' with existing initializer 'val_105'
[INFO] Replaced initializer 'val_22309' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_22311' with existing initializer 'val_110'
[INFO] Replaced initializer 'val_22317' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_22320' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_22324' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22327' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_22334' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22335' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22338' with existing initializer 'val_22331'
[INFO] Replaced initializer 'val_22341' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22344' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22345' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22360' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22361' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22364' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22365' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22500' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22501' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22504' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22505' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22510' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22518' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_22621' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22636' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22637' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22640' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22641' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22646' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22654' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_576' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_22757' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22772' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_22773' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_22776' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_22777' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_22782' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_22790' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_578' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_579' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_22885' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_22891' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_22901' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22906' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22911' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_22991' with existing initializer 'val_22983'
[INFO] Replaced initializer 'val_22994' with existing initializer 'val_22992'
[INFO] Replaced initializer 'mul_582' with existing initializer 'mul_581'
[INFO] Replaced initializer 'val_23037' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23056' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_23060' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23063' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_23064' with existing initializer 'val_28'
[INFO] Replaced initializer 'scalar_tensor_default_25' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_23110' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23111' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23114' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23115' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23120' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23128' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_23223' with existing initializer 'val_22478'
[INFO] Replaced initializer 'val_23229' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23244' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23245' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23248' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23249' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23254' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23262' with existing initializer 'val_22378'
[INFO] Replaced initializer 'val_23357' with existing initializer 'val_22615'
[INFO] Replaced initializer 'val_23363' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23378' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23379' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23382' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23383' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23388' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23396' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_590' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_591' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_23491' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_23497' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23512' with existing initializer 'val_1482'
[INFO] Replaced initializer 'val_23513' with existing initializer 'val_47'
[INFO] Replaced initializer 'val_23516' with existing initializer 'val_22323'
[INFO] Replaced initializer 'val_23517' with existing initializer 'val_28'
[INFO] Replaced initializer 'val_23522' with existing initializer 'val_22370'
[INFO] Replaced initializer 'val_23530' with existing initializer 'val_22378'
[INFO] Replaced initializer 'mul_593' with existing initializer 'mul_575'
[INFO] Replaced initializer 'mul_594' with existing initializer 'mul_575'
[INFO] Replaced initializer 'val_23625' with existing initializer 'val_22751'
[INFO] Replaced initializer 'val_23631' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23636' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23641' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23646' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23651' with existing initializer 'val_22896'
[INFO] Replaced initializer 'val_23654' with existing initializer 'val_22915'
[INFO] Replaced initializer 'val_23657' with existing initializer 'val_22919'
[INFO] Replaced initializer 'val_23660' with existing initializer 'val_22923'
[INFO] Replaced initializer 'val_23663' with existing initializer 'val_22927'
[INFO] Replaced initializer 'val_23666' with existing initializer 'val_22931'
[INFO] Replaced initializer 'view_445' with existing initializer 'view_405'
[INFO] Replaced initializer 'val_23714' with existing initializer 'val_22983'
[INFO] Replaced initializer 'view_446' with existing initializer 'view_406'
[INFO] Replaced initializer 'val_23722' with existing initializer 'val_22983'
[INFO] Replaced initializer 'val_23723' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23725' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23730' with existing initializer 'val_22999'
[INFO] Replaced initializer 'mul_596' with existing initializer 'mul_581'
[INFO] Replaced initializer 'mul_597' with existing initializer 'mul_581'
[INFO] Replaced initializer 'val_23761' with existing initializer 'val_23031'
[INFO] Replaced initializer 'val_23767' with existing initializer 'val_22485'
[INFO] Replaced initializer 'val_23786' with existing initializer 'val_0'
[INFO] Replaced initializer 'val_23789' with existing initializer 'val_22992'
[INFO] Replaced initializer 'val_23792' with existing initializer 'val_1747'
[INFO] Replaced initializer 'val_23793' with existing initializer 'val_28'
[INFO] Replaced initializer 'int64_1_cast' with existing initializer 'val_108'
[INFO] Replaced initializer 'scalar_tensor_default_46' with existing initializer 'val_108'
[INFO] Replaced initializer 'val_23824' with existing initializer 'val_23095'
[INFO] Wrote onnx_exports/modelopt-fp8/vggt-8x3x518x518-pcd.onnx
[INFO] Rebinding external data...
[INFO] External data: /home/ashim/Documents/projects/vggt/onnx_exports/modelopt-fp8/vggt-8x3x518x518-pcd.onnx.data
[INFO] Sizes: ONNX=10.1 MB, DATA=4.44 GB
[INFO] Pruning model for PCD-only mode...
[INFO]   Keeping output: cat_322
[INFO]   Keeping output: view_411
[INFO]   Keeping output: view_412
[INFO]   Removing output: view_451
[INFO]   Removing output: view_452
[INFO]   Removing output: unsqueeze
[INFO] Pruned model saved: onnx_exports/modelopt-fp8/vggt-8x3x518x518-pcd.onnx
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/modelopt-fp8/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (Half precision with TF32 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[10/24/2025-16:54:36] [TRT] [E] [defaultAllocator.cpp::allocate::53] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-16:54:36] [TRT] [W] Requested amount of GPU memory (3709323532 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-16:54:36] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception [region-alloc.cpp:allocate:60] 3709323532-byte region '__mye1750134-consts' allocation failed.
[10/24/2025-16:58:18] [TRT] [E] [defaultAllocator.cpp::allocate::53] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-16:58:18] [TRT] [W] Requested amount of GPU memory (3640023296 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-16:58:18] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception [region-alloc.cpp:allocate:60] 3640023296-byte region '__mye1772651-consts' allocation failed.
[INFO] Engine saved: onnx_exports/modelopt-fp8/vggt-8x3x518x518-pcd_fp16.engine (2.20 GB)
[INFO] Shared ONNX for subsequent builds: onnx_exports/modelopt-fp8/vggt-8x3x518x518-pcd.onnx
[INFO] 
[INFO] ======================================================================
[INFO] Building BF16 variant (2/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, bf16 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: NVIDIA ModelOpt FP8 recipe
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/modelopt-fp8/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (BFloat16 precision)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled BF16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[10/24/2025-17:07:00] [TRT] [E] [defaultAllocator.cpp::allocate::53] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:07:00] [TRT] [W] Requested amount of GPU memory (3709323532 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:07:00] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception [region-alloc.cpp:allocate:60] 3709323532-byte region '__mye1750360-consts' allocation failed.
[10/24/2025-17:10:09] [TRT] [E] [defaultAllocator.cpp::allocate::53] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:10:09] [TRT] [W] Requested amount of GPU memory (3640023296 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:10:09] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception [region-alloc.cpp:allocate:60] 3640023296-byte region '__mye1773073-consts' allocation failed.
[INFO] Engine saved: onnx_exports/modelopt-fp8/vggt-8x3x518x518-pcd_bf16.engine (2.20 GB)
[INFO] 
[INFO] ======================================================================
[INFO] Building FP8 variant (3/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, fp8 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: NVIDIA ModelOpt FP8 recipe
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/modelopt-fp8/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (FP8 precision (RTX 5090 limited support))...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled FP8
[WARNING] FP8 enabled: RTX 5090 has limited FP8 support. Expect 'Unsupported data type FP8' warnings for some ops. TensorRT will fallback to FP16 automatically.
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Building engine (this may take several minutes)...
[INFO] FP8 build: Ignore 'Unsupported data type FP8' warnings - this is normal
[INFO] FP8 strict mode: temporarily disabled FP16 fallback
[INFO] FP8 strict mode: temporarily disabled TF32 fallback
[10/24/2025-17:12:46] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:12:46] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception unimplemented scalar type!
[10/24/2025-17:12:46] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:12:47] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:12:47] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:13:29] [TRT] [E] [defaultAllocator.cpp::allocate::53] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:13:29] [TRT] [W] Requested amount of GPU memory (3640023296 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:13:30] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception [region-alloc.cpp:allocate:60] 3640023296-byte region '__mye1771714-consts' allocation failed.
[10/24/2025-17:13:30] [TRT] [E] IBuilder::buildSerializedNetwork: Error Code 10: Internal Error (Could not find any implementation for node {ForeignNode[ONNXTRT_castHelper_24231_output[Constant]...node_permute_94 + node_view_398]}.)
[WARNING] Strict FP8 build returned None. Re-enabling fallbacks: FP16, TF32
[INFO] Re-enabled FP16 fallback
[INFO] Re-enabled TF32 fallback
[10/24/2025-17:13:31] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:13:31] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception unimplemented scalar type!
[10/24/2025-17:13:31] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:13:36] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:13:36] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:14:26] [TRT] [E] [defaultAllocator.cpp::allocate::53] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:14:26] [TRT] [W] Requested amount of GPU memory (3640023296 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:14:26] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000000 due to exception [region-alloc.cpp:allocate:60] 3640023296-byte region '__mye1772498-consts' allocation failed.
[10/24/2025-17:15:59] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:16:05] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:16:05] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:16:08] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:16:09] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[10/24/2025-17:16:09] [TRT] [E] Error Code: 9: Skipping tactic 0x0000000000000001 due to exception Unsupported data type FP8.
[INFO] Engine saved: onnx_exports/modelopt-fp8/vggt-8x3x518x518-pcd_fp8.engine (2.22 GB)
[INFO] 
[INFO] ======================================================================
[INFO] Building INT8 variant (4/4)
[INFO] ======================================================================
[INFO] Pipeline config: 8 cameras, 518x518, int8 precision
[INFO] PCD-only mode: exporting depth + camera heads only (30% faster)
[INFO] Pre-quantisation: NVIDIA ModelOpt FP8 recipe
[INFO] INT8 calibration: source=random Gaussian batches=1 seed=1337 gpu=True
[INFO] Removing sequence operations...
[INFO] Rewrote 88 Softmax axes to -1
[INFO] Replaced 264 sequence operations
[INFO] Wrote onnx_exports/modelopt-fp8/vggt-8x3x518x518-pcd.NOSEQ.onnx
[INFO] Simplifying ONNX graph...
[INFO] Model IR version: 10
[INFO]   opset[0]: domain='', version=18
[ERROR] Simplification failed: model with IR version >= 3 must specify opset_import for ONNX
[INFO] Validating ONNX model...
[INFO] ONNX validation passed
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 32 GB (attempt 1/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
/home/ashim/Documents/projects/vggt/onnx/vggt_to_trt_chatgpt.py:1691: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.
  config.int8_calibrator = calibrator
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/modelopt-fp8/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-17:18:52] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:18:52] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:18:52] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:18:52] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:18:52] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:18:57] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:18:57] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:18:57] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:18:57] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 15466496000 detected for tactic 0x6d55c70c4c781969.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:18:57] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:18:57] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:18:57] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:18:57] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 15466496000 detected for tactic 0x48c115a824ac468d.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:18:58] [TRT] [W] Engine generation failed with backend strategy 3.
Error message: [optimizer.cpp::computeCosts::4117] Error Code 10: Internal Error (Could not find any implementation for node node_Softmax_2333.).
Skipping this backend strategy.
[10/24/2025-17:19:03] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:19:03] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:19:03] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:19:03] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 15466496000 detected for tactic 0x6d55c70c4c781969.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:19:03] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:19:03] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:19:03] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:19:03] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 15466496000 detected for tactic 0x48c115a824ac468d.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:19:03] [TRT] [W] Engine generation failed with backend strategy 2.
Error message: [optimizer.cpp::computeCosts::4117] Error Code 10: Internal Error (Could not find any implementation for node node_Softmax_2333.).
Skipping this backend strategy.
[10/24/2025-17:19:03] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:19:03] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:19:03] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:19:03] [TRT] [E] [calibrator.cpp::calibrateEngine::1149] Error Code 2: Internal Error (Assertion context != nullptr failed. Context could not be created.)
[WARNING] TensorRT build failed with workspace 32 GB; retrying with 24 GB
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 24 GB (attempt 2/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/modelopt-fp8/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-17:19:54] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:19:54] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:19:54] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:19:54] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:19:54] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:19:59] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::169] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:19:59] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:19:59] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:19:59] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 15466496000 detected for tactic 0x6d55c70c4c781969.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:19:59] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::169] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:19:59] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:19:59] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:19:59] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 15466496000 detected for tactic 0x48c115a824ac468d.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:19:59] [TRT] [W] Engine generation failed with backend strategy 3.
Error message: [optimizer.cpp::computeCosts::4117] Error Code 10: Internal Error (Could not find any implementation for node node_Softmax_2333.).
Skipping this backend strategy.
[10/24/2025-17:20:04] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::169] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:20:04] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:20:04] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:20:04] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 15466496000 detected for tactic 0x6d55c70c4c781969.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:20:04] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::169] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:20:04] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:20:04] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:20:04] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 15466496000 detected for tactic 0x48c115a824ac468d.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:20:04] [TRT] [W] Engine generation failed with backend strategy 2.
Error message: [optimizer.cpp::computeCosts::4117] Error Code 10: Internal Error (Could not find any implementation for node node_Softmax_2333.).
Skipping this backend strategy.
[10/24/2025-17:20:04] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:20:04] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:20:04] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:20:04] [TRT] [E] [calibrator.cpp::calibrateEngine::1149] Error Code 2: Internal Error (Assertion context != nullptr failed. Context could not be created.)
[WARNING] TensorRT build failed with workspace 24 GB; retrying with 20 GB
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 20 GB (attempt 3/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/modelopt-fp8/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-17:20:55] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:20:55] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:20:55] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:20:55] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:20:55] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:21:00] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:21:00] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:21:00] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:21:00] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 15466496000 detected for tactic 0x6d55c70c4c781969.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:21:00] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:21:00] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:21:00] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:21:00] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 15466496000 detected for tactic 0x48c115a824ac468d.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:21:00] [TRT] [W] Engine generation failed with backend strategy 3.
Error message: [optimizer.cpp::computeCosts::4117] Error Code 10: Internal Error (Could not find any implementation for node node_Softmax_2333.).
Skipping this backend strategy.
[10/24/2025-17:21:06] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:21:06] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:21:06] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:21:06] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 15466496000 detected for tactic 0x6d55c70c4c781969.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:21:06] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:21:06] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:21:06] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:21:06] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 15466496000 detected for tactic 0x48c115a824ac468d.Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().
[10/24/2025-17:21:06] [TRT] [W] Engine generation failed with backend strategy 2.
Error message: [optimizer.cpp::computeCosts::4117] Error Code 10: Internal Error (Could not find any implementation for node node_Softmax_2333.).
Skipping this backend strategy.
[10/24/2025-17:21:06] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:21:06] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:21:06] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:21:06] [TRT] [E] [calibrator.cpp::calibrateEngine::1149] Error Code 2: Internal Error (Assertion context != nullptr failed. Context could not be created.)
[WARNING] TensorRT build failed with workspace 20 GB; retrying with 16 GB
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 16 GB (attempt 4/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/modelopt-fp8/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-17:21:55] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:21:55] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:21:55] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:21:55] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:21:55] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:22:41] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:22:41] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:22:41] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:22:41] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:22:41] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:22:52] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::169] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:22:52] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:22:52] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:22:52] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 15466496000 detected for tactic 0x6d55c70c4c781969.
[10/24/2025-17:22:52] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::169] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:22:52] [TRT] [E] [virtualMemoryBuffer.cpp::resizePhysical::154] Error Code 2: OutOfMemory (Requested size was 15466496000 bytes.)
[10/24/2025-17:22:52] [TRT] [W] Requested amount of GPU memory (15466496000 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:22:52] [TRT] [W] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 15466496000 detected for tactic 0x48c115a824ac468d.
[10/24/2025-17:22:52] [TRT] [W] Engine generation failed with backend strategy 2.
Error message: [optimizer.cpp::computeCosts::4117] Error Code 10: Internal Error (Could not find any implementation for node node_Softmax_10335.).
Skipping this backend strategy.
[10/24/2025-17:22:52] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:22:52] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:22:52] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:22:52] [TRT] [E] [calibrator.cpp::calibrateEngine::1149] Error Code 2: Internal Error (Assertion context != nullptr failed. Context could not be created.)
[WARNING] TensorRT build failed with workspace 16 GB; retrying with 12 GB
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 12 GB (attempt 5/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/modelopt-fp8/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-17:23:40] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:23:40] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:23:40] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:23:40] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:23:40] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:24:22] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:24:22] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:24:22] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:24:22] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:24:22] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:25:04] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:25:04] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:25:04] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:25:04] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:25:04] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:25:04] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:25:04] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:25:04] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:25:05] [TRT] [E] [calibrator.cpp::calibrateEngine::1149] Error Code 2: Internal Error (Assertion context != nullptr failed. Context could not be created.)
[WARNING] TensorRT build failed with workspace 12 GB; retrying with 8 GB
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 8 GB (attempt 6/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/modelopt-fp8/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-17:25:51] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:25:51] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:25:51] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:25:51] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:25:51] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:26:33] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:26:33] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:26:33] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:26:33] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:26:33] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:27:15] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:27:15] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:27:15] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:27:15] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:27:15] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:27:15] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:27:15] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:27:15] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:27:15] [TRT] [E] [calibrator.cpp::calibrateEngine::1149] Error Code 2: Internal Error (Assertion context != nullptr failed. Context could not be created.)
[WARNING] TensorRT build failed with workspace 8 GB; retrying with 6 GB
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 6 GB (attempt 7/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/modelopt-fp8/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-17:28:02] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:28:02] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:28:02] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:28:02] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:28:02] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:28:44] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:28:44] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:28:44] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:28:44] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:28:44] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:29:26] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:29:26] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:29:26] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:29:26] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:29:26] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:29:26] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:29:26] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:29:26] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:29:26] [TRT] [E] [calibrator.cpp::calibrateEngine::1149] Error Code 2: Internal Error (Assertion context != nullptr failed. Context could not be created.)
[WARNING] TensorRT build failed with workspace 6 GB; retrying with 4 GB
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 4 GB (attempt 8/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/modelopt-fp8/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-17:30:14] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:30:14] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:30:14] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:30:14] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:30:14] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:30:57] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:30:57] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:30:57] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:30:57] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:30:57] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:31:39] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:31:39] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:31:39] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:31:39] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:31:39] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:31:39] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:31:39] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:31:39] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:31:39] [TRT] [E] [calibrator.cpp::calibrateEngine::1149] Error Code 2: Internal Error (Assertion context != nullptr failed. Context could not be created.)
[WARNING] TensorRT build failed with workspace 4 GB; retrying with 2 GB
[INFO] Building TensorRT engine (INT8 quantization with FP16 fallback)...
[INFO] TensorRT version: 10.11.0.33
[INFO] TensorRT workspace limit: 2 GB (attempt 9/9)
[INFO] Input shape: (8, 3, 518, 518)
[INFO] Enabled INT8
[INFO] Enabled FP16
[INFO] Enabled TF32
[INFO] Optimization level: 5
[INFO] Setting up INT8 calibration...
[WARNING] No calibration data provided; using random Gaussian noise
[INFO] INT8 calibrator using GPU staging buffer (24.57 MB)
[INFO] INT8 calibrator configured (1 batches, cache=onnx_exports/modelopt-fp8/calibration-8x518x518.cache)
[INFO] Building engine (this may take several minutes)...
[10/24/2025-17:32:26] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:32:26] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:32:26] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:32:26] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:32:26] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:33:09] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:33:09] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:33:09] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:33:09] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:33:09] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:33:52] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:33:52] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:33:52] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:33:52] [TRT] [W] Exception thrown when profiling the engine. Error message: [builderUtils.cpp::measureEngineInferenceTime::482] Error Code 2: Internal Error (Assertion context != nullptr failed. ).
[10/24/2025-17:33:52] [TRT] [W] Using rough estimation for the inference time of the engine.
[10/24/2025-17:33:52] [TRT] [E] [resizingAllocator.cpp::allocate::76] Error Code 1: Cuda Runtime (out of memory)
[10/24/2025-17:33:52] [TRT] [W] Requested amount of GPU memory (17908331008 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2025-17:33:52] [TRT] [E] [executionContext.cpp::initializeExecutionContext::639] Error Code 2: OutOfMemory (Requested size was 17908331008 bytes.)
[10/24/2025-17:33:52] [TRT] [E] [calibrator.cpp::calibrateEngine::1149] Error Code 2: Internal Error (Assertion context != nullptr failed. Context could not be created.)
[ERROR] Pipeline failed: TensorRT build returned None (build failed)
Traceback (most recent call last):
  File "/home/ashim/Documents/projects/vggt/onnx/vggt_to_trt_chatgpt.py", line 2135, in main
    action="store_true",
  File "/home/ashim/Documents/projects/vggt/onnx/vggt_to_trt_chatgpt.py", line 1912, in run
    else:
  File "/home/ashim/Documents/projects/vggt/onnx/vggt_to_trt_chatgpt.py", line 1820, in build_trt
    )
  File "/home/ashim/Documents/projects/vggt/onnx/vggt_to_trt_chatgpt.py", line 1820, in build_trt
    )
  File "/home/ashim/Documents/projects/vggt/onnx/vggt_to_trt_chatgpt.py", line 1820, in build_trt
    )
  [Previous line repeated 5 more times]
  File "/home/ashim/Documents/projects/vggt/onnx/vggt_to_trt_chatgpt.py", line 1830, in build_trt
    engine_bytes = builder.build_serialized_network(network, config)
RuntimeError: TensorRT build returned None (build failed)
Applied 1260 of general pattern rewrite rules.
